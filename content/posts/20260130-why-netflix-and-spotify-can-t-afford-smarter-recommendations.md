---
title: "Bandits and Agents: Netflix and Spotify Recommender Stacks in 2026"
seoTitle: "Hybrid Recommender Systems 2026: Multi-Armed Bandits, LLMs & Inference Economics"
date: 2026-01-30
publishDate: 2026-01-30T03:00:00Z
images:
- https://static.philippdubach.com/ograph/ograph-recommender-architecture.jpg
description: "How hybrid recommender systems balance multi-armed bandits against LLM inference cost economics in 2026. A deep dive into Netflix recommendation algorithm architecture and Spotify's AI DJ recommender system."
keywords:
- hybrid recommender systems 2026
- multi-armed bandits recommender systems
- LLM inference cost economics
- Netflix recommendation algorithm architecture
- Spotify AI DJ recommender system
- recommender systems 2026
- inference economics
- contextual bandits
- exploration exploitation tradeoff
- Thompson Sampling
- candidate generation ranking
- personalization at scale
categories:
- AI
- Tech
math: true
draft: false
---
Hyperscalers spent over [$350 billion on AI infrastructure](https://www.goldmansachs.com/insights/articles/why-ai-companies-may-invest-more-than-500-billion-in-2026) in 2025 alone, with projections exceeding $500 billion in 2026. The trillion-dollar question is not whether machines can reason, but whether anyone can afford to let them. Hybrid recommender systems sit at the center of this tension. Large Language Models promised to transform how Netflix suggests your next show or how Spotify curates your morning playlist. Instead, the industry has split into two parallel universes, divided not by capability but by cost.

On one side sits what engineers call the "classical stack": matrix factorization, two-tower embedding models, and contextual bandits. These methods respond in microseconds, scale linearly with users, and run on nothing more complicated than dot products. A query costs a fraction of a cent. On the other side is the "agentic stack": LLM-based reasoning engines that can handle requests like "find me a sci-fi movie that feels like Blade Runner but was made in the 90s." This second approach consumes thousands of tokens per recommendation. The cost difference is not incremental; it is [orders of magnitude](https://www.softwareseni.com/understanding-inference-economics-and-why-ai-costs-spiral-beyond-proof-of-concept/). LLM inference cost economics, more than any algorithmic breakthrough, is now the dominant force shaping recommender architecture.

The 2026 consensus is a hybrid architecture: use the cheap, fast models for candidate generation from millions of items, then invoke the expensive reasoning layer only for the final dozen items a user actually sees. This "funnel" pattern — retrieval, then ranking, then re-ranking — is the only way to make the economics work. The smartest model is reserved for the fewest items.

What makes this work in practice goes back to a formalism from [1933](https://www.jstor.org/stable/2332286): the multi-armed bandit. Imagine a gambler facing a row of slot machines, each with an unknown payout rate. She wants to maximize her winnings over a night of play. If she always pulls the arm with the highest observed payout, she might miss a better machine she never tried. If she explores too much, she wastes money on losers. The mathematics of this exploration–exploitation tradeoff define *regret*:

$$
R(T) = \mu^* \cdot T - \sum_{t=1}^{T} \mu(a_t)
$$

Here μ* is the best possible average reward, and μ(aₜ) is the reward from whatever arm she actually pulled at time t. Total regret is how much she left on the table by not knowing the optimal choice in advance. The goal of every multi-armed bandit algorithm in recommender systems is to drive this quantity sublinear in T — to learn fast enough that the cost of exploration vanishes relative to the horizon. {{< img src="slide10.png" alt="Multi-armed bandit recommender system diagram: a Learner taking Actions and receiving Rewards from an Environment, with the goal to maximize cumulative reward or minimize cumulative regret" width="80%" >}} The three main exploration strategies each take a different approach: epsilon-greedy adds random noise to avoid getting stuck; Upper Confidence Bound (UCB) prefers actions with uncertain values; Thompson Sampling selects actions according to the probability they are optimal. In practice, Thompson Sampling tends to outperform the others because its exploration is guided by posterior uncertainty rather than arbitrary randomness — it explores where it matters most. {{< img src="slide12.png" alt="Principles of Exploration in recommender systems: Naive Exploration (ε-greedy), Optimism in the Face of Uncertainty (UCB), and Probability Matching (Thompson Sampling)" width="80%" >}} Every recommendation you see on [Netflix's homepage](https://research.netflix.com/publication/lessons-learnt-from-consolidating-ml-models-in-a-large-scale-recommendation) is the output of an algorithm trying to minimize exactly this quantity, whether it realizes it or not.

Netflix's recommendation algorithm architecture runs this optimization across [three computation layers](https://www.slideshare.net/slideshow/a-multiarmed-bandit-framework-for-recommendations-at-netflix/102629078). Offline systems crunch terabytes of viewing history to train deep collaborative filtering models, a process that takes hours and happens on a schedule. Nearline systems update user embeddings seconds after a click, keeping the recommendations fresh without the cost of full retraining. Online systems respond to each page load in milliseconds, combining the precomputed signals with real-time context like time of day and device type. The architecture is a [latency-cost tradeoff](https://netflixtechblog.com/post-training-generative-recommenders-with-advantage-weighted-supervised-finetuning-61a538d717a9): deep analysis happens in batch, while the user-facing layer stays fast. {{< img src="slide28.png" alt="Netflix recommendation algorithm architecture: Member Activity and Contextual Information flow through an Offline System for model training, then to an Online System where the Multi-Armed Bandit produces recommendations" width="80%" >}} What Netflix learned from a decade of experimentation is counterintuitive. The goal is not to recommend what users will definitely watch, but what they would not have found on their own. They call this "incrementality." A greedy algorithm that always surfaces the highest-probability titles just confirms what users already knew — it exploits without exploring, and in doing so collapses the discovery space. A better approach is to measure the *causal effect* of the recommendation: how much does showing this thumbnail increase the probability of a play compared to not showing it? Some titles have low baseline interest but high incrementality. Those are the ones worth featuring. This is the exploration–exploitation tradeoff made concrete: the value of a recommendation is not its predicted rating, but its marginal contribution to discovery. {{< img src="slide41.png" alt="Netflix incrementality analysis: scatter plot showing incremental probability vs baseline probability, where Title A has low baseline but high incremental lift, while Title C has high baseline but less benefit from featuring" width="80%" >}} Spotify's AI DJ recommender system takes a different approach to the same problem. Their "[AI DJ](https://research.atspotify.com/2025/9/you-say-search-i-say-recs-a-scalable-agentic-approach-to-query-understanding)" feature uses what engineers internally call the "agentic router." When you ask for "music for a rainy reading session in 1990s Seattle," the router decides whether to invoke the expensive LLM reasoning layer or just fall back to keyword matching against collaborative filtering embeddings. Complex queries get the big model; simple ones get the fast path. This router is the economic governor of the entire system — an inference cost optimizer disguised as a product feature. Underneath the DJ's personality, built on Spotify's Sonantic voice synthesis and LLM-generated contextual narratives, sits a bandit framework called BaRT (Bandits for Recommendations as Treatments) that quietly balances what you know you like against what you might not yet know you need.

Not everyone is convinced the algorithms are making us better off. My own [analysis of social media success prediction](https://philippdubach.com/posts/social-media-success-prediction-bert-models-for-post-titles/) found that sophisticated language models often just memorize temporal patterns rather than learning what actually makes content good. They learn the news cycle, not the news.

The risk is that we build hybrid recommender systems that are technically brilliant but experientially hollow, engineering away the serendipity that made discovery meaningful in the first place. The recommender is becoming a curator, and the curator is becoming an agent. The architecture will keep evolving — foundation models for recommendations, reinforcement learning from human feedback applied to discovery, inference costs that continue their [10× annual decline](https://a16z.com/llmflation-llm-inference-cost/) — but the open question for 2026 is whether we want to be the curators of our own lives, or merely consumers of an optimized feed.

_Slides courtesy of "[A Multi-Armed Bandit Framework for Recommendations at Netflix](https://www.slideshare.net/slideshow/a-multiarmed-bandit-framework-for-recommendations-at-netflix/102629078)" by Jaya Kawale, Netflix._