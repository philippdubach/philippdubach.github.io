---
title: "Peter Thiel's Physics Department"
seoTitle: "Peter Thiel's Stagnation Thesis Meets AI for Science"
date: 2026-02-16
lastmod: 2026-02-16
publishDate: 2026-02-16T03:00:00Z
images:
- https://static.philippdubach.com/ograph/ograph-physics-department.jpg
description: "Peter Thiel says physics stalled in 1972. Then GPT-5.2 proved a new result in theoretical physics. The 75:1 AI compute gap between commerce and science."
keywords:
- AI scientific discovery
- Peter Thiel stagnation thesis
- GPT-5.2 gluon discovery
- AI breakthrough physics 2026
- AI compute gap science
- Great Stagnation physics
- total factor productivity decline
- AlphaFold Nobel Prize 2024
- AI for science
- DeepMind GNoME materials discovery
- AI fusion energy plasma control
- dual-use AI technology
- scientific progress stagnation
- bits vs atoms innovation
- research productivity decline
- computational irreducibility physics
- AI drug discovery clinical trials
- scattering amplitudes theoretical physics
- federal AI R&D spending
- Demis Hassabis radical abundance
draft: true
faq:
- question: "What did GPT-5.2 discover in theoretical physics?"
  answer: "On February 13, 2026, GPT-5.2 derived and formally proved that single-minus gluon tree amplitudes are nonzero in the half-collinear regime, overturning a decades-old assumption in scattering amplitude theory. Researchers from the Institute for Advanced Study, Vanderbilt, Cambridge, Harvard, and OpenAI co-authored the paper."
- question: "Is AI ending the Great Stagnation in physics?"
  answer: "Partially. AI has produced results in protein folding (AlphaFold, Nobel Prize 2024), materials science (GNoME, 2.2 million crystal structures), fusion plasma control, and now theoretical physics (GPT-5.2 gluon discovery). But critics like Noam Chomsky and Peter Woit argue AI does pattern matching, not the conceptual revolution that drives fundamental physics forward."
- question: "How much does industry spend on AI versus government science?"
  answer: "The ratio is roughly 75:1. Big Tech spent over $250 billion on AI infrastructure in 2024-2025. Total US federal AI R&D is $3.3 billion per year. The DOE Genesis Mission's $320 million first round is less than Meta spends on AI infrastructure in a single week."
- question: "Why has physics progress slowed down since the 1970s?"
  answer: "Total factor productivity growth fell from 1.7% annually in 1947-1973 to 0.4% since 2004. The Standard Model was complete by the early 1970s. Since then, physics has confirmed predictions (Higgs boson, gravitational waves) but produced no new fundamental theories. String theory has zero experimental confirmations after 55 years."
- question: "What are the dual-use risks of AI-accelerated physics?"
  answer: "Every physics breakthrough is inherently dual-use. Fission was discovered in 1938 and weaponized by 1945. The same AI-driven fusion advances that could deliver clean energy could optimize weapons physics. The same materials science that could produce room-temperature superconductors could produce materials for autonomous weapons systems."
---

> "Everything else in science is stamp collecting. Physics is the real thing. That gave us everything."
> -- Jimmy Carr, channeling Ernest Rutherford on the TRIGGERnometry podcast, December 11, 2025

On December 11, Jimmy Carr sat on the TRIGGERnometry podcast and delivered a riff that sounded like Peter Thiel's stagnation thesis filtered through a comedian's timing: "Minus the screens from any room, we're living in the 1970s. Nothing's happened in physics since '72. String theory has not got us anywhere. But if you take the compute power of AI and point it at physics, what happens? We could have a world of plenty. I hope that's the world we live in. But it could go another way."

Two months later, on February 13, GPT-5.2 [derived and formally proved](https://thequantuminsider.com/2026/02/13/ai-scientist-spots-what-physicists-missed-in-gluon-scattering/) a new result in theoretical physics: single-minus gluon scattering amplitudes, long assumed to vanish, are nonzero in the half-collinear regime. Nima Arkani-Hamed at the Institute for Advanced Study called the formulas "strikingly simple" after fifteen years of personal curiosity about the problem. Nathaniel Craig at UC Santa Barbara called it "journal-level research advancing the frontiers of theoretical physics."

The question Carr posed is no longer hypothetical. AI has been pointed at physics. The question is what happens next.

## Thiel's stagnation case

Carr was paraphrasing Thiel, who has been making this argument for fifteen years. The [Founders Fund manifesto](https://www.scribd.com/document/61379051/What-Happened-to-the-Future-Founders-Fund-Manifesto) (2011) put it bluntly: "We wanted flying cars, instead we got 140 characters." Thiel's framework distinguishes progress in bits from progress in atoms: spectacular digital gains since 1970, physical-world stagnation. Tyler Cowen named the broader phenomenon the Great Stagnation. On the [Douthat podcast](https://singjupost.com/a-i-mars-and-immortality-are-we-dreaming-big-enough-peter-thiel-transcript/) Thiel was more measured: "The claim was that the velocity had slowed, it wasn't zero."

The data supports the velocity claim. Total factor productivity growth, the metric that captures genuine scientific progress and technological improvement, ran at roughly **1.7%** annually from 1947 to 1973. Since 2004, it has averaged **0.4%**. Robert Gordon's *The Rise and Fall of American Growth* argues the "special century" of 1870 to 1970 was a one-time event. [Bloom, Jones, Van Reenen, and Webb](https://mattsclancy.substack.com/p/science-is-getting-harder) showed in the *American Economic Review* that maintaining Moore's Law required **18x** more R&D spending in 2014 versus 1971. Ideas are getting harder to find. Research productivity is declining exponentially.

{{< img src="tfp-growth-stagnation.png" alt="Peter Thiel's stagnation thesis in data: US Total Factor Productivity growth by era showing 1.7 percent annually from 1947 to 1973 during the postwar boom, collapsing to 0.5 percent from 1973 to 1996, briefly recovering to 2.0 percent during the IT revival of 1996 to 2004, then falling back to 0.4 percent from 2004 to present, a 76 percent decline from the postwar peak" width="90%" >}}

The Standard Model of particle physics was essentially complete by the early 1970s. Since then, we have confirmed things we already predicted: the Higgs boson (2012, 48 years after prediction), gravitational waves (2015, 99 years after Einstein), the accelerating expansion of the universe (1998). Important experimental work. But confirmations, not revolutions. No supersymmetric particles. No extra dimensions. No new fundamental energy sources. No unified field theory. String theory, the leading candidate for physics beyond the Standard Model, has produced [zero experimentally confirmed predictions](https://www.researchgate.net/publication/334607591_The_String_Theory_Landscape) in 55 years and admits roughly 10^500 possible solutions, which is another way of saying it predicts everything and therefore nothing.

[Sabine Hossenfelder](https://www.goodreads.com/author/quotes/17201066.Sabine_Hossenfelder) captured the frustration: "Theoretical physicists used to explain what was observed. Now they try to explain why they can't explain what was not observed."

## What AI has already done for science

The scorecard so far is more impressive than most people realize.

[AlphaFold](https://x.com/demishassabis/status/1845864764469334239) predicted the three-dimensional structures of **214 million proteins**, effectively solving the protein folding problem for structural biology. It won the 2024 Nobel Prize in Chemistry for Demis Hassabis and John Jumper, and has been used by over 2 million researchers in 190 countries. DeepMind's [GNoME](https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/) identified **2.2 million new crystal structures** and 381,000 predicted-stable materials, equivalent to roughly 800 years of prior human discovery in materials science. Lawrence Berkeley Lab's A-Lab robotically synthesized 41 of these in [17 days](https://deepmind.google/blog/millions-of-new-materials-discovered-with-deep-learning/).

In fusion, [DeepMind trained a reinforcement learning system](https://deepmind.google/blog/bringing-ai-to-the-next-generation-of-fusion-energy/) to autonomously control plasma in a real tokamak at EPFL, maintaining stability and sculpting plasma into configurations no human operator had achieved. [Princeton researchers](https://engineering.princeton.edu/news/2024/02/21/engineers-use-ai-wrangle-fusion-power-grid) predicted tearing instabilities **300 milliseconds** in advance and adjusted reactor parameters in real time: the first demonstration of preventing, not just suppressing, the instabilities that have plagued fusion for decades. [TAE Technologies](https://www.cleanenergy-platform.com/insight/inside-taes-2025-plasma-breakthroughand-how-it-changed-fusions-trajectory) used AI-optimized beam injection to sustain plasma above **70 million degrees C**. At Lawrence Livermore, the CogSim AI framework [predicted a 74% probability of ignition](https://lasers.llnl.gov/news/llnl-researchers-employed-ai-driven-model-predict-fusion-ignition-shot) days before the December 2022 shot that achieved it.

Microsoft and Pacific Northwest National Lab [screened 32.6 million inorganic materials](https://www.datacenterdynamics.com/en/news/microsoft-and-pnnl-use-ai-and-hpc-for-battery-materials-research/) in roughly 80 hours, identified 18 finalists, and produced a [working battery prototype](https://techround.co.uk/news/microsofts-ai-powered-battery-discovery-could-replace-lithium/) using **70% less lithium** within nine months. In drug discovery, at least [75 AI-discovered drugs](https://pmc.ncbi.nlm.nih.gov/articles/PMC11800368/) have entered clinical trials, up from 3 in 2016, with Phase I success rates of **80 to 90%** compared to the traditional 40%.

And then, three days ago, GPT-5.2 produced a genuine result in theoretical physics. Not applied. Theoretical. A proof that human physicists had not found.

In mathematical reasoning, the trajectory is striking. [AlphaGeometry](https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/) solved 25 of 30 Olympiad geometry problems in January 2024. By July 2024, [AlphaProof earned a silver medal](https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/) at the International Mathematical Olympiad. By 2025, [Gemini Deep Think scored gold](https://deepmind.google/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/): 5 of 6 problems, 35 points, end-to-end in natural language. Terence Tao, arguably the world's greatest living mathematician, [revised his prediction](https://siliconreckoner.substack.com/p/terence-tao-on-machine-assisted-proofs) for superhuman AI mathematics from 2029 to 2026.

## The 75:1 compute gap

Here is the number that should trouble anyone thinking seriously about AI for science. Big Tech spent over **$250 billion** on AI infrastructure in 2024 and 2025. Total US federal AI R&D spending: [**$3.3 billion** per year](https://federalbudgetiq.com/insights/federal-ai-and-it-research-and-development-spending-analysis/). That is a compute divide of roughly **75:1** between commercial and scientific AI investment. The [NAIRR pilot](https://cset.georgetown.edu/article/the-nairr-pilot-estimating-compute/) allocated about 3.2 yottaFLOPs to academic researchers, enough to train GPT-3.5 once but not enough for a single GPT-4-class run.

{{< img src="compute-gap-75-to-1.png" alt="The 75 to 1 AI compute gap between industry and science: Big Tech AI capex at over 250 billion dollars per year versus total federal AI R&D spending at 3.3 billion, DOE FASST at 2.4 billion authorized but pending, DOE Genesis at 320 million one-time, and NSF core AI at 494 million per year" width="90%" >}}

The DOE's [Genesis Mission](https://www.anl.gov/article/what-were-argonnes-top-science-research-breakthroughs-in-2025) announced $320 million in December 2025. That is less than what Meta spends on AI infrastructure in a week. The [FASST initiative](https://federalbudgetiq.com/insights/federal-ai-and-it-research-and-development-spending-analysis/) authorized $2.4 billion per year for five years, $12 billion total, but congressional appropriations are still pending. The US has three exascale supercomputers at national labs. These serve all of science, not just AI.

The implication is straightforward. If AI has already produced results in theoretical physics, materials science, fusion energy, and drug discovery with what amounts to scraps from the commercial table, what happens when someone makes a serious allocation? [Hassabis told Fortune](https://fortune.com/2026/02/11/demis-hassabis-nobel-google-deepmind-predicts-ai-renaissance-radical-abundance/) in February 2026 that in 10 to 15 years "we'll be in a kind of new golden era of discovery, a kind of new renaissance." He described a vision of "radical abundance" where AI has "successfully bottled the scientific method."

[Goldman Sachs estimates](https://www.goldmansachs.com/insights/articles/generative-ai-could-raise-global-gdp-by-7-percent) generative AI could raise global GDP by **7%**, roughly $7 trillion. [McKinsey pegs](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-next-innovation-revolution-powered-by-ai) R&D-specific value at **$360 to $560 billion** annually, but explicitly noted they did not attempt to estimate "the value of truly breakthrough innovations that transform markets (if, for example, nuclear fusion was to enable limitless, clean electricity production)." The thing they couldn't model is the thing that matters most.

## The stamp collector's dilemma

The bear case is simple and serious. AI is the greatest pattern-matching system ever built. Physics does not advance by pattern matching. It advances by conceptual revolution: Riemannian geometry for general relativity, an entirely new mathematical framework for quantum mechanics, gauge theory for the Standard Model. None of these were discoverable in existing data.

[Noam Chomsky](https://medium.com/@abdullrahmanburhan36/noam-chomsky-on-the-false-promise-of-chatgpt-18c70cda5e24) argued in the *New York Times* that AI's deepest flaw "is the absence of the most critical capacity of any intelligence: to say not only what is the case, but also what is not the case and what could and could not be the case." [Peter Woit](https://www.math.columbia.edu/~woit/wordpress/?p=15362) at Columbia spent "over 100 hours probing these models" on open problems and found they "basically never try to come up with something new" when the answer is not already in the training data.

[Dario Amodei](https://www.darioamodei.com/essay/machines-of-loving-grace) was notably careful in "Machines of Loving Grace." He predicted AI could compress 50 to 100 years of biological progress into 5 to 10 years, but on physics he hedged: particle physicists are "limited by data from particle accelerators" and "it's not clear that they would do drastically better if they were superintelligent." Some problems are not compute-limited. They are experiment-limited, or concept-limited, or both.

Stephen Wolfram's principle of computational irreducibility poses the hardest theoretical limit: some systems cannot be predicted by any shortcut. The only way to know what they do is to run them. If fundamental physics contains computationally irreducible problems, no amount of AI compute will crack them.

But [Mario Krenn](https://mariokrenn.wordpress.com/) at Max Planck offers a counterpoint from the lab bench. His team published in *Physical Review X* on AI-discovered gravitational wave detector designs that outperform human designs, and in *Science Advances* on an AI-discovered violation of Bell inequality with unentangled photons. He does not claim AI understands physics. He claims it finds things physicists miss: "I let the algorithm run, and within a few hours it found exactly the solution that we as human scientists couldn't find for many weeks."

{{< img src="ai-science-paradox.png" alt="The AI scientific discovery paradox: quantity metrics surging with 3x more papers published, 4.8x more citations received, and 33 percent more arXiv preprints, but quality metrics declining with 4.6 percent less topical territory covered, 22 percent less cross-paper engagement, and researchers herding toward the same topics" width="90%" >}}

## The two roads

The nuclear parallel is the one that matters. Fission was discovered in Berlin in December 1938. Hiroshima was August 1945. Seven years from pure physics to weapon. The first nuclear power plant came nine years later. Oppenheimer captured the dynamic: "When you see something that is technically sweet, you go ahead and do it, and you argue about what to do about it only after you have had your technical success."

Every AI-accelerated physics breakthrough is inherently dual-use technology. The [IAEA reports](https://www.peaknano.com/blog/the-iaea-world-fusion-outlook-2025) 35 of 45 private fusion companies expect commercial pilot plants between 2030 and 2035. Commonwealth Fusion Systems has raised roughly $3 billion. [China established a state-owned fusion company](https://english.news.cn/20250724/213ed7ff0e954935bd5645b30a9dafe3/c.html) in July 2025. The fusion market is projected at **$430 billion by 2030**. The same plasma control AI that keeps a tokamak stable could, in principle, optimize weapons physics.

I don't know which road we're on. I'm not sure anyone does. But the velocity of AI scientific discovery, from Olympiad geometry problems to a gold medal at the International Mathematical Olympiad to a result in theoretical physics, all within 25 months, suggests the question will be answered empirically rather than philosophically. And probably sooner than the physicists expect.

The cost of intelligence has fallen roughly [150x](https://blog.samaltman.com/three-observations) in two years. The cost of pointing it at physics is a policy choice, not a technical constraint. The 75:1 compute gap between commercial and scientific AI spending is the number that determines how fast this goes. Whether it should go fast is a different question entirely.

Have feedback, comments, or ideas? I'd love to hear from you.
