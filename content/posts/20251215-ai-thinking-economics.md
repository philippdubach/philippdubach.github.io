---
title: 'When Machines Think: The Economics of AI Understanding'
date: 2025-12-15
images:
- https://static.philippdubach.com/ograph/ograph-ai-thinking.jpg
external_url: https://www.newyorker.com/magazine/2025/11/10/the-case-that-ai-is-thinking
description: Do AI models think or pattern-match? What neuroscience research means
  for value creation and whether understanding creates competitive moats.
keywords:
- artificial intelligence
- AI thinking
- machine learning economics
- neural networks
- AI commoditization
- cognitive economics
- AI model performance
- pattern matching
- AI understanding
- neuroscience AI
- AI investment
- large language models
draft: true
aliases:
- /2025/12/15/20251215-ai-thinking-economics/
---
In November 2025, The New Yorker published ["The Case That A.I. Is Thinking"](https://www.newyorker.com/magazine/2025/11/10/the-case-that-ai-is-thinking), a piece that captured the strange moment we're in. Dario Amodei predicts AI smarter than Nobel Prize winners by 2027. Sam Altman writes about digital superintelligence. Meanwhile, most people interact with AI tools that feel like Clippy, Microsoft's old assistant that was more annoying than useful. The gap between the hype and daily experience creates a fog where it's tempting to conclude there's nothing to see here.


But there's a more interesting question than whether AI is thinking. What does thinking mean for value creation? If compression equals understanding, as Eric Baum argued decades ago, and if today's models compress the internet into something that fits on your laptop, does that create economic moats or just better commodities? The neuroscience insights are real. The economic implications are less clear.

Eric Baum's 2003 book "What Is Thought?" makes a simple argument: understanding is compression, and compression is understanding. When you compress data effectively, you're not just shrinking files. You're finding the underlying structure. A calculator program compresses millions of arithmetic examples more efficiently than a zip file because it understands the rules. Large language models trained on terabytes of text compress that into models one six-hundredth the size. DeepSeek, one of the best open-source models, can write novels, suggest medical diagnoses, and speak dozens of languages. The training data is massive. The model you download is tiny. That compression ratio keeps improving.

From an information theory perspective, this makes sense. Better compression means better understanding of the underlying patterns. But information theory doesn't tell us about economic value. If understanding is just better compression, and compression keeps improving, what happens to the companies building these models? The [cost of training frontier models has collapsed](https://www.semianalysis.com/p/deepseek-debates). DeepSeek proved that anyone with $500 million can build a competitive model. That's still a high barrier, but it's not a moat. When OpenAI launched ChatGPT in November 2022, it had a massive quality advantage. Today, dozens of models cluster around similar performance. The compression is getting better. The competitive advantage is getting smaller.


Neuroscientists are using AI models to study how brains work. Doris Tsao at UC Berkeley, who decoded how macaque monkeys perceive faces, says the advances in machine learning have taught us more about intelligence than neuroscience discovered in the past hundred years. Jonathan Cohen at Princeton argues that large language models mirror the neocortex, the part of the brain responsible for higher-order thinking. Kenneth Norman at Princeton calls working AI systems that instantiate theories of human intelligence the dream of cognitive neuroscience.

This convergence between AI and neuroscience is real. But what does it mean for business? If AI models work like brains, does that create defensible advantages? The evidence suggests the opposite. [OpenAI's API pricing has dropped by 97% since GPT-3's launch](https://techcrunch.com/2025/08/08/openai-priced-gpt-5-so-low-it-may-spark-a-price-war/). Every year brings an order of magnitude decline in the price of a given output. The models are getting more brain-like. The prices are collapsing toward marginal cost. That's what happens when capabilities commoditize.



The commoditization is visible in the data. [GPT-4 launched in March 2023](https://openai.com/index/gpt-4-research/) with a substantial lead. Within six months, [Claude 2 was comparable](https://www.anthropic.com/news/claude-2). Within a year, multiple models clustered around similar capability. The diffusion is fast. When you plot benchmark scores on standard evaluations like MMLU and HumanEval, leaders change weekly. The gaps are small. Consumer awareness doesn't track model quality. ChatGPT dominates with [over 700 million weekly active users](https://openai.com/index/how-people-are-using-chatgpt/) not because it has the best model anymore, but because it got there first and built brand.

This pattern suggests that brain-like processing, even if real, doesn't create lasting competitive advantages. The models are getting smarter. They're also getting cheaper and more interchangeable. If understanding is compression, and compression keeps improving, the value flows to whoever owns customer relationships and distribution, not to whoever has the best compression algorithm.

Douglas Hofstadter's framework helps explain why. He argues that cognition is recognition. Thinking is seeing one thing as another. You see a patch of color as a car. You see a chess position as similar to games you've played before. You see a meeting as an emperor-has-no-clothes situation. For Hofstadter, that's intelligence in a nutshell. Large language models appear to have this seeing-as machine at their core. They represent words as coordinates in high-dimensional space. Words that appear together get nudged closer. This creates dense representations where analogy becomes geometry. Take the vector for Paris, subtract France, add Italy, and you get Rome.

But here's the economic question: does this create moats or is it just better pattern matching? [Recent research suggests LLMs may rely more on statistical correlations than true reasoning](https://arxiv.org/abs/2308.03762). If it's mostly pattern-matching, even very sophisticated pattern-matching, they still need massive datasets and we're back to conventional network effects. If they can actually reason over conceptual spaces, that's different. But the evidence so far suggests the pattern-matching view is closer to reality. Current LLMs still fail in characteristic ways on tasks requiring actual causal reasoning, spatial reasoning, or planning over extended horizons. They're getting better, but the improvement curve on these specific capabilities looks different from the improvement curve on language modeling.

{{< img src="learning_efficiency1.png" alt="Human vs AI Learning Efficiency: Data Requirements Comparison" width="80%" >}}

The efficiency gap is telling. GPT-4 was exposed to trillions of words in training. Children need only a few million to become fluent. Human babies have inductive biases that accelerate learning. They expect the world is made of objects. They expect other beings have beliefs and intentions. When a parent says banana, an infant connects that word to the entire yellow object, not just its tip or peel. Infants perform experiments. They're motivated by emotions. Their learning is efficient because it's embodied, adaptive, deliberate, and continuous.

AI's experience, in comparison, is impoverished. Large language models are trained on data that's already extraordinarily refined. Language is like experience pre-chewed. Other kinds of data are less dense with meaning. Vision models still struggle with common-sense reasoning about physics. When an LLM is given a virtual walk-through of a building and then asked questions about routes and shortcuts, it tends to fail or hallucinate nonexistent paths. The models are getting better at language. They're not getting proportionally better at understanding the physical world.

This efficiency gap has economic implications. If human learning is orders of magnitude more efficient, what does that mean for AI's economic viability? The models require massive compute and data. The outputs are getting cheaper. The training costs are enormous. [Microsoft, Google, Amazon, and Meta will invest roughly $400 billion in AI infrastructure in 2025](https://techblog.comsoc.org/2025/11/01/ai-spending-boom-accelerates-big-tech-to-invest-invest-an-aggregate-of-400-billion-in-2025-more-in-2026/), more than global telecommunications capex. Microsoft now spends over 30% of revenue on capex, double what Verizon spends. What has this produced? Models that are simultaneously more capable and less defensible.

{{< img src="investment_vs_capability1.png" alt="AI Investment vs Model Capability Gains Over Time" width="80%" >}}

The investment reality is stark. The hyperscalers are spending historic amounts. The models are clustering around similar performance. The prices are collapsing. When GPT-5 came out in August 2025, it was a merely incremental improvement, and so profound a disappointment that it threatened to pop the AI investment bubble. The moment demands a middle kind of skepticism: one that takes today's AI models seriously without believing that there are no hard problems left.

The neuroscience insights are real. The models do appear to work in brain-like ways. Compression does seem to create understanding. But the economic value flows elsewhere. If models are commodities or near-commodities, model providers have little market power. The value flows to the users of AI, not the providers. Engineering firms using AI for materials development capture value through better materials. Pharmaceutical companies using AI for drug discovery capture value through better drugs. Retailers using AI for inventory management capture value through better margins. The AI providers compete with each other to offer the capability at the lowest price.

This is basic microeconomics. You capture value when you have market power, either through monopoly, through differentiation, or through control of a scarce input. If models are commodities, model providers have none of these. The counterargument is vertical integration. Maybe the hyperscalers that control cloud infrastructure plus model development plus customer relationships can capture value even if models themselves commoditize. This is more plausible, essentially the AWS playbook. Amazon didn't make money by having the best database. They made money by owning the infrastructure, the customer relationships, and the entire stack from hardware to application platform.

So where does this leave us? The technology exists and the underlying capabilities are real. The neuroscience insights are genuine. But the current evidence points toward a world where value flows to applications and customer relationships, and where the $400 billion the hyperscalers are spending buys them competitive positioning rather than monopoly. The models are thinking, or something close to it. That doesn't mean the companies building them will capture the economic value. Understanding may be compression. Compression may be getting better. But in competitive markets, better compression becomes cheaper compression. The thinking is real. The moats are not.

