---
title: "Is AI Really Eating the World?"
date: 2025-11-21
images: ['https://static.philippdubach.com/ograph/ograph-eatingtheworld.jpg']
external_url: "https://www.ben-evans.com/presentations/"
description: "We go through Benedict Evans' analysis of generative AI as a potential platform shift, exploring the massive $400 billion investment by tech giants, the commoditization of AI models, and the uncertainty around whether AI will follow traditional technology adoption patterns or break the cycle entirely."
keywords: ["Benedict Evans", "generative AI", "platform shift", "ChatGPT", "AI investment", "hyperscalers", "AI commoditization", "OpenAI", "AI adoption", "enterprise AI", "AI infrastructure", "technology cycles", "AI automation", "LLM deployment", "AI transformation"]
draft: true
---
In November 2025, Benedict Evans gave two presentations on generative AI, three years after ChatGPT's launch. His core argument remains uncomfortable: we know this matters, but we don't know how. I'm interested in Evans's work because he applies pattern recognition from previous platform shifts to a technology that may or may not follow those patterns.

Evans structures technology history in cycles. Every 10-15 years, the industry reorganizes around a new platform: mainframes (1960s-70s), PCs (1980s), web (1990s), smartphones (2000s-2010s). Each shift pulls all innovation, investment, and company creation into its orbit. Generative AI appears to be the next platform shift. But there's a second possibility: this could break the cycle entirely. The range of outcomes spans from "just more software" to a single unified intelligence that handles everything. The industry doesn't know which.

The hyperscalers are spending unprecedented amounts. In 2025, [Microsoft, Google, Amazon, and Meta will invest roughly $400 billion](https://www.axios.com/2024/11/01/big-tech-capital-spending-ai) in AI infrastructure, more than global telecommunications capex. Microsoft now spends over 30% of revenue on capex, double what Verizon spends. What has this produced? Models that increasingly look like commodities. When ChatGPT launched, OpenAI had a massive quality advantage. Today, dozens of models cluster around similar performance. [DeepSeek proved that anyone with $500 million can build a frontier model](https://www.reuters.com/technology/artificial-intelligence/chinas-deepseek-shows-ai-models-can-be-trained-fraction-cost-2024-01-10/). Half a billion isn't nothing, but it's within reach for large corporations.

Meanwhile, costs have collapsed. Every year brings an order of magnitude decline in the price of a given output. Meta gives comparable models away. Apple wants them to run locally on phones. The engineering achievement is real. The moat isn't obvious. Evans includes a chart showing model scores versus consumer awareness. ChatGPT dominates both. Perplexity, beloved in tech circles, barely registers.

The core uncertainty: will this keep getting better if we keep making it bigger? Transformers worked because we made them vastly larger than anyone thought feasible. But we don't have a theory for why it works. Recent stories suggest progress may be slowing, though this may just reflect practical difficulties: not enough GPUs, not enough power, not enough training data.

> "The risk of under-investing is significantly greater than the risk of over-investing." — Sundar Pichai, Q2 2024

> "The very worst case would be that we have just pre-built for a couple of years." — Mark Zuckerberg, Q3 2025

This is FOMO as corporate strategy, backed by $400 billion annually. [Nvidia is the immediate beneficiary, now doing $57 billion quarterly revenue](https://www.cnbc.com/2025/01/29/nvidia-nvda-earnings-report-q4-2025.html). The entire industry is supply-constrained by semiconductor fab capacity and power grid access. US power demand grows at roughly 2% annually. AI might add another 1%, but building that power takes years.

Evans uses an extended metaphor: automation that works disappears. In the 1950s, automatic elevators were AI. Today they're just elevators. AI, Larry Tesler noted in 1970, is whatever machines can't do yet. Once it works, it's just software. The question: will LLMs follow this pattern, or is this different?

Current deployment shows clear winners. Software development has seen massive adoption. Marketing has found immediate uses: generating ad assets at scale, making video accessible to companies that couldn't afford it before. Customer support has attracted investment, though with the caveat that LLMs produce plausible answers, not necessarily correct ones.

Beyond these, adoption looks scattered. [Deloitte surveys from June 2025 show that roughly 10% of US consumers use generative AI chatbots daily](https://www2.deloitte.com/us/en/insights/topics/emerging-technologies/state-of-generative-ai.html). Another 20% use them weekly or monthly. Enterprise deployment is further behind. McKinsey data shows that most AI "agents" remain in pilot or experimental stages. A quarter of CIOs have launched something. Forty percent don't expect production deployment until 2026 or later.

Evans notes that [cloud adoption took 20 years to reach 30% of enterprise workloads](https://www.goldmansachs.com/insights/articles/AI-poised-to-drive-160-increase-in-power-demand). It's still growing. New technology always takes longer than advocates expect.

The analogy Evans finds most useful is spreadsheets. VisiCalc in the late 1970s transformed accounting. If you were an accountant, you had to have it. If you were a lawyer, you thought "that's nice for my accountant." ChatGPT today has the same dynamic. Certain people with certain jobs find it immediately transformative. Everyone else sees a demo and doesn't know what to do with the blank prompt.

> "People don't know what they want until you show it to them. You've got to start with the experience and work backwards to the technology." — Steve Jobs

The standard pattern for deploying technology: first, absorb it (make it a feature, automate obvious tasks). Second, innovate (create new products, unbundle incumbents). Third, disrupt (redefine what the market is). We're mostly in stage one. Stage two is happening in pockets. Startups are attacking specific enterprise problems: converting COBOL to Java, reconfiguring telco billing systems. [Y Combinator's recent batches are overwhelmingly AI-focused](https://www.ycombinator.com/companies), betting on thousands of new companies unbundling existing software. Stage three remains speculative.

LLMs are probabilistic. They produce plausible outputs, not deterministic answers. Air Canada deployed a customer support chatbot that gave customers a generous refund policy, unfortunately not Air Canada's actual policy. [A judge ruled in the customer's favor](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know). This creates design problems. Where do you put the human in the loop? What tasks can tolerate error rates? We have no indication error rates are going away.

The automation question: do you do the same work with fewer people, or more work with the same people? History suggests the latter. Steam engines in 19th century Britain provided labor equivalent to roughly five times the total UK population by 1900. They didn't eliminate jobs. They changed what people did and expanded what was possible. The companies whose competitive advantage was "we can afford to hire enough people to do this" face pressure. The companies whose advantage was unique data, unique customer relationships, or unique distribution may get stronger.

All current recommendation systems work by capturing and analyzing user behavior. What if LLMs can bypass this? Instead of needing 100 million users to train the recommendation algorithm, what if the LLM can reason from concept rather than correlation? If this works, it unbundles the network effect. You don't need a billion users to build good recommendations. Evans frames this as a question, not a prediction.

OpenAI announced commitments for 30+ gigawatts of datacenter capacity at $1.4 trillion, with an aspiration for 1 GW/week of new construction at $20 billion per gigawatt. These numbers strain credulity. They also may be someone's actual plan. Evans includes reporting about "circular revenue": money flowing from hyperscalers to Nvidia to OpenAI and back, with equity stakes and capacity commitments layered throughout.

Evans shows a scatter plot of model benchmark scores. Every few weeks, someone releases a new model that tops the leaderboard briefly. Leaders change weekly. The gaps are small. Meanwhile, consumer awareness doesn't track model quality. ChatGPT dominates brand recognition. If models are commodities, value moves up the stack to product design, distribution, vertical integration, and customer relationships. This is the SaaS pattern. Databases became commodities. Value moved to applications.

Evans consistently ends presentations with questions rather than conclusions. How far does scaling continue? Where do moats develop if models are commodities? What tasks benefit from probabilistic automation despite error rates? How quickly do enterprises deploy this? What gets unbundled? Does this change recommendation systems and discovery, or just make existing approaches more efficient?

The technology exists. The capabilities are real. The capital is committed. What we don't have is a map from here to there. Evans's contribution is naming that uncertainty explicitly while identifying the patterns that might help us understand what happens next. Whether this becomes "just software" or something fundamentally different remains to be seen. Either outcome would matter enormously.