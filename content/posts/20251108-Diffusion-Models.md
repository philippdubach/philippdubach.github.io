---
title: "Working with Models"
date: 2025-11-08
images: ['https://static.philippdubach.com/ograph/ograph-models.jpg']
external_url: "https://arxiv.org/abs/2510.21890"
description: "We explore diffusion models and their mathematical foundations, referencing a comprehensive monograph that traces their origins and core principles, along with recommended learning resources including Stanford's CS236 Deep Generative Models course."
keywords: ["diffusion models", "mathematical foundations", "CS236 course", "Stefano Ermon", "deep generative models", "Stanford course", "forward process", "data distribution", "noise modeling", "machine learning", "generative modeling", "YouTube lectures", "model development", "data corruption", "prior distribution"]
draft: false
---

There was this "[I work with Models](https://us1.discourse-cdn.com/flex001/uploads/ultralytics1/original/1X/45c604467b6f4212858281cf28f71a77083fb45e.jpeg)" joke which I first heard years ago from an analyst working on a valuation model ([see my previous post](/2025/10/19/everything-is-a-dcf-model/)). I guess it has become more relevant than ever:

>This monograph presents the core principles that have guided the development of diffusion models, tracing their origins and showing how diverse formulations arise from shared mathematical ideas. Diffusion modeling starts by defining a forward process that gradually corrupts data into noise, linking the data distribution to a simple prior through a continuum of intermediate distributions.

If you want to get into this topic in the first place, be sure to check out [Stefano Ermon's CS236 Deep Generative Models Course](https://deepgenerativemodels.github.io). Lecture recordings of the full course can also be found on [YouTube](https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8).

_Original paper linked in this post's title._