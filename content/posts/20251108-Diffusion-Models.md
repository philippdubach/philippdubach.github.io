---
title: "Working with Models"
date: 2025-11-08
images: ['https://static.philippdubach.com/ograph/ograph-models.jpg']
external_url: "https://arxiv.org/abs/2510.21890"
description: "A blog post discussing diffusion models and their mathematical foundations, referencing a monograph that traces their origins and core principles, with recommended resources including Stanford's CS236 Deep Generative Models course."
keywords: ["diffusion models", "deep generative models", "machine learning models", "CS236 course", "Stefano Ermon", "Stanford course", "mathematical modeling", "data distribution", "forward process", "noise modeling", "generative AI", "deep learning", "model development", "artificial intelligence", "computer vision"]
draft: false
---

There was this "[I work with Models](https://us1.discourse-cdn.com/flex001/uploads/ultralytics1/original/1X/45c604467b6f4212858281cf28f71a77083fb45e.jpeg)" joke which I first heard years ago from an analyst working on a valuation model ([see my previous post](/2025/10/19/everything-is-a-dcf-model/)). I guess it has become more relevant than ever:

>This monograph presents the core principles that have guided the development of diffusion models, tracing their origins and showing how diverse formulations arise from shared mathematical ideas. Diffusion modeling starts by defining a forward process that gradually corrupts data into noise, linking the data distribution to a simple prior through a continuum of intermediate distributions.

If you want to get into this topic in the first place, be sure to check out [Stefano Ermon's CS236 Deep Generative Models Course](https://deepgenerativemodels.github.io). Lecture recordings of the full course can also be found on [YouTube](https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8).

_Original paper linked in this post's title._