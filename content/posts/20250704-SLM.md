---
title: NVIDIA Likes Small Language Models
date: 2025-07-04
images:
- https://static.philippdubach.com/ograph/ograph-post.jpg
seoTitle: "Small Language Models for Agentic AI: NVIDIA's SLM Case"
description: "NVIDIA's SLM research claims 40-70% of LLM tasks can shift to small language models. Here's what their cost analysis misses about real-world deployment."
keywords:
- small language models
- SLM vs LLM
- small language models agentic AI
- NVIDIA SLM research
- edge AI language models
external_url: https://arxiv.org/abs/2506.02153
categories:
- AI
type: Commentary
draft: false
aliases:
- /2025/07/04/nvidia-likes-small-language-models/

faq:
- question: Can small language models replace large language models for AI agent tasks?
  answer: According to NVIDIA's research, 40-70% of queries in popular open-source agents like MetaGPT and Open Operator could be handled by specialized SLMs under 10 billion parameters. The paper proposes heterogeneous agentic systems where SLMs handle routine, repetitive tasks while LLMs are reserved for complex reasoning that requires full generalist capabilities.
- question: What are the biggest limitations of small language models for agentic AI?
  answer: Context window constraints are the highest technical barrier. Modern agentic applications require substantial context, with system prompts alone consuming around 25,000 tokens. Most SLMs running on consumer hardware are capped at 32k or 128k contexts, and achieving reasonable inference speeds at these limits requires gaming-grade hardware with at least 8GB VRAM for a 7B model.
- question: Are small language models actually more cost-efficient than LLMs when you account for retries?
  answer: The efficiency picture is more complex than simple FLOP comparisons suggest. SLMs may require 3-4 attempts via multishot prompting for tasks that LLMs complete on the first try with 90% success rates. When you factor in failed attempts, orchestration overhead, and the infrastructure efficiency gap between optimized datacenters and consumer hardware, some SLM deployments may actually consume more total energy than centralized LLM inference.
- question: What is a small language model and how is it defined?
  answer: As of 2025, a small language model (SLM) is generally defined as a language model under 10 billion parameters that can fit onto common consumer electronic devices and perform inference with latency low enough for practical use. Notable examples include Microsoft's Phi series, NVIDIA's Nemotron-H family, and Hugging Face's SmolLM2, which achieve performance comparable to much larger models while being 10-30x more efficient.
---
>A Small Language Model (SLM) is a LM that can fit onto a common consumer electronic device and perform inference with latency sufficiently low to be practical when serving the agentic requests of one user. [...] We note that as of 2025, we would be comfortable with considering most models below 10bn parameters in size to be SLMs.

The [(NVIDIA) researchers](https://research.nvidia.com/labs/lpr/slm-agents/) argue that most agentic applications perform repetitive, specialized tasks that don't require the full generalist capabilities of LLMs. They propose heterogeneous agentic systems where SLMs handle most tasks while LLMs are used selectively for complex reasoning. They present three main arguments: (1) SLMs are sufficiently powerful for agentic tasks, as demonstrated by recent models like [Microsoft's Phi series](https://azure.microsoft.com/en-us/products/phi), [NVIDIA's Nemotron-H family](https://research.nvidia.com/labs/adlr/nemotronh/), and [Hugging Face's SmolLM2 series](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9), which achieve comparable performance to much larger models [while being 10-30x more efficient](https://arxiv.org/abs/2501.05465). (2) SLMs are inherently more operationally suitable for agentic systems due to their faster inference, lower latency, and ability to run on edge devices. (3) SLMs are necessarily more economical, offering significant cost savings in inference, fine-tuning, and deployment.

The paper addresses counterarguments about LLMs' superior language understanding and centralization benefits with studies (see Appendix B: _LLM-to-SLM Replacement Case Studies_) showing that 40-70% of LLM queries in popular open-source agents (MetaGPT, Open Operator, Cradle) could be replaced by specialized SLMs. One comment I read [raised important concerns](https://news.ycombinator.com/item?id=44432478) about the paper's analysis, particularly regarding context window which are arguably the highest technical barrier to SLM adoption in agentic systems. Modern agentic applications require substantial context: Claude 4 Sonnet's system prompt alone [reportedly uses around 25k tokens](https://simonwillison.net/2025/May/25/claude-4-system-prompt/), and a typical coding agent needs system instructions, tool definitions, file context, and project documentation, totaling 5-10k tokens before any actual work begins. Most SLMs that can run on consumer hardware are capped at 32k or 128k contexts architecturally, but achieving reasonable inference speeds at these limits requires gaming hardware (8GB VRAM for a 7b model at 128k context).

{{< readnext slug="the-most-expensive-assumption-in-ai" >}}

The paper concludes that the shift to SLMs is inevitable due to economic and operational advantages, despite current barriers including infrastructure investment in LLM serving, generalist benchmark focus, and limited awareness of SLM capabilities. But the economic efficiency claims also face scrutiny under system-level analysis. In Section 3.2 they present simplistic FLOP comparisons while ignoring critical inefficiencies: the reliance on [multishot-prompting](https://www.promptingguide.ai/techniques/fewshot) where SLMs might require 3-4 attempts for tasks that LLMs complete with 90% success rate, task decomposition overhead that multiplies context setup costs and error rates, and infrastructure efficiency differences between optimized datacenters (PUE ratios near 1.1, >90% GPU utilization) and consumer hardware (5-10% GPU utilization, residential HVAC, 80-85% power conversion efficiency). When accounting for failed attempts, orchestration overhead, and infrastructure efficiency, many "economical" SLM deployments might actually consume more total energy than centralized LLM inference.

_(05/07/2025) Update: On the topic of speed I just came across [Ultra-Fast Language Models Based on Diffusion](https://arxiv.org/abs/2506.17298). You can also test it yourself using the [free playground link](https://chat.inceptionlabs.ai/), and it is in fact extremely fast. Try the "Diffusion Effect" in the top right corner which toggles an interesting visualization. I'm not sure how realistic this is, it shows text appearing as random noise before gradually resolving into clear words; though the actual process likely involves tokens evolving from imprecise vectors in a multidimensional space toward more precise representations until they crystallize into specific words._

_(06/07/2025) Update II: Apparently there is also a [Google DeepMind Gemini Diffusion Model](https://deepmind.google/models/gemini-diffusion/)._
