<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Project on philippdubach</title><link>http://localhost:1313/tags/project/</link><description>Recent content in Project on philippdubach</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 06 Jul 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/project/index.xml" rel="self" type="application/rss+xml"/><item><title>Blackjack with Computer Vision</title><link>http://localhost:1313/2025/07/06/blackjack-with-computer-vision/</link><pubDate>Sun, 06 Jul 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/2025/07/06/blackjack-with-computer-vision/</guid><description>&lt;p>After installing &lt;a href="https://www.anthropic.com/claude-code">Claude Code&lt;/a>&lt;/p>
&lt;blockquote>
&lt;p>the agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster through natural language commands&lt;/p>&lt;/blockquote>
&lt;p>I was looking for a task to test its abilities. Fairly quickly we wrote &lt;a href="https://gist.github.com/philippdubach/741cbd56498e43375892966ca691b9c2">less than 200 lines of python code predicting black jack odds&lt;/a> using Monte Carlo Simulation. When I went on to test this little tool on &lt;a href="https://games.washingtonpost.com/games/blackjack">Washington Post&amp;rsquo;s&lt;/a> online Black Jack (I also didn&amp;rsquo;t know that existed!) I quickly noticed how impractical it was to manually input all the card values on the table manually. What if the tool would also automatically recognize the cards that are on the table and calculate the odds from it? I have never done anything with computer vision so this seemed like a good challenge.
&lt;picture style="display: block; width: 80%; margin: 0 auto; padding: 1rem 0;">
&lt;source media="(max-width: 768px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/classification.gif 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/classification.gif 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/classification.gif 640w"
sizes="80vw">
&lt;source media="(max-width: 1024px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/classification.gif 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/classification.gif 1024w"
sizes="80vw">
&lt;source media="(min-width: 1025px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/classification.gif 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/classification.gif 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/classification.gif 2000w"
sizes="80vw">
&lt;img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/classification.gif"
alt="alt text here"
class=""
loading="lazy"
style="width: 100%; height: auto; display: block;">
&lt;/picture>
To get to any reasonable result we have to start with classification where we &amp;ldquo;teach&amp;rdquo; the model to categorize data by showing them lots of examples with correct labels. But where do the labels come from? I manually annotated &lt;a href="https://universe.roboflow.com/cards-agurd/playing_card_classification">409 playing cards across 117 images&lt;/a> using Roboflow Annotate (at first I only did half as much - why this wasn&amp;rsquo;t a good idea we&amp;rsquo;ll see in a minute). Once enough screenshots of cards were annotated we can train the model to recognize the cards and predict card values on tables it has never seen before. I was able to use a &lt;a href="https://www.nvidia.com/en-us/data-center/tesla-t4/">NVIDIA T4 GPU&lt;/a> inside Google Colab which offers some GPU time for free when capacity is available.
&lt;picture style="display: block; width: 80%; margin: 0 auto; padding: 1rem 0;">
&lt;source media="(max-width: 768px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/gpu_setup_colab.png 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/gpu_setup_colab.png 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/gpu_setup_colab.png 640w"
sizes="80vw">
&lt;source media="(max-width: 1024px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/gpu_setup_colab.png 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/gpu_setup_colab.png 1024w"
sizes="80vw">
&lt;source media="(min-width: 1025px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/gpu_setup_colab.png 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/gpu_setup_colab.png 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/gpu_setup_colab.png 2000w"
sizes="80vw">
&lt;img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/gpu_setup_colab.png"
alt="alt text here"
class=""
loading="lazy"
style="width: 100%; height: auto; display: block;">
&lt;/picture>
During training, the algorithm learns patterns from this example data, adjusting its internal parameters millions of times until it gets really good at recognizing the differences between categories (in this case different cards). Once trained, the model can then make predictions on new, unseen data by applying the patterns it learned. With the annotated dataset ready, it was time to implement the actual computer vision model. I chose to run inference on &lt;a href="https://docs.ultralytics.com/de/models/yolo11/">Ultralytics&amp;rsquo; YOLOv11&lt;/a> pre-trained model, a state-of-the-art object detection algorithm. I set up the environment in Google Colab following the &lt;a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolo11-object-detection-on-custom-dataset.ipynb">&amp;ldquo;How to Train YOLO11 Object Detection on a Custom Dataset&amp;rdquo;&lt;/a> notebook. After extracting the annotated dataset from Roboflow, I began training the model using the pre-trained YOLOv11s weights as a starting point. This approach, called &lt;a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning&lt;/a>, allows the model to leverage patterns already learned from millions of general images and adapt them to this specific task.
I initially set it up to &lt;a href="https://docs.ultralytics.com/guides/model-training-tips/#other-techniques-to-consider-when-handling-a-large-dataset">run for 350 epochs&lt;/a>, though the model&amp;rsquo;s built-in early stopping mechanism kicked in after 242 epochs when no improvement was observed for 100 consecutive epochs. The best results were achieved at epoch 142, taking around 13 minutes to complete on the Tesla T4 GPU.
The initial results were quite promising, with an overall mean Average Precision (mAP) of 80.5% at IoU threshold 0.5. Most individual card classes achieved good precision and recall scores, with only a few cards like the 6 and Queen showing slightly lower precision values.
&lt;picture style="display: block; width: 80%; margin: 0 auto; padding: 1rem 0;">
&lt;source media="(max-width: 768px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/run1_results.png 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/run1_results.png 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/run1_results.png 640w"
sizes="80vw">
&lt;source media="(max-width: 1024px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/run1_results.png 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/run1_results.png 1024w"
sizes="80vw">
&lt;source media="(min-width: 1025px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/run1_results.png 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/run1_results.png 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/run1_results.png 2000w"
sizes="80vw">
&lt;img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/run1_results.png"
alt="Training results showing confusion matrix and loss curves"
class=""
loading="lazy"
style="width: 100%; height: auto; display: block;">
&lt;/picture>
However, looking at the confusion matrix and loss curves revealed some interesting patterns. While the model was learning effectively (as shown by the steadily decreasing loss), there were still some misclassifications between similar cards, particularly among the numbered cards. This highlighted exactly why I mentioned earlier that annotating only half the amount of data initially &amp;ldquo;wasn&amp;rsquo;t a good idea&amp;rdquo; - more training examples would likely improve these edge cases and reduce confusion between similar-looking cards. My first attempt at solving the remaining accuracy issues was to add another layer to the workflow by sending the detected cards to Anthropic&amp;rsquo;s Claude API for additional OCR processing.
&lt;picture style="display: block; width: 80%; margin: 0 auto; padding: 1rem 0;">
&lt;source media="(max-width: 768px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/claude_vision_workflow_results.png 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/claude_vision_workflow_results.png 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/claude_vision_workflow_results.png 640w"
sizes="80vw">
&lt;source media="(max-width: 1024px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/claude_vision_workflow_results.png 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/claude_vision_workflow_results.png 1024w"
sizes="80vw">
&lt;source media="(min-width: 1025px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/claude_vision_workflow_results.png 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/claude_vision_workflow_results.png 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/claude_vision_workflow_results.png 2000w"
sizes="80vw">
&lt;img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/claude_vision_workflow_results.png"
alt="Roboflow workflow with Claude API integration"
class=""
loading="lazy"
style="width: 100%; height: auto; display: block;">
&lt;/picture>
This hybrid approach was very effective - the combination of YOLO&amp;rsquo;s object detection to dynamically crop down the Black Jack table to individual cards with Claude&amp;rsquo;s advanced vision capabilities yielded 99.9% accuracy on the predicted cards. However, this solution came with a significant drawback: the additional API layer consumed valuable time and the large model&amp;rsquo;s processing overhead, making it impractical for real-time gameplay.
Seeking a faster solution, I implemented the same workflow &lt;a href="https://github.com/JaidedAI/EasyOCR">locally using easyOCR&lt;/a> instead. EasyOCR seems to be really good at extracting black text on white background but &lt;a href="https://stackoverflow.com/questions/68261703/how-to-improve-accuracy-prediction-for-easyocr">might struggle with everything else&lt;/a>. While it was able to correctly identify the card numbers when it detected them, it struggled to recognize around half of the cards in the first place - even when fed pre-cropped card images directly from the YOLO model. This inconsistency made it unreliable for the application.
Rather than continue band-aid solutions, I decided to go back and improve my dataset. I doubled the training data by adding another 60 screenshots with the same train/test split as before. More importantly, I went through all the previous annotations and fixed many of the bounding polygons. I noticed that several misidentifications were caused by the model detecting face-down dealer cards as valid cards, which happened because some annotations for face-up cards inadvertently included parts of the card backs next to them. The improved dataset and cleaned annotations delivered what I was hoping for: The confusion matrix now shows a much cleaner diagonal pattern, indicating that the model now correctly identifies most cards without the cross-contamination issues we saw earlier.
&lt;picture style="display: block; width: 80%; margin: 0 auto; padding: 1rem 0;">
&lt;source media="(max-width: 768px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/run_best.png 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/run_best.png 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/run_best.png 640w"
sizes="80vw">
&lt;source media="(max-width: 1024px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/run_best.png 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/run_best.png 1024w"
sizes="80vw">
&lt;source media="(min-width: 1025px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/run_best.png 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/run_best.png 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/run_best.png 2000w"
sizes="80vw">
&lt;img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/run_best.png"
alt="Final training results with improved dataset"
class=""
loading="lazy"
style="width: 100%; height: auto; display: block;">
&lt;/picture>
Both the training and validation losses converge smoothly without signs of overfitting, while the precision and recall metrics climb steadily to plateau near perfect scores. The mAP@50 reaches an impressive 99.5%. Most significantly, the confusion matrix now shows that the model has virtually eliminated false positives with background elements. The &amp;ldquo;background&amp;rdquo; column (rightmost) in the confusion matrix is now much cleaner, with only minimal misclassifications of actual cards as background noise.
&lt;picture style="display: block; width: 80%; margin: 0 auto; padding: 1rem 0;">
&lt;source media="(max-width: 768px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/local_run_interference_visual.png 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/local_run_interference_visual.png 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/local_run_interference_visual.png 640w"
sizes="80vw">
&lt;source media="(max-width: 1024px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/local_run_interference_visual.png 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/local_run_interference_visual.png 1024w"
sizes="80vw">
&lt;source media="(min-width: 1025px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/local_run_interference_visual.png 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/local_run_interference_visual.png 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/local_run_interference_visual.png 2000w"
sizes="80vw">
&lt;img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/local_run_interference_visual.png"
alt="Real-time blackjack card detection and odds calculation"
class=""
loading="lazy"
style="width: 100%; height: auto; display: block;">
&lt;/picture>
With the model trained and performing, it was time to deploy it and play some blackjack. Initially, I tested the system using &lt;a href="https://docs.roboflow.com/deploy/serverless-hosted-api-v2">Roboflow&amp;rsquo;s hosted API&lt;/a>, which took around 4 seconds per inference - far too slow for practical gameplay. However, running the model locally on my laptop dramatically improved performance, achieving inference times of less than 0.1 seconds per image (1.3ms preprocess, 45.5ms inference, 0.4ms postprocess per image). I then &lt;a href="https://python-mss.readthedocs.io/">integrated the model with MSS&lt;/a> to capture a real-time feed of my browser window. The system automatically overlays the detected cards with their predicted values and confidence scores
&lt;picture style="display: block; width: 80%; margin: 0 auto; padding: 1rem 0;">
&lt;source media="(max-width: 768px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/black_jack_odds_demo.gif 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/black_jack_odds_demo.gif 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/black_jack_odds_demo.gif 640w"
sizes="80vw">
&lt;source media="(max-width: 1024px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/black_jack_odds_demo.gif 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/black_jack_odds_demo.gif 1024w"
sizes="80vw">
&lt;source media="(min-width: 1025px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/black_jack_odds_demo.gif 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/black_jack_odds_demo.gif 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/black_jack_odds_demo.gif 2000w"
sizes="80vw">
&lt;img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/black_jack_odds_demo.gif"
alt="Overview of selected fitted curves"
class=""
loading="lazy"
style="width: 100%; height: auto; display: block;">
&lt;/picture>
The final implementation successfully combines the pieces: the computer vision model detects and identifies cards in real-time, feeds this information to the Monte Carlo simulation, and displays both the card recognition results and the calculated odds directly on screen - do not try this at your local (online) casino!&lt;/p></description></item><item><title>Modeling Glycemic Response with XGBoost</title><link>http://localhost:1313/2025/05/30/modeling-glycemic-response-with-xgboost/</link><pubDate>Fri, 30 May 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/2025/05/30/modeling-glycemic-response-with-xgboost/</guid><description>&lt;p>Earlier this year I wrote how &lt;a href="http://localhost:1313/2025/01/02/i-built-a-cgm-data-reader/">I built a CGM data reader&lt;/a> after wearing a continuous glucose monitor myself. Since I was already logging my macronutrients and learning more about molecular biology in an &lt;a href="https://ocw.mit.edu/courses/res-7-008-7-28x-molecular-biology/">MIT MOOC&lt;/a> I became curious if given a meal&amp;rsquo;s macronutrients (carbs, protein, fat) and some basic individual characteristics (age, BMI), these could serve as features in a regressor machine learning model to predict the curve parameters of the postprandial glucose curve (how my blood sugar levels change after eating). I came across a paper on &lt;a href="https://www.cell.com/cell/fulltext/S0092-8674(15)01481-6?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0092867415014816%3Fshowall%3Dtrue">Personalized Nutrition by Prediction of Glycemic Responses&lt;/a> which did exactly that. Unfortunately, neither the data nor the code were publicly available. And - I wanted to predict my &lt;em>own&lt;/em> glycemic response curve. So I decided to build my own model. In the process I wrote this &lt;a href="https://static.philippdubach.com/pdf/Modeling_Postprandial_Glycemic_Response_in_Non_Diabetic_Adults_Using_XGBRegressor.pdf">working paper&lt;/a>.
&lt;a href="https://static.philippdubach.com/pdf/Modeling_Postprandial_Glycemic_Response_in_Non_Diabetic_Adults_Using_XGBRegressor.pdf">
&lt;picture style="display: block; width: 80%; margin: 0 auto; padding: 1rem 0;">
&lt;source media="(max-width: 768px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/working_paper_overview.jpg 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/working_paper_overview.jpg 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/working_paper_overview.jpg 640w"
sizes="80vw">
&lt;source media="(max-width: 1024px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/working_paper_overview.jpg 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/working_paper_overview.jpg 1024w"
sizes="80vw">
&lt;source media="(min-width: 1025px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/working_paper_overview.jpg 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/working_paper_overview.jpg 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/working_paper_overview.jpg 2000w"
sizes="80vw">
&lt;img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/working_paper_overview.jpg"
alt="Overview of Working Paper Pages"
class=""
loading="lazy"
style="width: 100%; height: auto; display: block;">
&lt;/picture>&lt;/a>
The paper represents an exercise in applying machine learning techniques to medical applications. The methodologies employed were largely inspired by &lt;a href="https://www.cell.com/cell/fulltext/S0092-8674(15)01481-6?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0092867415014816%3Fshowall%3Dtrue">Zeevi et al.&lt;/a>&amp;rsquo;s approach. I quickly realized that training a model on my own data &lt;em>only&lt;/em> was not very promising if not impossible. To tackle this, I used the publicly available &lt;a href="https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2005143">Hall dataset&lt;/a> containing continuous glucose monitoring data from 57 adults, which I narrowed down to 112 standardized meals from 19 non-diabetic subjects with their respective glucose curve after the meal (full methodology in the paper).
&lt;picture style="display: block; width: 80%; margin: 0 auto; padding: 1rem 0;">
&lt;source media="(max-width: 768px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/cgm-workflow-graph.jpg 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/cgm-workflow-graph.jpg 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/cgm-workflow-graph.jpg 640w"
sizes="80vw">
&lt;source media="(max-width: 1024px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/cgm-workflow-graph.jpg 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/cgm-workflow-graph.jpg 1024w"
sizes="80vw">
&lt;source media="(min-width: 1025px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/cgm-workflow-graph.jpg 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/cgm-workflow-graph.jpg 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/cgm-workflow-graph.jpg 2000w"
sizes="80vw">
&lt;img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/cgm-workflow-graph.jpg"
alt="Overview of the CGM pipeline workflow"
class=""
loading="lazy"
style="width: 100%; height: auto; display: block;">
&lt;/picture>
Rather than trying to predict the entire glucose curve, I simplified the problem by fitting each postprandial response to a normalized Gaussian function. This gave me three key parameters to predict: amplitude (how high glucose rises), time-to-peak (when it peaks), and curve width (how long the response lasts).
&lt;picture style="display: block; width: 80%; margin: 0 auto; padding: 1rem 0;">
&lt;source media="(max-width: 768px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/cgm-fitted-curve-large1.jpg 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/cgm-fitted-curve-large1.jpg 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/cgm-fitted-curve-large1.jpg 640w"
sizes="80vw">
&lt;source media="(max-width: 1024px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/cgm-fitted-curve-large1.jpg 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/cgm-fitted-curve-large1.jpg 1024w"
sizes="80vw">
&lt;source media="(min-width: 1025px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/cgm-fitted-curve-large1.jpg 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/cgm-fitted-curve-large1.jpg 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/cgm-fitted-curve-large1.jpg 2000w"
sizes="80vw">
&lt;img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/cgm-fitted-curve-large1.jpg"
alt="Overview of single fitted curve of cgm measurements"
class=""
loading="lazy"
style="width: 100%; height: auto; display: block;">
&lt;/picture>
The Gaussian approximation worked surprisingly well for characterizing most glucose responses. While some curves fit better than others, the majority of postprandial responses were well-captured, though there&amp;rsquo;s clear variation between individuals and meals. Some responses were high amplitude, narrow width, while others are more gradual and prolonged.
&lt;picture style="display: block; width: 80%; margin: 0 auto; padding: 1rem 0;">
&lt;source media="(max-width: 768px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/example-fitted-cgm-measurements.jpg 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/example-fitted-cgm-measurements.jpg 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/example-fitted-cgm-measurements.jpg 640w"
sizes="80vw">
&lt;source media="(max-width: 1024px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/example-fitted-cgm-measurements.jpg 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/example-fitted-cgm-measurements.jpg 1024w"
sizes="80vw">
&lt;source media="(min-width: 1025px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/example-fitted-cgm-measurements.jpg 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/example-fitted-cgm-measurements.jpg 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/example-fitted-cgm-measurements.jpg 2000w"
sizes="80vw">
&lt;img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/example-fitted-cgm-measurements.jpg"
alt="Overview of selected fitted curves"
class=""
loading="lazy"
style="width: 100%; height: auto; display: block;">
&lt;/picture>
I then trained an XGBoost regressor with 27 engineered features including meal composition, participant characteristics, and interaction terms. XGBoost was chosen for its ability to handle mixed data types, built-in feature importance, and strong performance on tabular data. The pipeline included hyperparameter tuning with 5-fold cross-validation to optimize learning rate, tree depth, and regularization parameters. Rather than relying solely on basic meal macronutrients, I engineered features across multiple categories and implemented CGM statistical features calculated over different time windows (24-hour and 4-hour periods), including time-in-range and glucose variability metrics. Architecture wise, I trained three separate XGBoost regressors - one for each Gaussian parameter.&lt;/p></description></item><item><title>Trading on Market Sentiment</title><link>http://localhost:1313/2025/02/20/trading-on-market-sentiment/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/2025/02/20/trading-on-market-sentiment/</guid><description>&lt;p>&lt;em>This post is based in part on a 2022 presentation I gave for the &lt;a href="https://www.ft.com/content/3bd45acd-b323-3c6b-ba98-ac78b456f308">ICBS Student Investment Fund&lt;/a> and my seminar work at Imperial College London.&lt;/em>&lt;/p>
&lt;p>As we were looking for new investment strategies for our Macro Sentiment Trading team, OpenAI had just published their &lt;a href="https://platform.openai.com/docs/models/gpt-3-5-turbo">GPT-3.5 Model&lt;/a>. After first experiments with the model, we asked ourselves: How would large language models like GPT-3.5 perform in predicting sentiment in financial markets, where the signal-to-noise ratio is notoriously low? And could they potentially even outperform industry benchmarks at interpreting market sentiment from news headlines? The idea wasn&amp;rsquo;t entirely new. &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3389884">Studies&lt;/a> &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1702854">[2]&lt;/a> &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=685145">[3]&lt;/a> have shown that investor sentiment, extracted from news and social media, can forecast market movements. But most approaches rely on traditional NLP models or proprietary systems like &lt;a href="https://www.ravenpack.com">RavenPack&lt;/a>. With the recent advances in large language models, I wanted to test whether these more sophisticated models could provide a competitive edge in sentiment-based trading. Before looking at model selection, it&amp;rsquo;s worth understanding what makes trading on sentiment so challenging. News headlines present two fundamental problems that any robust system must address.
&lt;picture style="display: block; width: 80%; margin: 0 auto; padding: 1rem 0;">
&lt;source media="(max-width: 768px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/news-relevance-timeline.jpg 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/news-relevance-timeline.jpg 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/news-relevance-timeline.jpg 640w"
sizes="80vw">
&lt;source media="(max-width: 1024px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/news-relevance-timeline.jpg 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/news-relevance-timeline.jpg 1024w"
sizes="80vw">
&lt;source media="(min-width: 1025px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/news-relevance-timeline.jpg 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/news-relevance-timeline.jpg 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/news-relevance-timeline.jpg 2000w"
sizes="80vw">
&lt;img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/news-relevance-timeline.jpg"
alt="Relative frequency of monthly Google News Search terms over 5 years. Numbers represent search interest relative to highest point. A value of 100 is the peak popularity for the term."
class=""
loading="lazy"
style="width: 100%; height: auto; display: block;">
&lt;/picture>
First, headlines are inherently non-stationary. Unlike other data sources, news reflects the constantly shifting landscape of global events, political climates, economic trends, etc. A model trained on COVID-19 vaccine headlines from 2020 might struggle with geopolitical tensions in 2023. This temporal drift means algorithms must be adaptive to maintain relevance.
&lt;picture style="display: block; width: 80%; margin: 0 auto; padding: 1rem 0;">
&lt;source media="(max-width: 768px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/headline-market-impact.jpg 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/headline-market-impact.jpg 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/headline-market-impact.jpg 640w"
sizes="80vw">
&lt;source media="(max-width: 1024px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/headline-market-impact.jpg 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/headline-market-impact.jpg 1024w"
sizes="80vw">
&lt;source media="(min-width: 1025px)"
srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/headline-market-impact.jpg 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/headline-market-impact.jpg 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/headline-market-impact.jpg 2000w"
sizes="80vw">
&lt;img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/headline-market-impact.jpg"
alt="Impact of headlines measured by subsequent index move (Data Source: Bloomberg)"
class=""
loading="lazy"
style="width: 100%; height: auto; display: block;">
&lt;/picture>
Second, the relationship between headlines and market impact is far from obvious. Consider these actual headlines from November 2020: &amp;ldquo;Pfizer Vaccine Prevents 90% of COVID Infections&amp;rdquo; drove the S&amp;amp;P 500 up 1.85%, while &amp;ldquo;Pfizer Says Safety Milestone Achieved&amp;rdquo; barely moved the market at -0.05%. The same company, similar positive news, dramatically different market reactions.&lt;/p></description></item><item><title>I Built a CGM Data Reader</title><link>http://localhost:1313/2025/01/02/i-built-a-cgm-data-reader/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/2025/01/02/i-built-a-cgm-data-reader/</guid><description>&lt;p>Last year I put a Continuous Glucose Monitor (CGM) sensor, specifically the &lt;a href="https://www.freestyle.abbott">Abbott Freestyle Libre 3&lt;/a>, on my left arm. Why? I wanted to optimize my nutrition for endurance cycling competitions. Where I live, the sensor is easy to get—without any medical prescription—and even easier to use. Unfortunately, Abbott&amp;rsquo;s &lt;a href="https://apps.apple.com/us/app/freestyle-librelink-us/id1325992472">FreeStyle LibreLink&lt;/a> app is less than optimal (3,250 other people with an average rating of 2.9/5.0 seem to agree). In their defense, the web app LibreView does offer some nice reports which can be generated as PDFs—not very dynamic, but still something! What I had in mind was more in the fashion of the &lt;a href="https://ultrahuman.com/m1">Ultrahuman M1 dashboard&lt;/a>. Unfortunately, I wasn&amp;rsquo;t allowed to use my Libre sensor (EU firmware) with their app (yes, I spoke to customer service).&lt;/p></description></item><item><title>Crypto Mean Reversion Trading</title><link>http://localhost:1313/2024/11/11/crypto-mean-reversion-trading/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/2024/11/11/crypto-mean-reversion-trading/</guid><description>&lt;p>In late 2021, Lars Kaiser&amp;rsquo;s paper on &lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S1544612318304513">seasonality in cryptocurrencies&lt;/a> inspired me to use my &lt;a href="https://docs.kraken.com/api/">Kraken API Key&lt;/a> to try and make some money. A quick summary of the paper: (1) Kaiser analyzes seasonality patterns across 10 cryptocurrencies (Bitcoin, Ethereum, etc.), examining returns, volatility, trading volume, and spreads (2) Finds no consistent calendar effects in cryptocurrency returns, supporting weak-form market efficiency (3) Observes robust patterns in trading activity - lower volume, volatility, and spreads in January, weekends, and summer months (4) Documents significant impact of January 2018 market sell-off on seasonality patterns (5) Reports a &amp;ldquo;reverse Monday effect&amp;rdquo; for Bitcoin (positive Monday returns) and &amp;ldquo;reverse January effect&amp;rdquo; (negative January returns) (6) Trading activity patterns suggest crypto markets are dominated by retail rather than institutional investors.&lt;/p></description></item><item><title>My First 'Optimal' Portfolio</title><link>http://localhost:1313/2024/03/15/my-first-optimal-portfolio/</link><pubDate>Fri, 15 Mar 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/2024/03/15/my-first-optimal-portfolio/</guid><description>&lt;p>My introduction to quantitative portfolio optimization happened during my undergraduate years, inspired by Attilio Meucci&amp;rsquo;s &lt;a href="https://link.springer.com/book/10.1007/978-3-540-27904-4">Risk and Asset Allocation&lt;/a> and the convex optimization &lt;a href="https://web.stanford.edu/~boyd/teaching.html">teachings of Diamond and Boyd at Stanford&lt;/a>. With enthusiasm and perhaps more confidence than expertise, I created my first &amp;ldquo;optimal&amp;rdquo; portfolio. What struck me most was the disconnect between theory and accessibility. Modern Portfolio Theory had been established since 1990, yet the optimization tools remained largely locked behind proprietary software.&lt;/p></description></item><item><title>The Tech behind this Site</title><link>http://localhost:1313/2024/01/15/the-tech-behind-this-site/</link><pubDate>Mon, 15 Jan 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/2024/01/15/the-tech-behind-this-site/</guid><description>&lt;p>Similar to how Simon Willison describes his difficulties managing images for his &lt;a href="https://simonwillison.net/2024/Dec/22/link-blog/">approach to running a link blog&lt;/a> I found it hard to remain true to pure markdown syntax but have images embedded in a responsive way on this site.&lt;/p>
&lt;p>My current pipeline is as follows: I host my all my images in a R2 bucket and serve them from &lt;code>static.philippdubach.com&lt;/code>. I use Cloudflares&amp;rsquo;s image resizing CDN do I never have to worry about serving images in appropriate size or format. I basically just upload them with the highes possible quality and Cloudflare takes care of the rest.&lt;/p></description></item></channel></rss>