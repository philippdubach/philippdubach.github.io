<!doctype html><html lang=en-us><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta http-equiv=Content-Security-Policy content="default-src 'self'; script-src 'self' 'unsafe-inline' https://gc.zgo.at https://cdn.jsdelivr.net; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; font-src 'self' data:; connect-src 'self' https://philippdubach.goatcounter.com https://weekly-top-goatcounter-api.philippd.workers.dev https://newsletter-api.philippd.workers.dev https://gc.zgo.at https://pdub.click; frame-ancestors 'self'; base-uri 'self';"><link rel=preconnect href=https://gc.zgo.at crossorigin><link rel=preconnect href=https://static.philippdubach.com crossorigin><link rel=preconnect href=https://pdub.click crossorigin><link rel=preconnect href=https://newsletter-api.philippd.workers.dev crossorigin><link rel=dns-prefetch href=https://gc.zgo.at><link rel=dns-prefetch href=https://static.philippdubach.com><link rel=dns-prefetch href=https://pdub.click><link rel=dns-prefetch href=https://newsletter-api.philippd.workers.dev><meta name=robots content="index, follow"><title>AI Models as Standalone P&amp;Ls - philippdubach.com</title><meta name=description content="Examining OpenAI's $11.5B quarterly losses and Anthropic's framework for treating each AI model as an independent profitable business unit."><meta name=keywords content="AI model profitability,OpenAI losses,Dario Amodei,Anthropic CEO,AI business models,model training costs,AI scaling laws,open-source AI competition,AI revenue models,machine learning economics,AI investment returns,model deployment costs,AGI development,AI company valuation,artificial intelligence P&amp;L"><meta name=author content="Philipp Dubach"><meta name=generator content="Hugo 0.149.0"><link rel=canonical href=http://localhost:1313/2025/11/09/ai-models-as-standalone-pls/><style>*{margin:0;padding:0;box-sizing:border-box}.visually-hidden{position:absolute;width:1px;height:1px;padding:0;margin:-1px;overflow:hidden;clip:rect(0,0,0,0);white-space:nowrap;border:0}.skip-link{position:absolute;top:-100%;left:50%;transform:translateX(-50%);background:#333;color:#fff;padding:.75rem 1.5rem;text-decoration:none;font-weight:500;z-index:10000;border-radius:0 0 4px 4px;transition:top .2s ease-in-out}.skip-link:focus{top:0;outline:2px solid #007acc;outline-offset:2px}a:focus-visible,button:focus-visible,input:focus-visible,textarea:focus-visible,select:focus-visible{outline:2px solid #007acc;outline-offset:2px}.post-content a:focus-visible,.archive-title a:focus-visible,.navigation a:focus-visible{outline:2px solid #007acc;outline-offset:2px;background-color:#e6f3ff}body{font-family:helvetica neue,Helvetica,Arial,segoe ui,sans-serif;padding-top:90px;line-height:1.6;color:#333;background-color:#ffff}blockquote{margin:1em 0;padding-left:1em;border-left:2px solid #aaa;color:#595959}blockquote p{margin:.5em 0}.container{display:flex;max-width:1200px;margin:0 auto;min-height:100vh}.sidebar{width:200px;min-width:200px;flex-shrink:0;background-color:#ffff;padding:1.5rem;border-right:0 solid #e9ecef;position:sticky;top:0;height:100vh;overflow-y:auto}.site-title a{text-decoration:none;color:#333;font-size:1.5rem;font-weight:700}.site-description{color:#595959;margin:.5rem 0 2rem;font-size:.9rem}.navigation ul{list-style:none;margin-bottom:2rem}.navigation li{margin-bottom:.5rem}.navigation a{text-decoration:none;color:#007acc;font-weight:500}.navigation a:hover{text-decoration:underline}.social-links a{display:inline-block;margin-right:1rem;color:#595959;text-decoration:none;font-size:.9rem}.social-links a:hover{color:#007acc}.content{flex:1;padding:2rem;padding-bottom:3rem;max-width:800px}.posts{max-width:90%}.post{margin-bottom:1rem;padding-bottom:1rem}.post:last-child{border-bottom:none}.post-title{margin-bottom:.5rem;line-height:1.3}.post-title a{text-decoration:none;color:#333;font-size:1.25rem;font-weight:600;line-height:1.3}.post-title a:hover{color:#007acc}.post-meta{font-size:.85rem;color:#595959;margin-bottom:0}.post-meta a{color:#007acc;text-decoration:none}.post-meta a:hover{text-decoration:underline}.post-content{line-height:1.7}.post-content p{margin-bottom:1rem}.post-content p:last-child{margin-bottom:.5rem}.post-content a{color:#007acc;text-decoration:none}.post-content a:hover{text-decoration:underline}.project-tag{background-color:#e9ecef;color:#495057;padding:2px 6px;border-radius:3px;font-size:9px;font-weight:500;text-transform:uppercase;letter-spacing:.3px;margin-left:6px;display:inline-block;vertical-align:middle;border:1px solid #dee2e6}.archive{max-width:90%}.year h2{margin:2rem 0 1rem;color:#333;font-size:1.5rem}.archive-item{display:flex;margin-bottom:.75rem;align-items:baseline}.archive-meta{width:60px;flex-shrink:0;font-size:.85rem;color:#595959}.archive-title{flex:1}.archive-title a{text-decoration:none;color:#333}.archive-title a:hover{color:#007acc}@supports(text-wrap:balance){.archive-title{text-wrap:balance}}.single .post-title{font-size:1.5rem;margin-bottom:1rem;line-height:1.3}.pagination{margin-top:2rem;margin-bottom:1rem;text-align:center;padding-top:2rem;padding-bottom:1rem;border-top:1px solid #e9ecef}.pagination a{color:#007acc;text-decoration:none;font-weight:500}.pagination a:hover{text-decoration:underline}.img-lightbox{cursor:pointer;transition:opacity .2s}.img-lightbox:hover{opacity:.9}a.lightbox-overlay,a.lightbox-overlay:link,a.lightbox-overlay:visited,a.lightbox-overlay:hover,a.lightbox-overlay:active,a.lightbox-overlay:focus{display:none;position:fixed;top:0;left:0;width:100%;height:100%;background-color:rgba(255,255,255,.85) !important;backdrop-filter:blur(4px);-webkit-backdrop-filter:blur(4px);z-index:9999;cursor:pointer;align-items:center;justify-content:center;padding:2rem;box-sizing:border-box;-webkit-tap-highlight-color:transparent;text-decoration:none !important;color:transparent !important;outline:none !important;border:none !important}.lightbox-overlay:target,a.lightbox-overlay:target{display:flex}.lightbox-overlay img{max-width:95%;max-height:95%;object-fit:contain;background:0 0;box-shadow:0 4px 20px rgba(0,0,0,.15);border-radius:4px}.feedback-footer{margin-top:.75rem;padding-top:.75rem;border-top:1px solid #e9ecef;text-align:center;color:#595959;font-size:.9rem;margin-bottom:2rem}.feedback-footer p{margin:0;line-height:1.6}.feedback-footer a{color:#007acc;text-decoration:none}.feedback-footer a:hover{text-decoration:underline}#newsletter-form-container{display:flex;flex-direction:column;align-items:center;margin:2rem 0}.newsletter-form{margin:0;display:flex;gap:.5rem;align-items:center;max-width:300px;width:100%}.form-group{flex:1;min-width:0}.newsletter-input{width:100%;padding:.5rem .75rem;font-size:.9rem;border:1px solid #e9ecef;border-radius:0;font-family:inherit;color:#333;background-color:#fff;transition:border-color .2s}.newsletter-input:focus{outline:none;border-color:#333}.newsletter-button{padding:.5rem 1rem;font-size:.9rem;background-color:transparent;color:#333;border:1px solid #333;border-radius:0;cursor:pointer;font-family:inherit;font-weight:400;white-space:nowrap;transition:background-color .2s,color .2s}.newsletter-button:hover:not(:disabled){background-color:#333;color:#fff}.newsletter-button:disabled{opacity:.5;cursor:not-allowed}.subscriber-count{margin:.75rem 0 0;font-size:.85rem;color:#595959;text-align:center}.newsletter-message{margin:1.5rem 0 0;padding:0;border-radius:0;line-height:1.6;text-align:center;max-width:300px;width:100%}.newsletter-message-success{background-color:transparent;color:#333;border:none}.newsletter-message-error{background-color:transparent;color:#595959;border:none}.newsletter-message a{color:#007acc;text-decoration:none}.newsletter-message a:hover{text-decoration:underline}.newsletter-archive{margin:2rem 0}.newsletter-list{list-style:none;padding:0}.newsletter-item{margin-bottom:1rem;padding-bottom:.75rem;border-bottom:1px solid #e9ecef}.newsletter-item:last-child{border-bottom:none}.newsletter-date{color:#595959;font-size:.9rem;margin-right:.5rem;min-width:120px;display:inline-block}.newsletter-item a{color:#333;text-decoration:none}.newsletter-item a:hover{color:#007acc;text-decoration:underline}@media screen and (max-width:768px){html{font-size:8px !important}body{font-size:1rem !important}.sidebar{width:90px !important;min-width:90px !important;padding:.75rem !important}.container{max-width:600px !important;margin:0 20px !important}.content{max-width:400px !important;padding:1rem !important;padding-bottom:.5rem !important;overflow-x:hidden !important;min-width:0 !important}.pagination{margin-bottom:1.5rem !important;padding-bottom:1.5rem !important}.project-tag{padding:1px 3px !important;border-radius:1.5px !important;font-size:4.5px !important;letter-spacing:.15px !important;margin-left:3px !important}.archive-meta{width:30px !important}.post-title,.post-title a,.single .post-title{line-height:1.2 !important}.post-content p:last-child{margin-bottom:.25rem !important}.feedback-footer{margin-top:.5rem !important;padding-top:.5rem !important;margin-bottom:1rem !important}#newsletter-form-container{margin:1rem 0 !important}.newsletter-form{max-width:200px !important}.newsletter-message{max-width:200px !important}.newsletter-date{display:block !important;margin-bottom:.25rem !important;min-width:auto !important}}code{font-family:sfmono-regular,Consolas,liberation mono,Menlo,Monaco,monospace;font-size:.875rem;line-height:1.5}p code,li code,td code,h1 code,h2 code,h3 code,h4 code,h5 code,h6 code{background-color:#f6f8fa;padding:.2em .4em;border-radius:4px;font-size:.875em;color:#24292f;word-break:break-word}pre{background-color:#f6f8fa;border-radius:6px;padding:0;margin:1rem 0;overflow:hidden}.highlight{margin:1rem 0;border-radius:6px;overflow:hidden;background-color:#f6f8fa;max-width:100%}.highlight pre{margin:0;background-color:transparent;border-radius:0;overflow-x:auto}.highlight table,.highlight .lntable{width:100%;border-collapse:collapse;border-spacing:0;padding:0;margin:0;border:0;display:table}.highlight table td,.highlight .lntd{padding:0;margin:0;border:0;vertical-align:top}.highlight .lntd:first-child{width:3.5rem;min-width:3.5rem;user-select:none;background-color:#ebedf0;border-right:1px solid #d0d7de}.highlight .lntd:first-child pre{padding:0;margin:0;overflow:hidden}.highlight .lntd:first-child pre code{display:block;padding:1rem 0}.highlight .lnt,.highlight .ln{display:block;padding:0 .75rem;color:#6e7781;text-align:right;font-size:inherit;line-height:inherit;white-space:pre;user-select:none}.highlight .lntd:last-child{overflow-x:auto}.highlight .lntd:last-child pre{padding:0;margin:0;overflow-x:auto}.highlight .lntd:last-child pre code{display:block;white-space:pre;word-wrap:normal;overflow-x:visible;padding:1rem}.chroma .line{display:block;padding:0;margin:0}.chroma .cl{display:block}.chroma .hl{background-color:#fff8c5}.bg{background-color:#f6f8fa}.chroma{background-color:#f6f8fa;color:#1f2328}.chroma .err{color:#cf222e;background-color:transparent}.chroma .k{color:#cf222e}.chroma .kc{color:#cf222e}.chroma .kd{color:#cf222e}.chroma .kn{color:#cf222e}.chroma .kp{color:#cf222e}.chroma .kr{color:#cf222e}.chroma .kt{color:#cf222e}.chroma .na{color:#1f2328}.chroma .nc{color:#1f2328}.chroma .no{color:#0550ae}.chroma .nd{color:#8250df}.chroma .ni{color:#6639ba}.chroma .nl{color:#0550ae;font-weight:700}.chroma .nn{color:#24292e}.chroma .nx{color:#1f2328}.chroma .nt{color:#116329}.chroma .nb{color:#6639ba}.chroma .bp{color:#6a737d}.chroma .nv{color:#953800}.chroma .vc{color:#953800}.chroma .vg{color:#953800}.chroma .vi{color:#953800}.chroma .vm{color:#953800}.chroma .nf{color:#8250df}.chroma .fm{color:#8250df}.chroma .s{color:#0a3069}.chroma .sa{color:#0a3069}.chroma .sb{color:#0a3069}.chroma .sc{color:#0a3069}.chroma .dl{color:#0a3069}.chroma .sd{color:#0a3069}.chroma .s2{color:#0a3069}.chroma .se{color:#0a3069}.chroma .sh{color:#0a3069}.chroma .si{color:#0a3069}.chroma .sx{color:#0a3069}.chroma .sr{color:#0a3069}.chroma .s1{color:#0a3069}.chroma .ss{color:#032f62}.chroma .m{color:#0550ae}.chroma .mb{color:#0550ae}.chroma .mf{color:#0550ae}.chroma .mh{color:#0550ae}.chroma .mi{color:#0550ae}.chroma .il{color:#0550ae}.chroma .mo{color:#0550ae}.chroma .o{color:#0550ae}.chroma .ow{color:#cf222e}.chroma .p{color:#1f2328}.chroma .c{color:#6e7781;font-style:italic}.chroma .ch{color:#6e7781;font-style:italic}.chroma .cm{color:#6e7781;font-style:italic}.chroma .c1{color:#6e7781;font-style:italic}.chroma .cs{color:#6e7781;font-style:italic}.chroma .cp{color:#6e7781}.chroma .cpf{color:#6e7781}.chroma .gd{color:#82071e;background-color:#ffebe9}.chroma .ge{font-style:italic}.chroma .gi{color:#116329;background-color:#dafbe1}.chroma .go{color:#1f2328}.chroma .gl{text-decoration:underline}.chroma .gs{font-weight:700}.chroma .w{color:#f6f8fa}@media screen and (max-width:768px){.highlight{max-width:100%;width:100%;overflow-x:auto}.highlight table,.highlight .lntable{width:auto;min-width:100%;table-layout:auto}.highlight .lntd:first-child{width:2.5rem;min-width:2.5rem}.highlight .lnt,.highlight .ln{padding:0 .5rem;font-size:inherit}code{font-size:.75rem}.highlight .lntd:first-child pre code{padding:.75rem 0}.highlight .lntd:last-child pre code{padding:.75rem}}</style><link rel=icon type=image/png href=/icons/favicon-96x96.png sizes=96x96><link rel=icon type=image/svg+xml href=/icons/favicon.svg><link rel="shortcut icon" href=/icons/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/icons/apple-touch-icon.png><meta name=apple-mobile-web-app-title content="philippdubach.com"><link rel=manifest href=/icons/site.webmanifest><meta name=theme-color content="#2c3e50"><meta name=msapplication-TileColor content="#2c3e50"><meta property="og:title" content="AI Models as Standalone P&amp;Ls"><meta property="og:description" content="Examining OpenAI's $11.5B quarterly losses and Anthropic's framework for treating each AI model as an independent profitable business unit."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/2025/11/09/ai-models-as-standalone-pls/"><meta property="og:site_name" content="philippdubach.com"><meta property="og:locale" content="en_US"><meta property="og:logo" content="http://localhost:1313/icons/favicon.ico"><meta property="og:image" content="https://static.philippdubach.com/ograph/ograph-aipnl.jpg"><meta property="og:image:secure_url" content="https://static.philippdubach.com/ograph/ograph-aipnl.jpg"><meta property="og:image:url" content="https://static.philippdubach.com/ograph/ograph-aipnl.jpg"><meta property="og:image:type" content="image/jpeg"><meta property="article:published_time" content="2025-11-09T00:00:00Z"><meta property="article:modified_time" content="2025-11-09T00:00:00Z"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="AI Models as Standalone P&amp;Ls"><meta name=twitter:description content="Examining OpenAI's $11.5B quarterly losses and Anthropic's framework for treating each AI model as an independent profitable business unit."><meta name=twitter:image content="https://static.philippdubach.com/ograph/ograph-aipnl.jpg"><meta name=twitter:image:src content="https://static.philippdubach.com/ograph/ograph-aipnl.jpg"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"http:\/\/localhost:1313\/"},{"@type":"ListItem","position":2,"name":"Posts","item":"http:\/\/localhost:1313\/posts/"},{"@type":"ListItem","position":3,"name":"AI Models as Standalone P\u0026Ls","item":"http:\/\/localhost:1313\/2025\/11\/09\/ai-models-as-standalone-pls\/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"AI Models as Standalone P\u0026Ls","name":"AI Models as Standalone P\u0026Ls","description":"Examining OpenAI\u0027s $11.5B quarterly losses and Anthropic\u0027s framework for treating each AI model as an independent profitable business unit.","keywords":["AI model profitability","OpenAI losses","Dario Amodei","Anthropic CEO","AI business models","model training costs","AI scaling laws","open-source AI competition","AI revenue models","machine learning economics","AI investment returns","model deployment costs","AGI development","AI company valuation","artificial intelligence P\u0026L"],"articleBody":"\" Microsoft reported earnings for the quarter ended Sept. [\\u0026hellip;] buried in its financial filings were a couple of passages suggesting that OpenAI suffered a net loss of $11.5 billion or more during the quarter.\\nFor every dollar of revenue, they\\u0026rsquo;re allegedly spending roughly $5 to deliver the product. What initially sounds like a joke about \\u0026ldquo;making it up on volume\\u0026rdquo; points to a more fundamental problem facing OpenAI and its competitors. AI companies are locked into continuously releasing more powerful (and expensive) models. If they stop, open-source alternatives will catch up and offer equivalent capabilities at substantially lower costs. This creates an uncomfortable dynamic. If your current model requires spending more than you earn just to fund the next generation, the path to profitability becomes unclear—perhaps impossible.\\nAnthropic CEO Dario Amodei (everybody\\u0026rsquo;s favorite AI CEO) recently offered a different perspective in a conversation with Stripe co-founder John Collison. He argues that treating each model as an independent business unit reveals a different picture than conventional accounting suggests.\\nLet\\u0026rsquo;s say in 2023, you train a model that costs $100 million, and then you deploy it in 2024 and it makes $200 million of revenue.\\nSo far, this looks profitable, a solid 2x return on the training investment. But here\\u0026rsquo;s where it gets complicated.\\nMeanwhile, because of the scaling laws, in 2024, you also train a model that costs $1 billion. If you look in a conventional way at the profit and loss of the company you\\u0026rsquo;ve lost $100 million the first year, you\\u0026rsquo;ve lost $800 million the second year, and you\\u0026rsquo;ve lost $8 billion in the third year, so it looks like it\\u0026rsquo;s getting worse and worse.\\nThe pattern continues:\\nIn 2025, you get $2 billion of revenue from that $1 billion model trained the previous year.\\nAgain, viewed in isolation, this model returned 2x its training cost.\\nAnd you spend $10 billion to train the model for the following year.\\nThe losses appear to accelerate dramatically, from $100 million to $800 million to $8 billion.\\nThis is where Amodei\\u0026rsquo;s reframing becomes interesting.\\nIf you consider each model to be a company, the model that was trained in 2023 was profitable. You paid $100 million and then it made $200 million of revenue.\\u0026quot;\\nHe also acknowledges there are inference costs (the actual computing expenses of running the model for users) but suggests these don\\u0026rsquo;t fundamentally change the picture in his simplified example. His core argument:\\nIf every model was a company, the model in this example is actually profitable. What\\u0026rsquo;s going on is that at the same time as you\\u0026rsquo;re reaping the benefits from one company, you\\u0026rsquo;re founding another company that\\u0026rsquo;s much more expensive and requires much more upfront R\\u0026amp;D investment.\\nThis is essentially an argument that AI companies are building a portfolio of profitable products, but the accounting makes it look terrible because each successive \\u0026ldquo;product\\u0026rdquo; costs 10x more than the last to develop. The losses stem from overlapping these profitable cycles while exponentially increasing investment scale. But this framework only works if two critical assumptions hold: (1) Each model consistently returns roughly 2x its training cost in revenue, and (2) The improvements from spending 10x more justify that investment—meaning customers will pay enough more for the better model to maintain that 2x return.\\nAmodei outlines two ways this resolves:\\nSo the way that it\\u0026rsquo;s going to shake out is this will keep going up until the numbers go very large and the models can\\u0026rsquo;t get larger, and, you know, then it\\u0026rsquo;ll be a large, very profitable business.\\nIn this first scenario, scaling hits physical or practical limits. You\\u0026rsquo;ve maxed out available compute, data, or capability improvements. Training costs plateau because you literally can\\u0026rsquo;t build a meaningfully larger model. At that point, companies stop needing exponentially larger investments and begin harvesting profits from their final-generation models. The second scenario is less optimistic:\\nOr at some point the models will stop getting better, right? The march to AGI will be halted for some reason.\\nIf the improvements stop delivering proportional returns before reaching natural limits, companies face what Amodei calls overhang.\\nAnd then perhaps there\\u0026rsquo;ll be some overhang, so there\\u0026rsquo;ll be a one-time, \\u0026lsquo;Oh man, we spent a lot of money and we didn\\u0026rsquo;t get anything for it,\\u0026rsquo; and then the business returns to whatever scale it was at.\\nWhat Amodei\\u0026rsquo;s framework doesn\\u0026rsquo;t directly address is the open-source problem. If training Model C costs $10 billion but open-source alternatives reach comparable performance six months later, that 2x return window might not materialize. The entire argument depends on maintaining a significant capability lead that customers will pay premium prices for. There\\u0026rsquo;s also the question of whether the 2x return assumption holds as models become more expensive. The jump from $100 million to $1 billion to $10 billion in training costs assumes that customers will consistently value the improvements enough to double revenue.\\n\"","wordCount":810,"inLanguage":"en","image":"https:\/\/static.philippdubach.com\/ograph\/ograph-aipnl.jpg","datePublished":"2025-11-09T00:00:00Z","dateModified":"2025-11-09T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/localhost:1313\/2025\/11\/09\/ai-models-as-standalone-pls\/"},"publisher":{"@type":"Organization","name":"philippdubach","logo":{"@type":"ImageObject","url":"http:\/\/localhost:1313\/icons/favicon.ico"}},"author":{"@type":"Person","name":"Philipp Dubach"}}</script></head><body class=linkblog><a href=#main-content class=skip-link>Skip to main content</a><div class=container><aside class=sidebar role=complementary aria-label="Site navigation and information"><div class=sidebar-content><div class=site-header><div class=site-title><a href=http://localhost:1313/ aria-label="Home - philippdubach">philippdubach</a></div><p class=site-description>Blog about Personal Projects, Articles and Papers on Economics, Finance and Technology</p></div><nav class=navigation aria-label="Main navigation"><ul><li><a href=/>Home</a></li><li><a href=/projects/>Projects</a></li><li><a href=/research/>Research</a></li><li><a href=/about/>About</a></li><li><a href=/posts/>Archive</a></li><li><a href=/subscribe>Subscribe</a></li></ul></nav><div class=social-links aria-label="Social media links"><a href=https://github.com/philippdubach target=_blank rel=noopener aria-label="GitHub profile (opens in new tab)">GitHub</a>
<a href=mailto:info@philippdubach.com aria-label="Send email">Email</a></div></div></aside><main id=main-content class=content role=main><article class="post single"><header class=post-header><h1 class=post-title>AI Models as Standalone P&amp;Ls</h1><div class=post-meta><time datetime=2025-11-09T00:00:00Z>November 9, 2025
</time>• 900 words
• 4 min read
• <a href=https://www.theregister.com/2025/10/29/microsoft_earnings_q1_26_openai_loss/ target=_blank rel=noopener>via</a>
• <a href=# id=share-link title="Copy short link" aria-label="Copy short link" style=text-decoration:none>∞</a>
<span id=share-status></span></div></header><div class=post-content><blockquote><p>Microsoft reported earnings for the quarter ended Sept. [&mldr;] buried in its financial filings were a couple of passages suggesting that OpenAI suffered a net loss of $11.5 billion or more during the quarter.</p></blockquote><p>For every dollar of revenue, they&rsquo;re allegedly spending roughly $5 to deliver the product. What initially sounds like a joke about &ldquo;making it up on volume&rdquo; points to a more fundamental problem facing OpenAI and its competitors. AI companies are locked into continuously releasing more powerful (and expensive) models. If they stop, <a href=https://arxiv.org/abs/2311.16989>open-source alternatives will catch up</a> and offer equivalent capabilities at substantially lower costs. This creates an uncomfortable dynamic. If your current model requires spending more than you earn just to fund the next generation, the path to profitability becomes unclear—perhaps impossible.</p><p>Anthropic CEO Dario Amodei (everybody&rsquo;s favorite AI CEO) recently offered a different perspective in a <a href="https://youtu.be/GcqQ1ebBqkc?si=sEDGAVBuZsjtLpZS&amp;t=1016">conversation with Stripe co-founder John Collison</a>. He argues that treating each model as an independent business unit reveals a different picture than conventional accounting suggests.</p><blockquote><p>Let&rsquo;s say in 2023, you train a model that costs $100 million, and then you deploy it in 2024 and it makes $200 million of revenue.</p></blockquote><p>So far, this looks profitable, a solid 2x return on the training investment. But here&rsquo;s where it gets complicated.</p><blockquote><p>Meanwhile, because of the scaling laws, in 2024, you also train a model that costs $1 billion. If you look in a conventional way at the profit and loss of the company you&rsquo;ve lost $100 million the first year, you&rsquo;ve lost $800 million the second year, and you&rsquo;ve lost $8 billion in the third year, so it looks like it&rsquo;s getting worse and worse.</p></blockquote><p>The pattern continues:</p><blockquote><p>In 2025, you get $2 billion of revenue from that $1 billion model trained the previous year.</p></blockquote><p>Again, viewed in isolation, this model returned 2x its training cost.</p><blockquote><p>And you spend $10 billion to train the model for the following year.</p></blockquote><p>The losses appear to accelerate dramatically, from $100 million to $800 million to $8 billion.</p><p>This is where Amodei&rsquo;s reframing becomes interesting.</p><blockquote><p>If you consider each model to be a company, the model that was trained in 2023 was profitable. You paid $100 million and then it made $200 million of revenue."</p></blockquote><p>He also acknowledges there are inference costs (the actual computing expenses of running the model for users) but suggests these don&rsquo;t fundamentally change the picture in his simplified example. His core argument:</p><blockquote><p>If every model was a company, the model in this example is actually profitable. What&rsquo;s going on is that at the same time as you&rsquo;re reaping the benefits from one company, you&rsquo;re founding another company that&rsquo;s much more expensive and requires much more upfront R&amp;D investment.</p></blockquote><p>This is essentially an argument that AI companies are building a portfolio of profitable products, but the accounting makes it look terrible because each successive &ldquo;product&rdquo; costs 10x more than the last to develop. The losses stem from overlapping these profitable cycles while exponentially increasing investment scale. But this framework only works if two critical assumptions hold: (1) Each model consistently returns roughly 2x its training cost in revenue, and (2) The improvements from spending 10x more justify that investment—meaning customers will pay enough more for the better model to maintain that 2x return.</p><p>Amodei outlines two ways this resolves:</p><blockquote><p>So the way that it&rsquo;s going to shake out is this will keep going up until the numbers go very large and the models can&rsquo;t get larger, and, you know, then it&rsquo;ll be a large, very profitable business.</p></blockquote><p>In this first scenario, scaling hits physical or practical limits. You&rsquo;ve maxed out available compute, data, or capability improvements. Training costs plateau because you literally can&rsquo;t build a meaningfully larger model. At that point, companies stop needing exponentially larger investments and begin harvesting profits from their final-generation models. The second scenario is less optimistic:</p><blockquote><p>Or at some point the models will stop getting better, right? The march to AGI will be halted for some reason.</p></blockquote><p>If the improvements stop delivering proportional returns before reaching natural limits, companies face what Amodei calls overhang.</p><blockquote><p>And then perhaps there&rsquo;ll be some overhang, so there&rsquo;ll be a one-time, &lsquo;Oh man, we spent a lot of money and we didn&rsquo;t get anything for it,&rsquo; and then the business returns to whatever scale it was at.</p></blockquote><p>What Amodei&rsquo;s framework doesn&rsquo;t directly address is the open-source problem. If training Model C costs $10 billion but open-source alternatives <a href=https://synaptic.com/resources/open-source-ai-2024>reach comparable performance six months later</a>, that 2x return window might not materialize. The entire argument depends on maintaining a significant capability lead that customers will pay premium prices for. There&rsquo;s also the question of whether the 2x return assumption holds as models become more expensive. The jump from $100 million to $1 billion to $10 billion in training costs assumes that customers will consistently value the improvements enough to double revenue.</p></div></article><footer class=feedback-footer><p>Have feedback, comments, or ideas? <a href=mailto:info@philippdubach.com>I'd love to hear from you</a>.</p><p class=latest-post>Latest: <a href=/2025/12/30/apples-ai-bet-playing-the-long-game-or-missing-the-moment/>Apple's AI Bet: Playing the Long Game or Missing the Moment?</a></p><p class=most-read id=top-post-week style=min-height:1.5em></p></footer><script>(function(){var e,n,t=document.getElementById("top-post-week");if(t&&fetch("https://weekly-top-goatcounter-api.philippd.workers.dev").then(function(e){return e.ok?e.json():Promise.reject()}).then(function(e){if(e.hits&&e.hits.length>0){var n=e.hits[0];t.innerHTML='Most read: <a href="'+n.path+'">'+n.title.replace(" - philippdubach.com","")+"</a>"}else t.remove()}).catch(function(){t.remove()}),n=document.getElementById("share-link"),e=document.getElementById("share-status"),!n)return;n.addEventListener("click",function(t){t.preventDefault(),e.textContent="...";var s,n=window.location.href;navigator.clipboard&&navigator.clipboard.write&&typeof ClipboardItem!="undefined"?(s=new ClipboardItem({"text/plain":fetch("https://pdub.click/yourls-api.php?signature=4807288869&action=shorturl&format=json&url="+encodeURIComponent(n)).then(function(e){return e.json()}).then(function(e){var t=e.shorturl||"";return new Blob([t],{type:"text/plain"})}).catch(function(){return new Blob([""],{type:"text/plain"})})}),navigator.clipboard.write([s]).then(function(){e.textContent="url copied",setTimeout(function(){e.textContent=""},2e3)}).catch(function(){e.textContent="error",setTimeout(function(){e.textContent=""},2e3)})):fetch("https://pdub.click/yourls-api.php?signature=4807288869&action=shorturl&format=json&url="+encodeURIComponent(n)).then(function(e){return e.json()}).then(function(t){var n=t.shorturl;n&&navigator.clipboard&&navigator.clipboard.writeText?navigator.clipboard.writeText(n).then(function(){e.textContent="url copied",setTimeout(function(){e.textContent=""},2e3)}).catch(function(){e.textContent=n,setTimeout(function(){e.textContent=""},4e3)}):n?(e.textContent=n,setTimeout(function(){e.textContent=""},4e3)):(e.textContent="error",setTimeout(function(){e.textContent=""},2e3))}).catch(function(){e.textContent="error",setTimeout(function(){e.textContent=""},2e3)})})})()</script></main></div></body></html>