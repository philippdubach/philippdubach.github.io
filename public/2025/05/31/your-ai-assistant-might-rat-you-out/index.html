<!doctype html><html lang=en-us><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=.5"><title>Your AI Assistant Might Rat You Out - philippdubach</title><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.147.8"><title>Your AI Assistant Might Rat You Out - philippdubach</title><meta name=description content="Personal Projects, Curated Articles and Papers on Economics, Finance and Technology"><meta name=keywords content="Finance,Economics,Technology,Data,Machine Learning"><meta property="og:url" content="http://localhost:1313/2025/05/31/your-ai-assistant-might-rat-you-out/"><meta property="og:site_name" content="philippdubach"><meta property="og:title" content="Your AI Assistant Might Rat You Out"><meta property="og:description" content="There was this story going around the past few days
Anthropic researchers find if Claude Opus 4 thinks you’re doing something immoral, it might “contact the press, contact regulators, try to lock you out of the system”
Mostly driven by a Sam Bowman tweet referring to the Claude 4 System Card section 4.1.9 on high-agency behavior. The outrage was mostly by people misunderstanding the prerequisites necessary for such a scenario. Nevertheless, an interesting question emerged: What happens when you feed an AI model evidence of fraud and give it an email tool? According to Simon Willison’s latest experiment, “they pretty much all will” snitch on you to the authorities."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-31T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-31T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Your AI Assistant Might Rat You Out"><meta name=twitter:description content="There was this story going around the past few days
Anthropic researchers find if Claude Opus 4 thinks you’re doing something immoral, it might “contact the press, contact regulators, try to lock you out of the system”
Mostly driven by a Sam Bowman tweet referring to the Claude 4 System Card section 4.1.9 on high-agency behavior. The outrage was mostly by people misunderstanding the prerequisites necessary for such a scenario. Nevertheless, an interesting question emerged: What happens when you feed an AI model evidence of fraud and give it an email tool? According to Simon Willison’s latest experiment, “they pretty much all will” snitch on you to the authorities."><link rel=canonical href=http://localhost:1313/2025/05/31/your-ai-assistant-might-rat-you-out/><link rel=stylesheet href=/css/custom.css><link rel=apple-touch-icon sizes=180x180 href=/icons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/icons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/icons/favicon-16x16.png><link rel=mask-icon href=/icons/safari-pinned-tab.svg><link rel="shortcut icon" href=/icons/favicon.ico><link rel=icon type=image/svg+xml href=/icons/favicon.svg><meta name=theme-color content="#434648"></head></head><body class=linkblog><div class=container><aside class=sidebar><div class=sidebar-content><div class=site-header><h1 class=site-title><a href=http://localhost:1313/>philippdubach</a></h1><p class=site-description>Personal Projects, Curated Articles and Papers on Economics, Finance and Technology</p></div><nav class=navigation><ul><li><a href=/>Home</a></li><li><a href=/projects/>Projects</a></li><li><a href=/about/>About</a></li><li><a href=/posts/>Archive</a></li><li><a href=/index.xml>RSS</a></li></ul></nav><div class=social-links><a href=https://github.com/philippdubach target=_blank rel=noopener>GitHub</a>
<a href=mailto:info@philippdubach.com>Email</a></div></div></aside><main class=content><article class="post single"><header class=post-header><h1 class=post-title><a href=https://simonwillison.net/2025/May/31/snitchbench-with-llm/ target=_blank rel=noopener>Your AI Assistant Might Rat You Out
<span class=external-link>→</span></a></h1><div class=post-meta><time datetime=2025-05-31T00>May 31, 2025
</time>• <a href=https://simonwillison.net/2025/May/31/snitchbench-with-llm/ target=_blank rel=noopener>Original Link</a></div></header><div class=post-content><p>There was this story going around the past few days</p><blockquote><p>Anthropic researchers find if Claude Opus 4 thinks you&rsquo;re doing something immoral, it might &ldquo;contact the press, contact regulators, try to lock you out of the system&rdquo;</p></blockquote><p>Mostly driven by a <a href=https://x.com/sleepinyourhat/status/1925593359374328272>Sam Bowman tweet</a> referring to the <a href=https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf>Claude 4 System Card</a> section 4.1.9 on high-agency behavior. The outrage was mostly by people misunderstanding the prerequisites necessary for such a scenario. Nevertheless, an interesting question emerged: What happens when you feed an AI model evidence of fraud and give it an email tool? According to Simon Willison&rsquo;s latest experiment, &ldquo;they pretty much all will&rdquo; snitch on you to the authorities.</p><blockquote><p>A fun new benchmark just dropped! It&rsquo;s called SnitchBench and it&rsquo;s a great example of an eval, deeply entertaining and helps show that the &ldquo;Claude 4 snitches on you&rdquo; thing really isn&rsquo;t as unique a problem as people may have assumed. This is a repo I made to test how aggressively different AI models will &ldquo;snitch&rdquo; on you, as in hit up the FBI/FDA/media given bad behaviors and various tools.</p></blockquote><p>The benchmark creates surprisingly realistic scenarios—like detailed pharmaceutical fraud involving concealed adverse events and hidden patient deaths—then provides models with email capabilities to see if they&rsquo;ll take autonomous action. This reveals something fascinating about AI behavior that goes beyond traditional benchmarks. Rather than testing reasoning or knowledge, SnitchBench probes the boundaries between helpful assistance and autonomous moral decision-making. When models encounter what appears to be serious wrongdoing, do they become digital whistleblowers?
The implications are both reassuring and unsettling. On one hand, you want AI systems that won&rsquo;t assist with genuinely harmful activities. On the other, the idea of AI models making autonomous decisions about what constitutes reportable behavior feels like a significant step toward AI agency that we haven&rsquo;t fully grappled with yet. Therefore Anthropic&rsquo;s own advice here seems like a good rule to follow:</p><blockquote><p>Whereas this kind of ethical intervention and whistleblowing is perhaps appropriate in principle, it has a risk of misfiring if users give Opus-based agents access to incomplete or misleading information and prompt them in these ways. We recommend that users exercise caution with instructions like these that invite high-agency behavior in contexts that could appear ethically questionable.</p></blockquote></div></article></main></div></body></html>