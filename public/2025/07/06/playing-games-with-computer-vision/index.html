<!doctype html><html lang=en-us><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Playing Games with Computer Vision - philippdubach</title><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.147.8"><title>Playing Games with Computer Vision - philippdubach</title><meta name=description content="Personal Projects, Curated Articles and Papers on Economics, Finance and Technology"><meta name=keywords content="Finance,Economics,Technology,Data,Machine Learning"><meta property="og:url" content="http://localhost:1313/2025/07/06/playing-games-with-computer-vision/"><meta property="og:site_name" content="philippdubach"><meta property="og:title" content="Playing Games with Computer Vision"><meta property="og:description" content="After installing Claude Code
the agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster through natural language commands.
I was looking for a task to test it’s abilities. Fairly quickly we wrote less than 200 lines of python code predicting black jack odds using Monte Carlo Simulation. When I went on to test this little tool on Washington Post’s online Black Jack (I also din’t know that existed!) I quickly noticed how impractical it was to manually input all the card values on the table manually. What if the tool would also automatically recognize the cards that are on the table and calculate the odds from it? I have never done anything with computer vision so this seemed like a good challenge. To get to any reasonable result we have to start with classification where we “teach” the model to categorize data by showing them lots of examples with correct labels. But where do the labels come from? I manually annotated 409 playing cards across 117 images using Roboflow Annotate (at first I only did half as much - why this wasn’t a good idea well see in a minute). Once enough screenshots of cards wer annotated we can train the model to recognize the cards an predict card values on tables it has never seen before. I was able to use a NVIDIA T4 GPU inside Google Colab which offer some GPU time for free when capacity is available. During training, the algorithm learns patterns from this example data, adjusting its internal parameters millions of times until it gets really good at recognizing the differences between categories (in this case different cards). Once trained, the model can then make predictions on new, unseen data by applying the patterns it learned. With the annotated dataset ready, it was time to implement the actual computer vision model. I chose run interferenc on Ultralytics’ YOLOv11 pre-trained model, a state-of-the-art object detection algorithm. I set up the environment in Google Colab following the “How to Train YOLO11 Object Detection on a Custom Dataset” notebook. After downloading the annotated dataset from Roboflow, I began training the YOLOv11 model using the pre-trained YOLOv11s weights as a starting point. This approach, called transfer learning, allows the model to leverage patterns already learned from millions of general images and adapt them to this specific task. I initially set it up to run for 350 epochs, though the model’s built-in early stopping mechanism kicked in after 242 epochs when no improvement was observed for 100 consecutive epochs. The best results were achieved at epoch 142, taking just 0.22 hours to complete on the Tesla T4 GPU. The initial results were quite promising, with an overall mean Average Precision (mAP) of 99.5% at IoU threshold 0.5. Most individual card classes achieved near-perfect precision and recall scores, with only a few cards like the 6 and Queen showing slightly lower precision values. However, looking at the confusion matrix and loss curves revealed some interesting patterns. While the model was learning effectively (as shown by the steadily decreasing loss), there were still some misclassifications between similar cards, particularly among the numbered cards. This highlighted exactly why I mentioned earlier that annotating only half the amount of data initially “wasn’t a good idea” - more training examples would likely improve these edge cases and reduce confusion between similar-looking cards. My first attempt at solving the remaining accuracy issues was to add another layer to the workflow by sending the detected cards to Anthropic’s Claude API for additional OCR processing. This hybrid approach was remarkably effective - the combination of YOLO’s object detection with Claude’s advanced vision capabilities yielded 99.9% accuracy on the predicted cards. However, this solution came with a significant drawback: the additional API layer consumed valuable time due to network latency and the large model’s processing overhead, making it impractical for real-time gameplay. Seeking a faster local solution, I implemented the same workflow using EasyOCR instead. While EasyOCR was able to correctly identify the card numbers when it detected them, it struggled to recognize around half of the cards in the first place - even when fed pre-cropped card images directly from the YOLO model. This inconsistency made it unreliable for the application. Rather than continue band-aid solutions, I decided to go back to the fundamentals and improve my dataset. I doubled the training data by adding another 60 screenshots with the same annotation split as before. More importantly, I went through all the previous annotations and fixed many of the bounding polygons. I noticed that several misidentifications were caused by the model detecting face-down dealer cards as valid cards, which happened because some annotations for face-up cards inadvertently included parts of the card backs next to them."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-06T00:00:00+00:00"><meta property="article:modified_time" content="2025-07-06T00:00:00+00:00"><meta property="article:tag" content="Project"><meta property="og:image" content="https://static.philippdubach.com/ograph/ograph-post.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://static.philippdubach.com/ograph/ograph-post.jpg"><meta name=twitter:title content="Playing Games with Computer Vision"><meta name=twitter:description content="After installing Claude Code
the agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster through natural language commands.
I was looking for a task to test it’s abilities. Fairly quickly we wrote less than 200 lines of python code predicting black jack odds using Monte Carlo Simulation. When I went on to test this little tool on Washington Post’s online Black Jack (I also din’t know that existed!) I quickly noticed how impractical it was to manually input all the card values on the table manually. What if the tool would also automatically recognize the cards that are on the table and calculate the odds from it? I have never done anything with computer vision so this seemed like a good challenge. To get to any reasonable result we have to start with classification where we “teach” the model to categorize data by showing them lots of examples with correct labels. But where do the labels come from? I manually annotated 409 playing cards across 117 images using Roboflow Annotate (at first I only did half as much - why this wasn’t a good idea well see in a minute). Once enough screenshots of cards wer annotated we can train the model to recognize the cards an predict card values on tables it has never seen before. I was able to use a NVIDIA T4 GPU inside Google Colab which offer some GPU time for free when capacity is available. During training, the algorithm learns patterns from this example data, adjusting its internal parameters millions of times until it gets really good at recognizing the differences between categories (in this case different cards). Once trained, the model can then make predictions on new, unseen data by applying the patterns it learned. With the annotated dataset ready, it was time to implement the actual computer vision model. I chose run interferenc on Ultralytics’ YOLOv11 pre-trained model, a state-of-the-art object detection algorithm. I set up the environment in Google Colab following the “How to Train YOLO11 Object Detection on a Custom Dataset” notebook. After downloading the annotated dataset from Roboflow, I began training the YOLOv11 model using the pre-trained YOLOv11s weights as a starting point. This approach, called transfer learning, allows the model to leverage patterns already learned from millions of general images and adapt them to this specific task. I initially set it up to run for 350 epochs, though the model’s built-in early stopping mechanism kicked in after 242 epochs when no improvement was observed for 100 consecutive epochs. The best results were achieved at epoch 142, taking just 0.22 hours to complete on the Tesla T4 GPU. The initial results were quite promising, with an overall mean Average Precision (mAP) of 99.5% at IoU threshold 0.5. Most individual card classes achieved near-perfect precision and recall scores, with only a few cards like the 6 and Queen showing slightly lower precision values. However, looking at the confusion matrix and loss curves revealed some interesting patterns. While the model was learning effectively (as shown by the steadily decreasing loss), there were still some misclassifications between similar cards, particularly among the numbered cards. This highlighted exactly why I mentioned earlier that annotating only half the amount of data initially “wasn’t a good idea” - more training examples would likely improve these edge cases and reduce confusion between similar-looking cards. My first attempt at solving the remaining accuracy issues was to add another layer to the workflow by sending the detected cards to Anthropic’s Claude API for additional OCR processing. This hybrid approach was remarkably effective - the combination of YOLO’s object detection with Claude’s advanced vision capabilities yielded 99.9% accuracy on the predicted cards. However, this solution came with a significant drawback: the additional API layer consumed valuable time due to network latency and the large model’s processing overhead, making it impractical for real-time gameplay. Seeking a faster local solution, I implemented the same workflow using EasyOCR instead. While EasyOCR was able to correctly identify the card numbers when it detected them, it struggled to recognize around half of the cards in the first place - even when fed pre-cropped card images directly from the YOLO model. This inconsistency made it unreliable for the application. Rather than continue band-aid solutions, I decided to go back to the fundamentals and improve my dataset. I doubled the training data by adding another 60 screenshots with the same annotation split as before. More importantly, I went through all the previous annotations and fixed many of the bounding polygons. I noticed that several misidentifications were caused by the model detecting face-down dealer cards as valid cards, which happened because some annotations for face-up cards inadvertently included parts of the card backs next to them."><link rel=canonical href=http://localhost:1313/2025/07/06/playing-games-with-computer-vision/><link rel=stylesheet href=/css/custom.css><link rel=icon type=image/png href=https://static.philippdubach.com/icons/favicon-96x96.png sizes=96x96><link rel=icon type=image/svg+xml href=https://static.philippdubach.com/icons/favicon.svg><link rel="shortcut icon" href=https://static.philippdubach.com/icons/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=https://static.philippdubach.com/icons/apple-touch-icon.png><meta name=apple-mobile-web-app-title content="philippdubach.com"><link rel=manifest href=https://static.philippdubach.com/icons/site.webmanifest><meta name=theme-color content="#434648"></head></head><body class=linkblog><div class=container><aside class=sidebar><div class=sidebar-content><div class=site-header><h1 class=site-title><a href=http://localhost:1313/>philippdubach</a></h1><p class=site-description>Personal Projects, Curated Articles and Papers on Economics, Finance and Technology</p></div><nav class=navigation><ul><li><a href=/>Home</a></li><li><a href=/projects/>Projects</a></li><li><a href=/about/>About</a></li><li><a href=/posts/>Archive</a></li><li><a href=/index.xml>RSS</a></li></ul></nav><div class=social-links><a href=https://github.com/philippdubach target=_blank rel=noopener>GitHub</a>
<a href=mailto:info@philippdubach.com>Email</a></div></div></aside><main class=content><article class="post single"><header class=post-header><h1 class=post-title>Playing Games with Computer Vision</h1><div class=post-meta><time datetime=2025-07-06T00>July 6, 2025</time></div></header><div class=post-content><p>After installing <a href=https://www.anthropic.com/claude-code>Claude Code</a></p><blockquote><p>the agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster through natural language commands.</p></blockquote><p>I was looking for a task to test it&rsquo;s abilities. Fairly quickly we wrote <a href=https://gist.github.com/philippdubach/741cbd56498e43375892966ca691b9c2>less than 200 lines of python code predicting black jack odds</a> using Monte Carlo Simulation. When I went on to test this little tool on <a href=https://games.washingtonpost.com/games/blackjack>Washington Post&rsquo;s</a> online Black Jack (I also din&rsquo;t know that existed!) I quickly noticed how impractical it was to manually input all the card values on the table manually. What if the tool would also automatically recognize the cards that are on the table and calculate the odds from it? I have never done anything with computer vision so this seemed like a good challenge.
<picture style="display:block;width:80%;margin:0 auto;padding:1rem 0"><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/classification.gif 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/classification.gif 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/classification.gif 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/classification.gif 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/classification.gif 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/classification.gif 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/classification.gif 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/classification.gif 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/classification.gif" alt="alt text here" loading=lazy style=width:100%;height:auto;display:block>
</picture>To get to any reasonable result we have to start with classification where we &ldquo;teach&rdquo; the model to categorize data by showing them lots of examples with correct labels. But where do the labels come from? I manually annotated <a href=https://universe.roboflow.com/cards-agurd/playing_card_classification>409 playing cards across 117 images</a> using Roboflow Annotate (at first I only did half as much - why this wasn&rsquo;t a good idea well see in a minute). Once enough screenshots of cards wer annotated we can train the model to recognize the cards an predict card values on tables it has never seen before. I was able to use a <a href=https://www.nvidia.com/en-us/data-center/tesla-t4/>NVIDIA T4 GPU</a> inside Google Colab which offer some GPU time for free when capacity is available.
<picture style="display:block;width:80%;margin:0 auto;padding:1rem 0"><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/gpu_setup_colab.png 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/gpu_setup_colab.png 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/gpu_setup_colab.png 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/gpu_setup_colab.png 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/gpu_setup_colab.png 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/gpu_setup_colab.png 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/gpu_setup_colab.png 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/gpu_setup_colab.png 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/gpu_setup_colab.png" alt="alt text here" loading=lazy style=width:100%;height:auto;display:block>
</picture>During training, the algorithm learns patterns from this example data, adjusting its internal parameters millions of times until it gets really good at recognizing the differences between categories (in this case different cards). Once trained, the model can then make predictions on new, unseen data by applying the patterns it learned. With the annotated dataset ready, it was time to implement the actual computer vision model. I chose run interferenc on <a href=https://docs.ultralytics.com/de/models/yolo11/>Ultralytics&rsquo; YOLOv11</a> pre-trained model, a state-of-the-art object detection algorithm. I set up the environment in Google Colab following the <a href=https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolo11-object-detection-on-custom-dataset.ipynb>&ldquo;How to Train YOLO11 Object Detection on a Custom Dataset&rdquo;</a> notebook. After downloading the annotated dataset from Roboflow, I began training the YOLOv11 model using the pre-trained YOLOv11s weights as a starting point. This approach, called <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, allows the model to leverage patterns already learned from millions of general images and adapt them to this specific task.
<picture style="display:block;width:80%;margin:0 auto;padding:1rem 0"><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/val_batch0_pred.jpg 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/val_batch0_pred.jpg 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/val_batch0_pred.jpg 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/val_batch0_pred.jpg 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/val_batch0_pred.jpg 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/val_batch0_pred.jpg 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/val_batch0_pred.jpg 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/val_batch0_pred.jpg 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/val_batch0_pred.jpg" alt="Training results showing confusion matrix and loss curves" loading=lazy style=width:100%;height:auto;display:block>
</picture>I initially set it up to run for 350 epochs, though the model&rsquo;s built-in early stopping mechanism kicked in after 242 epochs when no improvement was observed for 100 consecutive epochs. The best results were achieved at epoch 142, taking just 0.22 hours to complete on the Tesla T4 GPU.
The initial results were quite promising, with an overall mean Average Precision (mAP) of 99.5% at IoU threshold 0.5. Most individual card classes achieved near-perfect precision and recall scores, with only a few cards like the 6 and Queen showing slightly lower precision values.
<picture style="display:block;width:80%;margin:0 auto;padding:1rem 0"><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/run1_results.png 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/run1_results.png 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/run1_results.png 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/run1_results.png 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/run1_results.png 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/run1_results.png 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/run1_results.png 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/run1_results.png 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/run1_results.png" alt="Training results showing confusion matrix and loss curves" loading=lazy style=width:100%;height:auto;display:block>
</picture>However, looking at the confusion matrix and loss curves revealed some interesting patterns. While the model was learning effectively (as shown by the steadily decreasing loss), there were still some misclassifications between similar cards, particularly among the numbered cards. This highlighted exactly why I mentioned earlier that annotating only half the amount of data initially &ldquo;wasn&rsquo;t a good idea&rdquo; - more training examples would likely improve these edge cases and reduce confusion between similar-looking cards. My first attempt at solving the remaining accuracy issues was to add another layer to the workflow by sending the detected cards to Anthropic&rsquo;s Claude API for additional OCR processing.
<picture style="display:block;width:80%;margin:0 auto;padding:1rem 0"><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/claude_workflow.png 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/claude_workflow.png 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/claude_workflow.png 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/claude_workflow.png 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/claude_workflow.png 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/claude_workflow.png 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/claude_workflow.png 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/claude_workflow.png 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/claude_workflow.png" alt="Roboflow workflow with Claude API integration" loading=lazy style=width:100%;height:auto;display:block>
</picture>This hybrid approach was remarkably effective - the combination of YOLO&rsquo;s object detection with Claude&rsquo;s advanced vision capabilities yielded 99.9% accuracy on the predicted cards. However, this solution came with a significant drawback: the additional API layer consumed valuable time due to network latency and the large model&rsquo;s processing overhead, making it impractical for real-time gameplay.
Seeking a faster local solution, I implemented the same workflow using EasyOCR instead. While EasyOCR was able to correctly identify the card numbers when it detected them, it struggled to recognize around half of the cards in the first place - even when fed pre-cropped card images directly from the YOLO model. This inconsistency made it unreliable for the application.
<picture style="display:block;width:60%;margin:0 auto;padding:1rem 0"><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/easyocr_example.png 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/easyocr_example.png 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/easyocr_example.png 640w" sizes=60%><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/easyocr_example.png 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/easyocr_example.png 1024w" sizes=60%><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/easyocr_example.png 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/easyocr_example.png 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/easyocr_example.png 2000w" sizes=60%><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/easyocr_example.png" alt="EasyOCR attempting to read a cropped card image" loading=lazy style=width:100%;height:auto;display:block>
</picture>Rather than continue band-aid solutions, I decided to go back to the fundamentals and improve my dataset. I doubled the training data by adding another 60 screenshots with the same annotation split as before. More importantly, I went through all the previous annotations and fixed many of the bounding polygons. I noticed that several misidentifications were caused by the model detecting face-down dealer cards as valid cards, which happened because some annotations for face-up cards inadvertently included parts of the card backs next to them.</p><p>My first attempt at solving this was to add another layer and built a workflow to send the detected cards to the anthropic claude api and perform OCR</p><p>this yielded 99.9% accuracy on the predicted cards but - due to the large model aditional api layer- consumed valuable time</p><p>I then implemented the same workflow <a href=https://github.com/JaidedAI/EasyOCR>locally with easyOCR</a> which was able to identify the numbers correctly, but struggled to recognize around half of them in the first place; even when fead with the pre cropped card images from roboflow</p><p>so I went back to my dataset first I doubled the dataset by adding another 60 scrrencaoptures (same split as before)</p><p>also i went over the previous anotations and fixed som of the polygones since I noticed that a lot of the missidentifications was the model identifying flipped over dealer cards as cards since some of the turned up dealer cards had polygones that included part of the backside next to it</p><p>I then trained the model once again - and now had a mAP@50 98.8% and Precision 92.1%</p><p>first ran it with roboflow api around 4s; run it locally &lt;0.1s (Speed: 1.3ms preprocess, 45.5ms inference, 0.4ms postprocess per image at shape)</p><p>I also used mss to have a realtime feed of my browser window with an automatic overlay of the detected cards</p><picture style="display:block;width:80%;margin:0 auto;padding:1rem 0"><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/black_jack_odds_demo.gif 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/black_jack_odds_demo.gif 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/black_jack_odds_demo.gif 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/black_jack_odds_demo.gif 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/black_jack_odds_demo.gif 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/black_jack_odds_demo.gif 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/black_jack_odds_demo.gif 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/black_jack_odds_demo.gif 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/black_jack_odds_demo.gif" alt="Overview of selected fitted curves" loading=lazy style=width:100%;height:auto;display:block></picture></div></article></main></div></body></html>