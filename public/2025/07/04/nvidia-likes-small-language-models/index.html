<!doctype html><html lang=en-us><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>NVIDIA Likes Small Language Models - philippdubach.com</title><meta name=description content="NVIDIA researchers argue that Small Language Models (SLMs) under 10 billion parameters can handle most agentic tasks more efficiently than large language models, though context window limitations and economic efficiency claims face scrutiny."><meta name=keywords content="Small Language Models,SLMs,NVIDIA,agentic systems,language model efficiency,context window,inference latency,edge computing,Microsoft Phi,NVIDIA Nemotron,Hugging Face SmolLM2,multishot prompting,diffusion language models,LLM vs SLM,consumer hardware deployment"><meta name=author content="Philipp Dubach"><meta name=generator content="Hugo 0.147.8"><link rel=canonical href=http://localhost:1313/2025/07/04/nvidia-likes-small-language-models/><style>*{margin:0;padding:0;box-sizing:border-box}body{font-family:helvetica neue,Helvetica,Arial,segoe ui,sans-serif;padding-top:90px;line-height:1.6;color:#333;background-color:#f8f9fa}blockquote{margin:1em 0;padding-left:1em;border-left:2px solid #ccc;color:#666}blockquote p{margin:.5em 0}.container{display:flex;max-width:1200px;margin:0 auto;min-height:100vh}.sidebar{width:200px;background-color:#f8f9fa;padding:1.5rem;border-right:0 solid #e9ecef;position:sticky;top:0;height:100vh;overflow-y:auto}.site-title a{text-decoration:none;color:#333;font-size:1.5rem;font-weight:700}.site-description{color:#666;margin:.5rem 0 2rem;font-size:.9rem}.navigation ul{list-style:none;margin-bottom:2rem}.navigation li{margin-bottom:.5rem}.navigation a{text-decoration:none;color:#007acc;font-weight:500}.navigation a:hover{text-decoration:underline}.social-links a{display:inline-block;margin-right:1rem;color:#666;text-decoration:none;font-size:.9rem}.social-links a:hover{color:#007acc}.content{flex:1;padding:2rem;max-width:800px}.posts{max-width:90%}.post{margin-bottom:1rem;padding-bottom:1rem}.post:last-child{border-bottom:none}.post-title{margin-bottom:.5rem}.post-title a{text-decoration:none;color:#333;font-size:1.25rem;font-weight:600}.post-title a:hover{color:#007acc}.external-link{color:#007acc;font-weight:400;margin-left:.25rem}.post-meta{font-size:.85rem;color:#666;margin-bottom:0}.permalink{text-decoration:none;color:#999;font-weight:400}.permalink:hover{color:#007acc}.post-content{line-height:1.7}.post-content p{margin-bottom:1rem}.post-content a{color:#007acc;text-decoration:none}.post-content a:hover{text-decoration:underline}.project-tag{background-color:#e9ecef;color:#495057;padding:2px 6px;border-radius:3px;font-size:9px;font-weight:500;text-transform:uppercase;letter-spacing:.3px;margin-left:6px;display:inline-block;vertical-align:middle;border:1px solid #dee2e6}.archive{max-width:90%}.year h2{margin:2rem 0 1rem;color:#333;font-size:1.5rem}.archive-item{display:flex;margin-bottom:.75rem;align-items:baseline}.archive-meta{width:60px;flex-shrink:0;font-size:.85rem;color:#666}.archive-title{flex:1}.archive-title a{text-decoration:none;color:#333}.archive-title a:hover{color:#007acc}.archive-title .permalink{margin-left:.5rem}.single .post-title{font-size:1.75rem;margin-bottom:1rem}.pagination{margin-top:2rem;text-align:center;padding-top:2rem;border-top:1px solid #e9ecef}.pagination a{color:#007acc;text-decoration:none;font-weight:500}.pagination a:hover{text-decoration:underline}@media(prefers-color-scheme:dark){.pagination{border-top-color:#404040}.pagination a{color:#4da6ff}.pagination a:hover{color:#66b3ff}}@media(prefers-color-scheme:dark){.project-tag{background-color:#404040;color:#b0b0b0;border-color:#555}}@media screen and (max-width:768px){html{font-size:8px !important}body{font-size:1rem !important}.sidebar{width:100px !important;padding:.75rem !important}.container{max-width:600px !important;margin:0 20px !important}.content{max-width:400px !important;padding:1rem !important}.project-tag{padding:1px 3px !important;border-radius:1.5px !important;font-size:4.5px !important;letter-spacing:.15px !important;margin-left:3px !important}.archive-meta{width:30px !important}}@media(prefers-color-scheme:dark){body{background-color:#2a2a2a;color:#e0e0e0}.sidebar{background-color:#2a2a2a;border-right-color:#404040}.site-title a{color:#e0e0e0}.post-title a{color:#e0e0e0}.archive-title a{color:#e0e0e0}.post-title a:hover,.navigation a:hover,.social-links a:hover,.archive-title a:hover{color:#4da6ff}.post{border-bottom-color:#404040}.footer{border-top-color:#404040}}</style><link rel=icon type=image/png href=/icons/favicon-96x96.png sizes=96x96><link rel=icon type=image/svg+xml href=/icons/favicon.svg><link rel="shortcut icon" href=/icons/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/icons/apple-touch-icon.png><meta name=apple-mobile-web-app-title content="philippdubach.com"><link rel=manifest href=/icons/site.webmanifest><meta name=theme-color content="#2c3e50"><meta name=msapplication-TileColor content="#2c3e50"><meta property="og:title" content="NVIDIA Likes Small Language Models"><meta property="og:description" content="NVIDIA researchers argue that Small Language Models (SLMs) under 10 billion parameters can handle most agentic tasks more efficiently than large language models, though context window limitations and economic efficiency claims face scrutiny."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/2025/07/04/nvidia-likes-small-language-models/"><meta property="og:site_name" content="philippdubach.com"><meta property="og:locale" content="en_US"><meta property="og:logo" content="http://localhost:1313/icons/favicon.ico"><meta property="og:image" content="https://static.philippdubach.com/ograph/ograph-post.jpg"><meta property="article:published_time" content="2025-07-04T00:00:00Z"><meta property="article:modified_time" content="2025-07-04T00:00:00Z"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="NVIDIA Likes Small Language Models"><meta name=twitter:description content="NVIDIA researchers argue that Small Language Models (SLMs) under 10 billion parameters can handle most agentic tasks more efficiently than large language models, though context window limitations and economic efficiency claims face scrutiny."><meta name=twitter:image content="https://static.philippdubach.com/ograph/ograph-post.jpg"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"http:\/\/localhost:1313\/"},{"@type":"ListItem","position":2,"name":"Posts","item":"http:\/\/localhost:1313\/posts/"},{"@type":"ListItem","position":3,"name":"NVIDIA Likes Small Language Models","item":"http:\/\/localhost:1313\/2025\/07\/04\/nvidia-likes-small-language-models\/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"NVIDIA Likes Small Language Models","name":"NVIDIA Likes Small Language Models","description":"NVIDIA researchers argue that Small Language Models (SLMs) under 10 billion parameters can handle most agentic tasks more efficiently than large language models, though context window limitations and economic efficiency claims face scrutiny.","keywords":["Small Language Models","SLMs","NVIDIA","agentic systems","language model efficiency","context window","inference latency","edge computing","Microsoft Phi","NVIDIA Nemotron","Hugging Face SmolLM2","multishot prompting","diffusion language models","LLM vs SLM","consumer hardware deployment"],"articleBody":"\" A Small Language Model (SLM) is a LM that can fit onto a common consumer electronic device and perform inference with latency sufficiently low to be practical when serving the agentic requests of one user. [\\u0026hellip;] We note that as of 2025, we would be comfortable with considering most models below 10bn parameters in size to be SLMs.\\nThe (NVIDIA) researchers argue that most agentic applications perform repetitive, specialized tasks that don\\u0026rsquo;t require the full generalist capabilities of LLMs. They propose heterogeneous agentic systems where SLMs handle most tasks while LLMs are used selectively for complex reasoning. They present three main arguments: (1) SLMs are sufficiently powerful for agentic tasks, as demonstrated by recent models like Microsoft\\u0026rsquo;s Phi series, NVIDIA\\u0026rsquo;s Nemotron-H family, and Hugging Face\\u0026rsquo;s SmolLM2 series, which achieve comparable performance to much larger models while being 10-30x more efficient. (2) SLMs are inherently more operationally suitable for agentic systems due to their faster inference, lower latency, and ability to run on edge devices. (3) SLMs are necessarily more economical, offering significant cost savings in inference, fine-tuning, and deployment.\\nThe paper addresses counterarguments about LLMs\\u0026rsquo; superior language understanding and centralization benefits with studies (see Appendix B: LLM-to-SLM Replacement Case Studies) showing that 40-70% of LLM queries in popular open-source agents (MetaGPT, Open Operator, Cradle) could be replaced by specialized SLMs. One comment I read raised important concerns about the paper\\u0026rsquo;s analysis, particularly regarding context window which are arguably the highest technical barrier to SLM adoption in agentic systems. Modern agentic applications require substantial context: Claude 4 Sonnet\\u0026rsquo;s system prompt alone reportedly uses around 25k tokens, and a typical coding agent needs system instructions, tool definitions, file context, and project documentation, totaling 5-10k tokens before any actual work begins. Most SLMs that can run on consumer hardware are capped at 32k or 128k contexts architecturally, but achieving reasonable inference speeds at these limits requires gaming hardware (8GB VRAM for a 7b model at 128k context).\\nThe paper concludes that the shift to SLMs is inevitable due to economic and operational advantages, despite current barriers including infrastructure investment in LLM serving, generalist benchmark focus, and limited awareness of SLM capabilities. But the economic efficiency claims also face scrutiny under system-level analysis. In Section 3.2 they present simplistic FLOP comparisons while ignoring critical inefficiencies: the reliance on multishot-prompting where SLMs might require 3-4 attempts for tasks that LLMs complete with 90% success rate, task decomposition overhead that multiplies context setup costs and error rates, and infrastructure efficiency differences between optimized datacenters (PUE ratios near 1.1, \\u0026gt;90% GPU utilization) and consumer hardware (5-10% GPU utilization, residential HVAC, 80-85% power conversion efficiency). When accounting for failed attempts, orchestration overhead, and infrastructure efficiency, many \\u0026ldquo;economical\\u0026rdquo; SLM deployments might actually consume more total energy than centralized LLM inference.\\n(05/07/2025) Update: On the topic of speed I just came across Ultra-Fast Language Models Based on Diffusion. You can also test it yourself using the free playground link, and it is in fact extremely fast. Try the \\u0026ldquo;Diffusion Effect\\u0026rdquo; in the top right corner which toggles an interesting visualization. I\\u0026rsquo;m not sure how realistic this is, it shows text appearing as random noise before gradually resolving into clear words; though the actual process likely involves tokens evolving from imprecise vectors in a multidimensional space toward more precise representations until they crystallize into specific words.\\n(06/07/2025) Update II: Apparently there is also a Google DeepMind Gemini Diffusion Model.\\n\"","wordCount":568,"inLanguage":"en","image":"https:\/\/static.philippdubach.com\/ograph\/ograph-post.jpg","datePublished":"2025-07-04T00:00:00Z","dateModified":"2025-07-04T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/localhost:1313\/2025\/07\/04\/nvidia-likes-small-language-models\/"},"publisher":{"@type":"Organization","name":"philippdubach","logo":{"@type":"ImageObject","url":"http:\/\/localhost:1313\/icons/favicon.ico"}},"author":{"@type":"Person","name":"Philipp Dubach"}}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"http:\/\/localhost:1313\/"},{"@type":"ListItem","position":2,"name":"Posts","item":"http:\/\/localhost:1313\/posts/"},{"@type":"ListItem","position":3,"name":"NVIDIA Likes Small Language Models","item":"http:\/\/localhost:1313\/2025\/07\/04\/nvidia-likes-small-language-models\/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"NVIDIA Likes Small Language Models","name":"NVIDIA Likes Small Language Models","description":"NVIDIA researchers argue that Small Language Models (SLMs) under 10 billion parameters can handle most agentic tasks more efficiently than large language models, though context window limitations and economic efficiency claims face scrutiny.","keywords":["Small Language Models","SLMs","NVIDIA","agentic systems","language model efficiency","context window","inference latency","edge computing","Microsoft Phi","NVIDIA Nemotron","Hugging Face SmolLM2","multishot prompting","diffusion language models","LLM vs SLM","consumer hardware deployment"],"articleBody":"\" A Small Language Model (SLM) is a LM that can fit onto a common consumer electronic device and perform inference with latency sufficiently low to be practical when serving the agentic requests of one user. [\\u0026hellip;] We note that as of 2025, we would be comfortable with considering most models below 10bn parameters in size to be SLMs.\\nThe (NVIDIA) researchers argue that most agentic applications perform repetitive, specialized tasks that don\\u0026rsquo;t require the full generalist capabilities of LLMs. They propose heterogeneous agentic systems where SLMs handle most tasks while LLMs are used selectively for complex reasoning. They present three main arguments: (1) SLMs are sufficiently powerful for agentic tasks, as demonstrated by recent models like Microsoft\\u0026rsquo;s Phi series, NVIDIA\\u0026rsquo;s Nemotron-H family, and Hugging Face\\u0026rsquo;s SmolLM2 series, which achieve comparable performance to much larger models while being 10-30x more efficient. (2) SLMs are inherently more operationally suitable for agentic systems due to their faster inference, lower latency, and ability to run on edge devices. (3) SLMs are necessarily more economical, offering significant cost savings in inference, fine-tuning, and deployment.\\nThe paper addresses counterarguments about LLMs\\u0026rsquo; superior language understanding and centralization benefits with studies (see Appendix B: LLM-to-SLM Replacement Case Studies) showing that 40-70% of LLM queries in popular open-source agents (MetaGPT, Open Operator, Cradle) could be replaced by specialized SLMs. One comment I read raised important concerns about the paper\\u0026rsquo;s analysis, particularly regarding context window which are arguably the highest technical barrier to SLM adoption in agentic systems. Modern agentic applications require substantial context: Claude 4 Sonnet\\u0026rsquo;s system prompt alone reportedly uses around 25k tokens, and a typical coding agent needs system instructions, tool definitions, file context, and project documentation, totaling 5-10k tokens before any actual work begins. Most SLMs that can run on consumer hardware are capped at 32k or 128k contexts architecturally, but achieving reasonable inference speeds at these limits requires gaming hardware (8GB VRAM for a 7b model at 128k context).\\nThe paper concludes that the shift to SLMs is inevitable due to economic and operational advantages, despite current barriers including infrastructure investment in LLM serving, generalist benchmark focus, and limited awareness of SLM capabilities. But the economic efficiency claims also face scrutiny under system-level analysis. In Section 3.2 they present simplistic FLOP comparisons while ignoring critical inefficiencies: the reliance on multishot-prompting where SLMs might require 3-4 attempts for tasks that LLMs complete with 90% success rate, task decomposition overhead that multiplies context setup costs and error rates, and infrastructure efficiency differences between optimized datacenters (PUE ratios near 1.1, \\u0026gt;90% GPU utilization) and consumer hardware (5-10% GPU utilization, residential HVAC, 80-85% power conversion efficiency). When accounting for failed attempts, orchestration overhead, and infrastructure efficiency, many \\u0026ldquo;economical\\u0026rdquo; SLM deployments might actually consume more total energy than centralized LLM inference.\\n(05/07/2025) Update: On the topic of speed I just came across Ultra-Fast Language Models Based on Diffusion. You can also test it yourself using the free playground link, and it is in fact extremely fast. Try the \\u0026ldquo;Diffusion Effect\\u0026rdquo; in the top right corner which toggles an interesting visualization. I\\u0026rsquo;m not sure how realistic this is, it shows text appearing as random noise before gradually resolving into clear words; though the actual process likely involves tokens evolving from imprecise vectors in a multidimensional space toward more precise representations until they crystallize into specific words.\\n(06/07/2025) Update II: Apparently there is also a Google DeepMind Gemini Diffusion Model.\\n\"","wordCount":568,"inLanguage":"en","image":"https:\/\/static.philippdubach.com\/ograph\/ograph-post.jpg","datePublished":"2025-07-04T00:00:00Z","dateModified":"2025-07-04T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/localhost:1313\/2025\/07\/04\/nvidia-likes-small-language-models\/"},"publisher":{"@type":"Organization","name":"philippdubach","logo":{"@type":"ImageObject","url":"http:\/\/localhost:1313\/icons/favicon.ico"}},"author":{"@type":"Person","name":"Philipp Dubach"}}</script><script>(function(){let e=[],n=[],t=null;const s=["offsetHeight","offsetWidth","offsetTop","offsetLeft","scrollHeight","scrollWidth","scrollTop","scrollLeft","clientHeight","clientWidth","clientTop","clientLeft"];s.forEach(t=>{const s=Object.getOwnPropertyDescriptor(HTMLElement.prototype,t);s&&s.get&&Object.defineProperty(HTMLElement.prototype,t,{get:function(){return e.length>0&&(console.warn(`⚠️ FORCED REFLOW: Reading ${t} after DOM write!`),console.log("Recent writes:",e),console.log("Element:",this),console.trace()),n.push({prop:t,element:this.tagName}),s.get.call(this)}})});const o=Object.getOwnPropertyDescriptor(HTMLElement.prototype,"style");Object.defineProperty(HTMLElement.prototype,"style",{get:function(){const s=o.get.call(this);return new Proxy(s,{set:function(s,o,i){return e.push({property:o,value:i,element:s.parentElement?.tagName}),n=[],t||(t=requestAnimationFrame(()=>{e=[],t=null})),s[o]=i,!0}})}});const i=Element.prototype.getBoundingClientRect;Element.prototype.getBoundingClientRect=function(){return e.length>0&&(console.warn("⚠️ FORCED REFLOW: getBoundingClientRect() called after DOM write!"),console.trace()),i.call(this)}})()</script></head><body class=linkblog><div class=container><aside class=sidebar><div class=sidebar-content><div class=site-header><h1 class=site-title><a href=http://localhost:1313/>philippdubach</a></h1><p class=site-description>Personal Projects, Curated Articles and Papers on Economics, Finance and Technology</p></div><nav class=navigation><ul><li><a href=/>Home</a></li><li><a href=/projects/>Projects</a></li><li><a href=/about/>About</a></li><li><a href=/posts/>Archive</a></li><li><a href=/index.xml>RSS</a></li></ul></nav><div class=social-links><a href=https://github.com/philippdubach target=_blank rel=noopener>GitHub</a>
<a href=mailto:info@philippdubach.com>Email</a></div></div></aside><main class=content><article class="post single"><header class=post-header><h1 class=post-title><a href=https://arxiv.org/abs/2506.02153 target=_blank rel=noopener>NVIDIA Likes Small Language Models
<span class=external-link>→</span></a></h1><div class=post-meta><time datetime=2025-07-04T00>July 4, 2025
</time>• <a href=https://arxiv.org/abs/2506.02153 target=_blank rel=noopener>Original Link</a></div></header><div class=post-content><blockquote><p>A Small Language Model (SLM) is a LM that can fit onto a common consumer electronic device and perform inference with latency sufficiently low to be practical when serving the agentic requests of one user. [&mldr;] We note that as of 2025, we would be comfortable with considering most models below 10bn parameters in size to be SLMs.</p></blockquote><p>The <a href=https://research.nvidia.com/labs/lpr/slm-agents/>(NVIDIA) researchers</a> argue that most agentic applications perform repetitive, specialized tasks that don&rsquo;t require the full generalist capabilities of LLMs. They propose heterogeneous agentic systems where SLMs handle most tasks while LLMs are used selectively for complex reasoning. They present three main arguments: (1) SLMs are sufficiently powerful for agentic tasks, as demonstrated by recent models like <a href=https://azure.microsoft.com/en-us/products/phi>Microsoft&rsquo;s Phi series</a>, <a href=https://research.nvidia.com/labs/adlr/nemotronh/>NVIDIA&rsquo;s Nemotron-H family</a>, and <a href=https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9>Hugging Face&rsquo;s SmolLM2 series</a>, which achieve comparable performance to much larger models <a href=https://arxiv.org/abs/2501.05465>while being 10-30x more efficient</a>. (2) SLMs are inherently more operationally suitable for agentic systems due to their faster inference, lower latency, and ability to run on edge devices. (3) SLMs are necessarily more economical, offering significant cost savings in inference, fine-tuning, and deployment.</p><p>The paper addresses counterarguments about LLMs&rsquo; superior language understanding and centralization benefits with studies (see Appendix B: <em>LLM-to-SLM Replacement Case Studies</em>) showing that 40-70% of LLM queries in popular open-source agents (MetaGPT, Open Operator, Cradle) could be replaced by specialized SLMs. One comment I read <a href="https://news.ycombinator.com/item?id=44432478">raised important concerns</a> about the paper&rsquo;s analysis, particularly regarding context window which are arguably the highest technical barrier to SLM adoption in agentic systems. Modern agentic applications require substantial context: Claude 4 Sonnet&rsquo;s system prompt alone <a href=https://simonwillison.net/2025/May/25/claude-4-system-prompt/>reportedly uses around 25k tokens</a>, and a typical coding agent needs system instructions, tool definitions, file context, and project documentation, totaling 5-10k tokens before any actual work begins. Most SLMs that can run on consumer hardware are capped at 32k or 128k contexts architecturally, but achieving reasonable inference speeds at these limits requires gaming hardware (8GB VRAM for a 7b model at 128k context).</p><p>The paper concludes that the shift to SLMs is inevitable due to economic and operational advantages, despite current barriers including infrastructure investment in LLM serving, generalist benchmark focus, and limited awareness of SLM capabilities. But the economic efficiency claims also face scrutiny under system-level analysis. In Section 3.2 they present simplistic FLOP comparisons while ignoring critical inefficiencies: the reliance on <a href=https://www.promptingguide.ai/techniques/fewshot>multishot-prompting</a> where SLMs might require 3-4 attempts for tasks that LLMs complete with 90% success rate, task decomposition overhead that multiplies context setup costs and error rates, and infrastructure efficiency differences between optimized datacenters (PUE ratios near 1.1, >90% GPU utilization) and consumer hardware (5-10% GPU utilization, residential HVAC, 80-85% power conversion efficiency). When accounting for failed attempts, orchestration overhead, and infrastructure efficiency, many &ldquo;economical&rdquo; SLM deployments might actually consume more total energy than centralized LLM inference.</p><p><em>(05/07/2025) Update: On the topic of speed I just came across <a href=https://arxiv.org/abs/2506.17298>Ultra-Fast Language Models Based on Diffusion</a>. You can also test it yourself using the <a href=https://chat.inceptionlabs.ai/>free playground link</a>, and it is in fact extremely fast. Try the &ldquo;Diffusion Effect&rdquo; in the top right corner which toggles an interesting visualization. I&rsquo;m not sure how realistic this is, it shows text appearing as random noise before gradually resolving into clear words; though the actual process likely involves tokens evolving from imprecise vectors in a multidimensional space toward more precise representations until they crystallize into specific words.</em></p><p><em>(06/07/2025) Update II: Apparently there is also a <a href=https://deepmind.google/models/gemini-diffusion/>Google DeepMind Gemini Diffusion Model</a>.</em></p></div></article></main></div></body></html>