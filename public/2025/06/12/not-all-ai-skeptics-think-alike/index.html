<!DOCTYPE html>
<html lang="en-us">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Not All AI Skeptics Think Alike - Mainframe</title>
<meta name="description" content="Curated Articles and Papers on Economics, Finance and Technology">


<meta name="keywords" content="Finance,Economics,Technology,Data,Machine Learning">


<link rel="canonical" href="http://localhost:1313/2025/06/12/not-all-ai-skeptics-think-alike/">
<link rel="alternate" type="application/rss+xml" title="Mainframe" href="http://localhost:1313//index.xml">

<link rel="stylesheet" href="/css/custom.css">  
</head>
<body class="linkblog">
    <div class="container">
        <aside class="sidebar">
    <div class="sidebar-content">
        <div class="site-header">
            <h1 class="site-title">
                <a href="http://localhost:1313/">Mainframe</a>
            </h1>
            <p class="site-description">Curated Articles and Papers on Economics, Finance and Technology</p>
        </div>
        
        <nav class="navigation">
            <ul>
                
                <li><a href="/">Home</a></li>
                
                <li><a href="/projects/">Projects</a></li>
                
                <li><a href="/about/">About</a></li>
                
                <li><a href="/posts/">Archive</a></li>
                
                <li><a href="/index.xml">RSS</a></li>
                
            </ul>
        </nav>
        
        
        <div class="social-links">
            
            
            <a href="https://github.com/philippdubach" target="_blank" rel="noopener">GitHub</a>
            
            
            <a href="mailto:info@philippdubach.com">Email</a>
            
        </div>
        
    </div>
</aside>
        <main class="content">
            
            
<article class="post single">
    <header class="post-header">
        <h1 class="post-title">
            
                <a href="https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf" target="_blank" rel="noopener">
                    Not All AI Skeptics Think Alike
                    <span class="external-link">→</span>
                </a>
            
        </h1>
        
        <div class="post-meta">
            <time datetime="2025-06-12T00:00:00Z">
                June 12, 2025 at 12:00 AM
            </time>
            
                • <a href="https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf" target="_blank" rel="noopener">Original Link</a>
            
        </div>
        
    </header>
    <div class="post-content">
        <p>Apple&rsquo;s recent paper &ldquo;The Illusion of Thinking&rdquo; has been widely understood to demonstrate that reasoning models don&rsquo;t &lsquo;actually&rsquo; reason. Using controllable puzzle environments instead of contaminated math benchmarks, they discovered something fascinating: there are three distinct performance regimes when it comes to AI reasoning complexity. For simple problems, standard models actually outperform reasoning models while being more token-efficient. At medium complexity, reasoning models show their advantage. But at high complexity? Both collapse completely.
Here&rsquo;s the kicker: reasoning models exhibit counterintuitive scaling behavior—their thinking effort increases with problem complexity up to a point, then declines despite having adequate token budget. It&rsquo;s like watching a student give up mid-exam when the questions get too hard, even though they have plenty of time left.</p>
<blockquote>
<p>We observe that reasoning models initially increase their thinking tokens proportionally with problem complexity. However, upon approaching a critical threshold—which closely corresponds to their accuracy collapse point—models counterintuitively begin to reduce their reasoning effort despite increasing problem difficulty.</p></blockquote>
<p>The researchers found something even more surprising: even when they provided explicit algorithms—essentially giving the models the answers—performance didn&rsquo;t improve. The collapse happened at roughly the same complexity threshold. On the other hand <a href="https://www.seangoedecke.com/illusion-of-thinking/">Sean Goedecke</a> is not buying Apple&rsquo;s methodology: His core objection? Puzzles &ldquo;require computer-like algorithm-following more than they require the kind of reasoning you need to solve math problems.&rdquo;</p>
<blockquote>
<p>You can&rsquo;t compare eight-disk to ten-disk Tower of Hanoi, because you&rsquo;re comparing &ldquo;can the model work through the algorithm&rdquo; to &ldquo;can the model invent a solution that avoids having to work through the algorithm&rdquo;.</p></blockquote>
<p>From his own testing, models &ldquo;decide early on that hundreds of algorithmic steps are too many to even attempt, so they refuse to even start.&ldquo;That&rsquo;s strategic behavior, not reasoning failure. This matters because it shows how evaluation methodology shapes our understanding of AI capabilities. Goedecke argues Tower of Hanoi puzzles aren&rsquo;t useful for determining reasoning ability, and that the complexity threshold of reasoning models may not be fixed.</p>

    </div>
</article>

        </main>
    </div>
    



</body>
</html>