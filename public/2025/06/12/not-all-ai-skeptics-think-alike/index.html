<!doctype html><html lang=en-us><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Not All AI Skeptics Think Alike - philippdubach</title><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Not All AI Skeptics Think Alike - philippdubach</title><meta name=description content="Personal Projects, Curated Articles and Papers on Economics, Finance and Technology"><meta name=keywords content="Finance,Economics,Technology,Data,Machine Learning"><meta name=author content="Philipp Dubach"><meta name=generator content="Hugo 0.147.8"><link rel=canonical href=http://localhost:1313/2025/06/12/not-all-ai-skeptics-think-alike/><link rel=stylesheet href=/css/custom.css><link rel=icon type=image/png href=https://static.philippdubach.com/icons/favicon-96x96.png sizes=96x96><link rel=icon type=image/svg+xml href=https://static.philippdubach.com/icons/favicon.svg><link rel="shortcut icon" href=https://static.philippdubach.com/icons/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=https://static.philippdubach.com/icons/apple-touch-icon.png><meta name=apple-mobile-web-app-title content="philippdubach"><link rel=manifest href=https://static.philippdubach.com/icons/site.webmanifest><meta name=theme-color content="#2c3e50"><meta name=msapplication-TileColor content="#2c3e50"><meta property="og:title" content="Not All AI Skeptics Think Alike"><meta property="og:description" content="Personal Projects, Curated Articles and Papers on Economics, Finance and Technology"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/2025/06/12/not-all-ai-skeptics-think-alike/"><meta property="og:site_name" content="philippdubach"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://static.philippdubach.com/ograph/ograph-post.jpg"><meta property="article:published_time" content="2025-06-12T00:00:00Z"><meta property="article:modified_time" content="2025-06-12T00:00:00Z"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Not All AI Skeptics Think Alike"><meta name=twitter:description content="Personal Projects, Curated Articles and Papers on Economics, Finance and Technology"><meta name=twitter:image content="https://static.philippdubach.com/ograph/ograph-post.jpg"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"http:\/\/localhost:1313\/"},{"@type":"ListItem","position":2,"name":"Posts","item":"http:\/\/localhost:1313\/posts/"},{"@type":"ListItem","position":3,"name":"Not All AI Skeptics Think Alike","item":"http:\/\/localhost:1313\/2025\/06\/12\/not-all-ai-skeptics-think-alike\/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"Not All AI Skeptics Think Alike","name":"Not All AI Skeptics Think Alike","description":"Apple\u0026rsquo;s recent paper \u0026ldquo;The Illusion of Thinking\u0026rdquo; has been widely understood to demonstrate that reasoning models don\u0026rsquo;t …","articleBody":"\"Apple\\u0026rsquo;s recent paper \\u0026ldquo;The Illusion of Thinking\\u0026rdquo; has been widely understood to demonstrate that reasoning models don\\u0026rsquo;t \\u0026lsquo;actually\\u0026rsquo; reason. Using controllable puzzle environments instead of contaminated math benchmarks, they discovered something fascinating: there are three distinct performance regimes when it comes to AI reasoning complexity. For simple problems, standard models actually outperform reasoning models while being more token-efficient. At medium complexity, reasoning models show their advantage. But at high complexity? Both collapse completely. Here\\u0026rsquo;s the kicker: reasoning models exhibit counterintuitive scaling behavior—their thinking effort increases with problem complexity up to a point, then declines despite having adequate token budget. It\\u0026rsquo;s like watching a student give up mid-exam when the questions get too hard, even though they have plenty of time left.\\nWe observe that reasoning models initially increase their thinking tokens proportionally with problem complexity. However, upon approaching a critical threshold—which closely corresponds to their accuracy collapse point—models counterintuitively begin to reduce their reasoning effort despite increasing problem difficulty.\\nThe researchers found something even more surprising: even when they provided explicit algorithms—essentially giving the models the answers—performance didn\\u0026rsquo;t improve. The collapse happened at roughly the same complexity threshold. On the other hand, Sean Goedecke is not buying Apple\\u0026rsquo;s methodology: His core objection? Puzzles \\u0026ldquo;require computer-like algorithm-following more than they require the kind of reasoning you need to solve math problems.\\u0026rdquo;\\nYou can\\u0026rsquo;t compare eight-disk to ten-disk Tower of Hanoi, because you\\u0026rsquo;re comparing \\u0026ldquo;can the model work through the algorithm\\u0026rdquo; to \\u0026ldquo;can the model invent a solution that avoids having to work through the algorithm\\u0026rdquo;.\\nFrom his own testing, models \\u0026ldquo;decide early on that hundreds of algorithmic steps are too many to even attempt, so they refuse to even start.\\u0026rdquo; That\\u0026rsquo;s strategic behavior, not reasoning failure. This matters because it shows how evaluation methodology shapes our understanding of AI capabilities. Goedecke argues Tower of Hanoi puzzles aren\\u0026rsquo;t useful for determining reasoning ability, and that the complexity threshold of reasoning models may not be fixed.\\n\"","wordCount":323,"inLanguage":"en","image":"https:\/\/static.philippdubach.com\/ograph\/ograph-post.jpg","datePublished":"2025-06-12T00:00:00Z","dateModified":"2025-06-12T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/localhost:1313\/2025\/06\/12\/not-all-ai-skeptics-think-alike\/"},"publisher":{"@type":"Organization","name":"philippdubach","logo":{"@type":"ImageObject","url":"http:\/\/localhost:1313\/icons/favicon.ico"}},"author":{"@type":"Person","name":"Philipp Dubach"}}</script></head></head><body class=linkblog><div class=container><aside class=sidebar><div class=sidebar-content><div class=site-header><h1 class=site-title><a href=http://localhost:1313/>philippdubach</a></h1><p class=site-description>Personal Projects, Curated Articles and Papers on Economics, Finance and Technology</p></div><nav class=navigation><ul><li><a href=/>Home</a></li><li><a href=/projects/>Projects</a></li><li><a href=/about/>About</a></li><li><a href=/posts/>Archive</a></li><li><a href=/index.xml>RSS</a></li></ul></nav><div class=social-links><a href=https://github.com/philippdubach target=_blank rel=noopener>GitHub</a>
<a href=mailto:info@philippdubach.com>Email</a></div></div></aside><main class=content><article class="post single"><header class=post-header><h1 class=post-title><a href=https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf target=_blank rel=noopener>Not All AI Skeptics Think Alike
<span class=external-link>→</span></a></h1><div class=post-meta><time datetime=2025-06-12T00>June 12, 2025
</time>• <a href=https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf target=_blank rel=noopener>Original Link</a></div></header><div class=post-content><p>Apple&rsquo;s recent paper &ldquo;The Illusion of Thinking&rdquo; has been widely understood to demonstrate that reasoning models don&rsquo;t &lsquo;actually&rsquo; reason. Using controllable puzzle environments instead of contaminated math benchmarks, they discovered something fascinating: there are three distinct performance regimes when it comes to AI reasoning complexity. For simple problems, standard models actually outperform reasoning models while being more token-efficient. At medium complexity, reasoning models show their advantage. But at high complexity? Both collapse completely.
Here&rsquo;s the kicker: reasoning models exhibit counterintuitive scaling behavior—their thinking effort increases with problem complexity up to a point, then declines despite having adequate token budget. It&rsquo;s like watching a student give up mid-exam when the questions get too hard, even though they have plenty of time left.</p><blockquote><p>We observe that reasoning models initially increase their thinking tokens proportionally with problem complexity. However, upon approaching a critical threshold—which closely corresponds to their accuracy collapse point—models counterintuitively begin to reduce their reasoning effort despite increasing problem difficulty.</p></blockquote><p>The researchers found something even more surprising: even when they provided explicit algorithms—essentially giving the models the answers—performance didn&rsquo;t improve. The collapse happened at roughly the same complexity threshold. On the other hand, <a href=https://www.seangoedecke.com/illusion-of-thinking/>Sean Goedecke</a> is not buying Apple&rsquo;s methodology: His core objection? Puzzles &ldquo;require computer-like algorithm-following more than they require the kind of reasoning you need to solve math problems.&rdquo;</p><blockquote><p>You can&rsquo;t compare eight-disk to ten-disk Tower of Hanoi, because you&rsquo;re comparing &ldquo;can the model work through the algorithm&rdquo; to &ldquo;can the model invent a solution that avoids having to work through the algorithm&rdquo;.</p></blockquote><p>From his own testing, models &ldquo;decide early on that hundreds of algorithmic steps are too many to even attempt, so they refuse to even start.&rdquo; That&rsquo;s strategic behavior, not reasoning failure. This matters because it shows how evaluation methodology shapes our understanding of AI capabilities. Goedecke argues Tower of Hanoi puzzles aren&rsquo;t useful for determining reasoning ability, and that the complexity threshold of reasoning models may not be fixed.</p></div></article></main></div></body></html>