<!doctype html><html lang=en-us><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Not All AI Skeptics Think Alike - philippdubach.com</title><meta name=description content="Apple's 'The Illusion of Thinking' paper reveals three distinct performance regimes in AI reasoning models, showing they collapse at high complexity despite adequate resources, while critics argue the puzzle-based methodology may not accurately measure true reasoning capabilities."><meta name=keywords content="AI reasoning models,Apple Illusion of Thinking,AI skeptics,reasoning complexity,Tower of Hanoi puzzles,AI evaluation methodology,thinking tokens,algorithmic reasoning,AI performance regimes,reasoning failure,AI capabilities assessment,complexity threshold,AI benchmarks,Sean Goedecke,AI scaling behavior"><meta name=author content="Philipp Dubach"><meta name=generator content="Hugo 0.149.0"><link rel=canonical href=http://localhost:1313/2025/06/12/not-all-ai-skeptics-think-alike/><style>*{margin:0;padding:0;box-sizing:border-box}body{font-family:helvetica neue,Helvetica,Arial,segoe ui,sans-serif;padding-top:90px;line-height:1.6;color:#333;background-color:#f8f9fa}blockquote{margin:1em 0;padding-left:1em;border-left:2px solid #ccc;color:#666}blockquote p{margin:.5em 0}.container{display:flex;max-width:1200px;margin:0 auto;min-height:100vh}.sidebar{width:200px;background-color:#f8f9fa;padding:1.5rem;border-right:0 solid #e9ecef;position:sticky;top:0;height:100vh;overflow-y:auto}.site-title a{text-decoration:none;color:#333;font-size:1.5rem;font-weight:700}.site-description{color:#666;margin:.5rem 0 2rem;font-size:.9rem}.navigation ul{list-style:none;margin-bottom:2rem}.navigation li{margin-bottom:.5rem}.navigation a{text-decoration:none;color:#007acc;font-weight:500}.navigation a:hover{text-decoration:underline}.social-links a{display:inline-block;margin-right:1rem;color:#666;text-decoration:none;font-size:.9rem}.social-links a:hover{color:#007acc}.content{flex:1;padding:2rem;padding-bottom:3rem;max-width:800px}.posts{max-width:90%}.post{margin-bottom:1rem;padding-bottom:1rem}.post:last-child{border-bottom:none}.post-title{margin-bottom:.5rem;line-height:1.3}.post-title a{text-decoration:none;color:#333;font-size:1.25rem;font-weight:600;line-height:1.3}.post-title a:hover{color:#007acc}.post-meta{font-size:.85rem;color:#666;margin-bottom:0}.post-meta a{color:#007acc;text-decoration:none}.post-meta a:hover{text-decoration:underline}.post-content{line-height:1.7}.post-content p{margin-bottom:1rem}.post-content p:last-child{margin-bottom:.5rem}.post-content a{color:#007acc;text-decoration:none}.post-content a:hover{text-decoration:underline}.project-tag{background-color:#e9ecef;color:#495057;padding:2px 6px;border-radius:3px;font-size:9px;font-weight:500;text-transform:uppercase;letter-spacing:.3px;margin-left:6px;display:inline-block;vertical-align:middle;border:1px solid #dee2e6}.archive{max-width:90%}.year h2{margin:2rem 0 1rem;color:#333;font-size:1.5rem}.archive-item{display:flex;margin-bottom:.75rem;align-items:baseline}.archive-meta{width:60px;flex-shrink:0;font-size:.85rem;color:#666}.archive-title{flex:1}.archive-title a{text-decoration:none;color:#333}.archive-title a:hover{color:#007acc}@supports(text-wrap:balance){.archive-title{text-wrap:balance}}.single .post-title{font-size:1.75rem;margin-bottom:1rem;line-height:1.3}.pagination{margin-top:2rem;margin-bottom:1rem;text-align:center;padding-top:2rem;padding-bottom:1rem;border-top:1px solid #e9ecef}.pagination a{color:#007acc;text-decoration:none;font-weight:500}.pagination a:hover{text-decoration:underline}.img-lightbox{cursor:pointer;transition:opacity .2s}.img-lightbox:hover{opacity:.9}.lightbox-overlay{display:none;position:fixed;top:0;left:0;width:100%;height:100%;background:#f8f9fa;z-index:9999;cursor:pointer;align-items:center;justify-content:center;padding:2rem;box-sizing:border-box}.lightbox-overlay:target{display:flex}.lightbox-overlay img{max-width:95%;max-height:95%;object-fit:contain;background:#f8f9fa}.feedback-footer{margin-top:.75rem;padding-top:.75rem;border-top:1px solid #e9ecef;text-align:center;color:#666;font-size:.9rem;margin-bottom:2rem}.feedback-footer p{margin:0;line-height:1.6}.feedback-footer a{color:#007acc;text-decoration:none}.feedback-footer a:hover{text-decoration:underline}@media screen and (max-width:768px){html{font-size:8px !important}body{font-size:1rem !important}.sidebar{width:100px !important;padding:.75rem !important}.container{max-width:600px !important;margin:0 20px !important}.content{max-width:400px !important;padding:1rem !important;padding-bottom:.5rem !important}.pagination{margin-bottom:1.5rem !important;padding-bottom:1.5rem !important}.project-tag{padding:1px 3px !important;border-radius:1.5px !important;font-size:4.5px !important;letter-spacing:.15px !important;margin-left:3px !important}.archive-meta{width:30px !important}.post-title,.post-title a,.single .post-title{line-height:1.2 !important}.post-content p:last-child{margin-bottom:.25rem !important}.feedback-footer{margin-top:.5rem !important;padding-top:.5rem !important;margin-bottom:1rem !important}}</style><link rel=icon type=image/png href=/icons/favicon-96x96.png sizes=96x96><link rel=icon type=image/svg+xml href=/icons/favicon.svg><link rel="shortcut icon" href=/icons/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/icons/apple-touch-icon.png><meta name=apple-mobile-web-app-title content="philippdubach.com"><link rel=manifest href=/icons/site.webmanifest><meta name=theme-color content="#2c3e50"><meta name=msapplication-TileColor content="#2c3e50"><meta property="og:title" content="Not All AI Skeptics Think Alike"><meta property="og:description" content="Apple's 'The Illusion of Thinking' paper reveals three distinct performance regimes in AI reasoning models, showing they collapse at high complexity despite adequate resources, while critics argue the puzzle-based methodology may not accurately measure true reasoning capabilities."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/2025/06/12/not-all-ai-skeptics-think-alike/"><meta property="og:site_name" content="philippdubach.com"><meta property="og:locale" content="en_US"><meta property="og:logo" content="http://localhost:1313/icons/favicon.ico"><meta property="og:image" content="https://static.philippdubach.com/ograph/ograph-aisceptics.jpg"><meta property="article:published_time" content="2025-06-12T00:00:00Z"><meta property="article:modified_time" content="2025-06-12T00:00:00Z"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Not All AI Skeptics Think Alike"><meta name=twitter:description content="Apple's 'The Illusion of Thinking' paper reveals three distinct performance regimes in AI reasoning models, showing they collapse at high complexity despite adequate resources, while critics argue the puzzle-based methodology may not accurately measure true reasoning capabilities."><meta name=twitter:image content="https://static.philippdubach.com/ograph/ograph-aisceptics.jpg"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"http:\/\/localhost:1313\/"},{"@type":"ListItem","position":2,"name":"Posts","item":"http:\/\/localhost:1313\/posts/"},{"@type":"ListItem","position":3,"name":"Not All AI Skeptics Think Alike","item":"http:\/\/localhost:1313\/2025\/06\/12\/not-all-ai-skeptics-think-alike\/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"Not All AI Skeptics Think Alike","name":"Not All AI Skeptics Think Alike","description":"Apple\u0027s \u0027The Illusion of Thinking\u0027 paper reveals three distinct performance regimes in AI reasoning models, showing they collapse at high complexity despite adequate resources, while critics argue the puzzle-based methodology may not accurately measure true reasoning capabilities.","keywords":["AI reasoning models","Apple Illusion of Thinking","AI skeptics","reasoning complexity","Tower of Hanoi puzzles","AI evaluation methodology","thinking tokens","algorithmic reasoning","AI performance regimes","reasoning failure","AI capabilities assessment","complexity threshold","AI benchmarks","Sean Goedecke","AI scaling behavior"],"articleBody":"\"Apple\\u0026rsquo;s recent paper \\u0026ldquo;The Illusion of Thinking\\u0026rdquo; has been widely understood to demonstrate that reasoning models don\\u0026rsquo;t \\u0026lsquo;actually\\u0026rsquo; reason. Using controllable puzzle environments instead of contaminated math benchmarks, they discovered something fascinating: there are three distinct performance regimes when it comes to AI reasoning complexity. For simple problems, standard models actually outperform reasoning models while being more token-efficient. At medium complexity, reasoning models show their advantage. But at high complexity? Both collapse completely. Here\\u0026rsquo;s the kicker: reasoning models exhibit counterintuitive scaling behavior—their thinking effort increases with problem complexity up to a point, then declines despite having adequate token budget. It\\u0026rsquo;s like watching a student give up mid-exam when the questions get too hard, even though they have plenty of time left.\\nWe observe that reasoning models initially increase their thinking tokens proportionally with problem complexity. However, upon approaching a critical threshold—which closely corresponds to their accuracy collapse point—models counterintuitively begin to reduce their reasoning effort despite increasing problem difficulty.\\nThe researchers found something even more surprising: even when they provided explicit algorithms—essentially giving the models the answers—performance didn\\u0026rsquo;t improve. The collapse happened at roughly the same complexity threshold. On the other hand, Sean Goedecke is not buying Apple\\u0026rsquo;s methodology: His core objection? Puzzles \\u0026ldquo;require computer-like algorithm-following more than they require the kind of reasoning you need to solve math problems.\\u0026rdquo;\\nYou can\\u0026rsquo;t compare eight-disk to ten-disk Tower of Hanoi, because you\\u0026rsquo;re comparing \\u0026ldquo;can the model work through the algorithm\\u0026rdquo; to \\u0026ldquo;can the model invent a solution that avoids having to work through the algorithm\\u0026rdquo;.\\nFrom his own testing, models \\u0026ldquo;decide early on that hundreds of algorithmic steps are too many to even attempt, so they refuse to even start.\\u0026rdquo; That\\u0026rsquo;s strategic behavior, not reasoning failure. This matters because it shows how evaluation methodology shapes our understanding of AI capabilities. Goedecke argues Tower of Hanoi puzzles aren\\u0026rsquo;t useful for determining reasoning ability, and that the complexity threshold of reasoning models may not be fixed.\\n\"","wordCount":323,"inLanguage":"en","image":"https:\/\/static.philippdubach.com\/ograph\/ograph-aisceptics.jpg","datePublished":"2025-06-12T00:00:00Z","dateModified":"2025-06-12T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/localhost:1313\/2025\/06\/12\/not-all-ai-skeptics-think-alike\/"},"publisher":{"@type":"Organization","name":"philippdubach","logo":{"@type":"ImageObject","url":"http:\/\/localhost:1313\/icons/favicon.ico"}},"author":{"@type":"Person","name":"Philipp Dubach"}}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"http:\/\/localhost:1313\/"},{"@type":"ListItem","position":2,"name":"Posts","item":"http:\/\/localhost:1313\/posts/"},{"@type":"ListItem","position":3,"name":"Not All AI Skeptics Think Alike","item":"http:\/\/localhost:1313\/2025\/06\/12\/not-all-ai-skeptics-think-alike\/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"Not All AI Skeptics Think Alike","name":"Not All AI Skeptics Think Alike","description":"Apple\u0027s \u0027The Illusion of Thinking\u0027 paper reveals three distinct performance regimes in AI reasoning models, showing they collapse at high complexity despite adequate resources, while critics argue the puzzle-based methodology may not accurately measure true reasoning capabilities.","keywords":["AI reasoning models","Apple Illusion of Thinking","AI skeptics","reasoning complexity","Tower of Hanoi puzzles","AI evaluation methodology","thinking tokens","algorithmic reasoning","AI performance regimes","reasoning failure","AI capabilities assessment","complexity threshold","AI benchmarks","Sean Goedecke","AI scaling behavior"],"articleBody":"\"Apple\\u0026rsquo;s recent paper \\u0026ldquo;The Illusion of Thinking\\u0026rdquo; has been widely understood to demonstrate that reasoning models don\\u0026rsquo;t \\u0026lsquo;actually\\u0026rsquo; reason. Using controllable puzzle environments instead of contaminated math benchmarks, they discovered something fascinating: there are three distinct performance regimes when it comes to AI reasoning complexity. For simple problems, standard models actually outperform reasoning models while being more token-efficient. At medium complexity, reasoning models show their advantage. But at high complexity? Both collapse completely. Here\\u0026rsquo;s the kicker: reasoning models exhibit counterintuitive scaling behavior—their thinking effort increases with problem complexity up to a point, then declines despite having adequate token budget. It\\u0026rsquo;s like watching a student give up mid-exam when the questions get too hard, even though they have plenty of time left.\\nWe observe that reasoning models initially increase their thinking tokens proportionally with problem complexity. However, upon approaching a critical threshold—which closely corresponds to their accuracy collapse point—models counterintuitively begin to reduce their reasoning effort despite increasing problem difficulty.\\nThe researchers found something even more surprising: even when they provided explicit algorithms—essentially giving the models the answers—performance didn\\u0026rsquo;t improve. The collapse happened at roughly the same complexity threshold. On the other hand, Sean Goedecke is not buying Apple\\u0026rsquo;s methodology: His core objection? Puzzles \\u0026ldquo;require computer-like algorithm-following more than they require the kind of reasoning you need to solve math problems.\\u0026rdquo;\\nYou can\\u0026rsquo;t compare eight-disk to ten-disk Tower of Hanoi, because you\\u0026rsquo;re comparing \\u0026ldquo;can the model work through the algorithm\\u0026rdquo; to \\u0026ldquo;can the model invent a solution that avoids having to work through the algorithm\\u0026rdquo;.\\nFrom his own testing, models \\u0026ldquo;decide early on that hundreds of algorithmic steps are too many to even attempt, so they refuse to even start.\\u0026rdquo; That\\u0026rsquo;s strategic behavior, not reasoning failure. This matters because it shows how evaluation methodology shapes our understanding of AI capabilities. Goedecke argues Tower of Hanoi puzzles aren\\u0026rsquo;t useful for determining reasoning ability, and that the complexity threshold of reasoning models may not be fixed.\\n\"","wordCount":323,"inLanguage":"en","image":"https:\/\/static.philippdubach.com\/ograph\/ograph-aisceptics.jpg","datePublished":"2025-06-12T00:00:00Z","dateModified":"2025-06-12T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/localhost:1313\/2025\/06\/12\/not-all-ai-skeptics-think-alike\/"},"publisher":{"@type":"Organization","name":"philippdubach","logo":{"@type":"ImageObject","url":"http:\/\/localhost:1313\/icons/favicon.ico"}},"author":{"@type":"Person","name":"Philipp Dubach"}}</script><script>(function(){let e=[],n=[],t=null;const s=["offsetHeight","offsetWidth","offsetTop","offsetLeft","scrollHeight","scrollWidth","scrollTop","scrollLeft","clientHeight","clientWidth","clientTop","clientLeft"];s.forEach(t=>{const s=Object.getOwnPropertyDescriptor(HTMLElement.prototype,t);s&&s.get&&Object.defineProperty(HTMLElement.prototype,t,{get:function(){return e.length>0&&(console.warn(`⚠️ FORCED REFLOW: Reading ${t} after DOM write!`),console.log("Recent writes:",e),console.log("Element:",this),console.trace()),n.push({prop:t,element:this.tagName}),s.get.call(this)}})});const o=Object.getOwnPropertyDescriptor(HTMLElement.prototype,"style");Object.defineProperty(HTMLElement.prototype,"style",{get:function(){const s=o.get.call(this);return new Proxy(s,{set:function(s,o,i){return e.push({property:o,value:i,element:s.parentElement?.tagName}),n=[],t||(t=requestAnimationFrame(()=>{e=[],t=null})),s[o]=i,!0}})}});const i=Element.prototype.getBoundingClientRect;Element.prototype.getBoundingClientRect=function(){return e.length>0&&(console.warn("⚠️ FORCED REFLOW: getBoundingClientRect() called after DOM write!"),console.trace()),i.call(this)}})()</script></head><body class=linkblog><div class=container><aside class=sidebar><div class=sidebar-content><div class=site-header><h1 class=site-title><a href=http://localhost:1313/>philippdubach</a></h1><p class=site-description>Blog about Personal Projects, Articles and Papers on Economics, Finance and Technology</p></div><nav class=navigation><ul><li><a href=/>Home</a></li><li><a href=/projects/>Projects</a></li><li><a href=/about/>About</a></li><li><a href=/posts/>Archive</a></li><li><a href=/index.xml>RSS</a></li></ul></nav><div class=social-links><a href=https://github.com/philippdubach target=_blank rel=noopener>GitHub</a>
<a href=mailto:info@philippdubach.com>Email</a></div></div></aside><main class=content><article class="post single"><header class=post-header><h1 class=post-title>Not All AI Skeptics Think Alike</h1><div class=post-meta><time datetime=2025-06-12T00:00:00Z>June 12, 2025
</time>• 400 words
• 2 min read
• <a href=https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf target=_blank rel=noopener>via</a></div></header><div class=post-content><p>Apple&rsquo;s recent paper &ldquo;The Illusion of Thinking&rdquo; has been widely understood to demonstrate that reasoning models don&rsquo;t &lsquo;actually&rsquo; reason. Using controllable puzzle environments instead of contaminated math benchmarks, they discovered something fascinating: there are three distinct performance regimes when it comes to AI reasoning complexity. For simple problems, standard models actually outperform reasoning models while being more token-efficient. At medium complexity, reasoning models show their advantage. But at high complexity? Both collapse completely.
Here&rsquo;s the kicker: reasoning models exhibit counterintuitive scaling behavior—their thinking effort increases with problem complexity up to a point, then declines despite having adequate token budget. It&rsquo;s like watching a student give up mid-exam when the questions get too hard, even though they have plenty of time left.</p><blockquote><p>We observe that reasoning models initially increase their thinking tokens proportionally with problem complexity. However, upon approaching a critical threshold—which closely corresponds to their accuracy collapse point—models counterintuitively begin to reduce their reasoning effort despite increasing problem difficulty.</p></blockquote><p>The researchers found something even more surprising: even when they provided explicit algorithms—essentially giving the models the answers—performance didn&rsquo;t improve. The collapse happened at roughly the same complexity threshold. On the other hand, <a href=https://www.seangoedecke.com/illusion-of-thinking/>Sean Goedecke</a> is not buying Apple&rsquo;s methodology: His core objection? Puzzles &ldquo;require computer-like algorithm-following more than they require the kind of reasoning you need to solve math problems.&rdquo;</p><blockquote><p>You can&rsquo;t compare eight-disk to ten-disk Tower of Hanoi, because you&rsquo;re comparing &ldquo;can the model work through the algorithm&rdquo; to &ldquo;can the model invent a solution that avoids having to work through the algorithm&rdquo;.</p></blockquote><p>From his own testing, models &ldquo;decide early on that hundreds of algorithmic steps are too many to even attempt, so they refuse to even start.&rdquo; That&rsquo;s strategic behavior, not reasoning failure. This matters because it shows how evaluation methodology shapes our understanding of AI capabilities. Goedecke argues Tower of Hanoi puzzles aren&rsquo;t useful for determining reasoning ability, and that the complexity threshold of reasoning models may not be fixed.</p></div></article><footer class=feedback-footer><p>Have feedback, comments, or ideas? <a href=mailto:info@philippdubach.com>I'd love to hear from you</a>.</p><p class=latest-post>Latest: <a href=/2025/11/24/is-ai-really-eating-the-world-agi-networks-value-2/2/>Is AI Really Eating the World? AGI, Networks, Value [2/2]</a></p><p class=most-read id=top-post-week style=min-height:1.5em></p></footer><script src=/js/goatcounter-top.js></script></main></div></body></html>