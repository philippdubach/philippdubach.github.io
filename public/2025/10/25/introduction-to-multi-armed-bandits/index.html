<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta http-equiv=Content-Security-Policy content="default-src 'self'; script-src 'self' 'unsafe-inline' https://gc.zgo.at https://cdn.jsdelivr.net; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; font-src 'self' data:; connect-src 'self' https://philippdubach.goatcounter.com https://weekly-top-goatcounter-api.philippd.workers.dev https://gc.zgo.at; frame-ancestors 'self'; base-uri 'self';"><link rel=preconnect href=https://gc.zgo.at crossorigin><link rel=preconnect href=https://static.philippdubach.com crossorigin><link rel=dns-prefetch href=https://gc.zgo.at><link rel=dns-prefetch href=https://static.philippdubach.com><meta name=robots content="index, follow"><title>Introduction to Multi-Armed Bandits - philippdubach.com</title><meta name=description content="Blog about Personal Projects, Articles and Papers on Economics, Finance and Technology"><meta name=keywords content="Finance,Economics,Technology,Data,Machine Learning"><meta name=author content="Philipp Dubach"><meta name=generator content="Hugo 0.149.0"><link rel=canonical href=https://philippdubach.com/2025/10/25/introduction-to-multi-armed-bandits/><style>*{margin:0;padding:0;box-sizing:border-box}body{font-family:helvetica neue,Helvetica,Arial,segoe ui,sans-serif;padding-top:90px;line-height:1.6;color:#333;background-color:#ffff}blockquote{margin:1em 0;padding-left:1em;border-left:2px solid #ccc;color:#666}blockquote p{margin:.5em 0}.container{display:flex;max-width:1200px;margin:0 auto;min-height:100vh}.sidebar{width:200px;background-color:#ffff;padding:1.5rem;border-right:0 solid #e9ecef;position:sticky;top:0;height:100vh;overflow-y:auto}.site-title a{text-decoration:none;color:#333;font-size:1.5rem;font-weight:700}.site-description{color:#666;margin:.5rem 0 2rem;font-size:.9rem}.navigation ul{list-style:none;margin-bottom:2rem}.navigation li{margin-bottom:.5rem}.navigation a{text-decoration:none;color:#007acc;font-weight:500}.navigation a:hover{text-decoration:underline}.social-links a{display:inline-block;margin-right:1rem;color:#666;text-decoration:none;font-size:.9rem}.social-links a:hover{color:#007acc}.content{flex:1;padding:2rem;padding-bottom:3rem;max-width:800px}.posts{max-width:90%}.post{margin-bottom:1rem;padding-bottom:1rem}.post:last-child{border-bottom:none}.post-title{margin-bottom:.5rem;line-height:1.3}.post-title a{text-decoration:none;color:#333;font-size:1.25rem;font-weight:600;line-height:1.3}.post-title a:hover{color:#007acc}.post-meta{font-size:.85rem;color:#666;margin-bottom:0}.post-meta a{color:#007acc;text-decoration:none}.post-meta a:hover{text-decoration:underline}.post-content{line-height:1.7}.post-content p{margin-bottom:1rem}.post-content p:last-child{margin-bottom:.5rem}.post-content a{color:#007acc;text-decoration:none}.post-content a:hover{text-decoration:underline}.project-tag{background-color:#e9ecef;color:#495057;padding:2px 6px;border-radius:3px;font-size:9px;font-weight:500;text-transform:uppercase;letter-spacing:.3px;margin-left:6px;display:inline-block;vertical-align:middle;border:1px solid #dee2e6}.archive{max-width:90%}.year h2{margin:2rem 0 1rem;color:#333;font-size:1.5rem}.archive-item{display:flex;margin-bottom:.75rem;align-items:baseline}.archive-meta{width:60px;flex-shrink:0;font-size:.85rem;color:#666}.archive-title{flex:1}.archive-title a{text-decoration:none;color:#333}.archive-title a:hover{color:#007acc}@supports(text-wrap:balance){.archive-title{text-wrap:balance}}.single .post-title{font-size:1.5rem;margin-bottom:1rem;line-height:1.3}.pagination{margin-top:2rem;margin-bottom:1rem;text-align:center;padding-top:2rem;padding-bottom:1rem;border-top:1px solid #e9ecef}.pagination a{color:#007acc;text-decoration:none;font-weight:500}.pagination a:hover{text-decoration:underline}.img-lightbox{cursor:pointer;transition:opacity .2s}.img-lightbox:hover{opacity:.9}.lightbox-overlay{display:none;position:fixed;top:0;left:0;width:100%;height:100%;background:#f8f9fa;z-index:9999;cursor:pointer;align-items:center;justify-content:center;padding:2rem;box-sizing:border-box}.lightbox-overlay:target{display:flex}.lightbox-overlay img{max-width:95%;max-height:95%;object-fit:contain;background:#f8f9fa}.feedback-footer{margin-top:.75rem;padding-top:.75rem;border-top:1px solid #e9ecef;text-align:center;color:#666;font-size:.9rem;margin-bottom:2rem}.feedback-footer p{margin:0;line-height:1.6}.feedback-footer a{color:#007acc;text-decoration:none}.feedback-footer a:hover{text-decoration:underline}@media screen and (max-width:768px){html{font-size:8px !important}body{font-size:1rem !important}.sidebar{width:100px !important;padding:.75rem !important}.container{max-width:600px !important;margin:0 20px !important}.content{max-width:400px !important;padding:1rem !important;padding-bottom:.5rem !important}.pagination{margin-bottom:1.5rem !important;padding-bottom:1.5rem !important}.project-tag{padding:1px 3px !important;border-radius:1.5px !important;font-size:4.5px !important;letter-spacing:.15px !important;margin-left:3px !important}.archive-meta{width:30px !important}.post-title,.post-title a,.single .post-title{line-height:1.2 !important}.post-content p:last-child{margin-bottom:.25rem !important}.feedback-footer{margin-top:.5rem !important;padding-top:.5rem !important;margin-bottom:1rem !important}}</style><link rel=icon type=image/png href=/icons/favicon-96x96.png sizes=96x96><link rel=icon type=image/svg+xml href=/icons/favicon.svg><link rel="shortcut icon" href=/icons/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/icons/apple-touch-icon.png><meta name=apple-mobile-web-app-title content="philippdubach.com"><link rel=manifest href=/icons/site.webmanifest><meta name=theme-color content="#2c3e50"><meta name=msapplication-TileColor content="#2c3e50"><meta property="og:title" content="Introduction to Multi-Armed Bandits"><meta property="og:description" content="Blog about Personal Projects, Articles and Papers on Economics, Finance and Technology"><meta property="og:type" content="article"><meta property="og:url" content="https://philippdubach.com/2025/10/25/introduction-to-multi-armed-bandits/"><meta property="og:site_name" content="philippdubach.com"><meta property="og:locale" content="en_US"><meta property="og:logo" content="https://philippdubach.com/icons/favicon.ico"><meta property="og:image" content="https://static.philippdubach.com/ograph/ograph-mab.jpg"><meta property="og:image:secure_url" content="https://static.philippdubach.com/ograph/ograph-mab.jpg"><meta property="og:image:url" content="https://static.philippdubach.com/ograph/ograph-mab.jpg"><meta property="og:image:type" content="image/jpeg"><meta property="article:published_time" content="2025-10-25T00:00:00Z"><meta property="article:modified_time" content="2025-10-25T00:00:00Z"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Introduction to Multi-Armed Bandits"><meta name=twitter:description content="Blog about Personal Projects, Articles and Papers on Economics, Finance and Technology"><meta name=twitter:image content="https://static.philippdubach.com/ograph/ograph-mab.jpg"><meta name=twitter:image:src content="https://static.philippdubach.com/ograph/ograph-mab.jpg"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https:\/\/philippdubach.com\/"},{"@type":"ListItem","position":2,"name":"Posts","item":"https:\/\/philippdubach.com\/posts/"},{"@type":"ListItem","position":3,"name":"Introduction to Multi-Armed Bandits","item":"https:\/\/philippdubach.com\/2025\/10\/25\/introduction-to-multi-armed-bandits\/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"Introduction to Multi-Armed Bandits","name":"Introduction to Multi-Armed Bandits","description":"This 2019 textbook by Slivkins remains one of the best introductions to multi-armed bandits—a deceptively simple framework for algorithms making decisions under …","articleBody":"\"This 2019 textbook by Slivkins remains one of the best introductions to multi-armed bandits—a deceptively simple framework for algorithms making decisions under uncertainty. The book\\u0026rsquo;s structure is pedagogical and comprehensive: four chapters on IID rewards, three on adversarial scenarios, one on contextual bandits, and three on economic applications including learning in repeated games and exploration under incentive constraints.\\nThe framework is elegantly simple: you have K possible actions (arms) and T rounds, choosing an arm each round to maximize total reward. But I\\u0026rsquo;ve learned that implementation reveals complexities the textbook can\\u0026rsquo;t fully capture.\\nI\\u0026rsquo;ve deployed bandits in production for content selection, optimizing for clicks. They converge on optimal mixes far faster than manual A/B tests ever could—especially valuable when new arms arrive constantly (sometimes several times a day across hundreds of thousands of scenarios). The catch? Any changes to context force the bandits back toward exploration, hurting performance temporarily. And when you need to run experiments around the bandit-managed content, independence assumptions break down completely.\\nThe most critical insight I\\u0026rsquo;ve gained: bandits are feedback loops that couple things together in ways that can be difficult to tease apart. You face a choice—treat all traffic as a single universe (breaking experiment independence) or isolate bandits per cohort (making small cohorts take forever to converge). Both approaches put real limits on iteration speed.\\nThe scale question matters too. Despite theoretically powerful techniques, the statistical benefits often don\\u0026rsquo;t justify the added complexity unless you\\u0026rsquo;re operating at very large scale with relatively simple optimization goals. I\\u0026rsquo;ve had to think hard about when the real advantage—a slightly more optimal mix of people in experiment than manual approaches—actually moves the needle.\\nOne particularly striking lesson: a system I built to maximize clicks became stupidly good at surfacing inappropriate content because it got clicks. We eventually had to layer an ML model for arm selection with bandits picking between filtered options, then add more safeties for certain content categories.\\nBut when bandits fit the use case, the value extends beyond optimization. You gain the ability to accurately estimate improvement from other features—quantifying that spacing improvements delivered 10x more value than the Really Big Feature expected to dominate.\\nThe book\\u0026rsquo;s chapters on \\u0026ldquo;bandits with knapsacks\\u0026rdquo; and \\u0026ldquo;bandits and agents\\u0026rdquo; work as standalone surveys, and the appendix provides solid background on concentration and KL-divergence. Six years later, it\\u0026rsquo;s still the introduction I\\u0026rsquo;d recommend—just with the caveat that production deployment requires understanding when not to use them: when you care about information gained in experiments, when cohorts are too small to converge quickly, or when you can\\u0026rsquo;t tolerate the state management complexity.\\nAs I\\u0026rsquo;ve come to think of it: bandits work brilliantly until you need to reason about what they\\u0026rsquo;re doing. Then you wish they didn\\u0026rsquo;t work quite so well.\\n\"","wordCount":460,"inLanguage":"en","image":"https:\/\/static.philippdubach.com\/ograph\/ograph-mab.jpg","datePublished":"2025-10-25T00:00:00Z","dateModified":"2025-10-25T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/philippdubach.com\/2025\/10\/25\/introduction-to-multi-armed-bandits\/"},"publisher":{"@type":"Organization","name":"philippdubach","logo":{"@type":"ImageObject","url":"https:\/\/philippdubach.com\/icons/favicon.ico"}},"author":{"@type":"Person","name":"Philipp Dubach"}}</script></head><body class=linkblog><div class=container><aside class=sidebar><div class=sidebar-content><div class=site-header><div class=site-title><a href=https://philippdubach.com/>philippdubach</a></div><p class=site-description>Blog about Personal Projects, Articles and Papers on Economics, Finance and Technology</p></div><nav class=navigation><ul><li><a href=/>Home</a></li><li><a href=/projects/>Projects</a></li><li><a href=/posts/>Archive</a></li><li><a href=/about/>About</a></li><li><a href=/index.xml>RSS</a></li></ul></nav><div class=social-links><a href=https://github.com/philippdubach target=_blank rel=noopener>GitHub</a>
<a href=mailto:info@philippdubach.com>Email</a></div></div></aside><main class=content><article class="post single"><header class=post-header><h1 class=post-title>Introduction to Multi-Armed Bandits</h1><div class=post-meta><time datetime=2025-10-25T00:00:00Z>October 25, 2025
</time>• 500 words
• 3 min read
• <a href=https://arxiv.org/abs/1904.07272 target=_blank rel=noopener>via</a></div></header><div class=post-content><p>This 2019 textbook by Slivkins remains one of the best introductions to multi-armed bandits—a deceptively simple framework for algorithms making decisions under uncertainty. The book&rsquo;s structure is pedagogical and comprehensive: four chapters on IID rewards, three on adversarial scenarios, one on contextual bandits, and three on economic applications including learning in repeated games and exploration under incentive constraints.</p><p>The framework is elegantly simple: you have K possible actions (arms) and T rounds, choosing an arm each round to maximize total reward. But I&rsquo;ve learned that implementation reveals complexities the textbook can&rsquo;t fully capture.</p><p>I&rsquo;ve deployed bandits in production for content selection, optimizing for clicks. They converge on optimal mixes far faster than manual A/B tests ever could—especially valuable when new arms arrive constantly (sometimes several times a day across hundreds of thousands of scenarios). The catch? Any changes to context force the bandits back toward exploration, hurting performance temporarily. And when you need to run experiments around the bandit-managed content, independence assumptions break down completely.</p><p>The most critical insight I&rsquo;ve gained: bandits are feedback loops that couple things together in ways that can be difficult to tease apart. You face a choice—treat all traffic as a single universe (breaking experiment independence) or isolate bandits per cohort (making small cohorts take forever to converge). Both approaches put real limits on iteration speed.</p><p>The scale question matters too. Despite theoretically powerful techniques, the statistical benefits often don&rsquo;t justify the added complexity unless you&rsquo;re operating at very large scale with relatively simple optimization goals. I&rsquo;ve had to think hard about when the real advantage—a slightly more optimal mix of people in experiment than manual approaches—actually moves the needle.</p><p>One particularly striking lesson: a system I built to maximize clicks became stupidly good at surfacing inappropriate content because it got clicks. We eventually had to layer an ML model for arm selection with bandits picking between filtered options, then add more safeties for certain content categories.</p><p>But when bandits fit the use case, the value extends beyond optimization. You gain the ability to accurately estimate improvement from other features—quantifying that spacing improvements delivered 10x more value than the Really Big Feature expected to dominate.</p><p>The book&rsquo;s chapters on &ldquo;bandits with knapsacks&rdquo; and &ldquo;bandits and agents&rdquo; work as standalone surveys, and the appendix provides solid background on concentration and KL-divergence. Six years later, it&rsquo;s still the introduction I&rsquo;d recommend—just with the caveat that production deployment requires understanding when not to use them: when you care about information gained in experiments, when cohorts are too small to converge quickly, or when you can&rsquo;t tolerate the state management complexity.</p><p>As I&rsquo;ve come to think of it: bandits work brilliantly until you need to reason about what they&rsquo;re doing. Then you wish they didn&rsquo;t work quite so well.</p></div></article><footer class=feedback-footer><p>Have feedback, comments, or ideas? <a href=mailto:info@philippdubach.com>I'd love to hear from you</a>.</p><p class=latest-post>Latest: <a href=/2025/12/15/book-review-why-machines-learn-by-anil-ananthaswamy/>Book Review: Why Machines Learn by Anil Ananthaswamy</a></p><p class=most-read id=top-post-week style=min-height:1.5em></p></footer><script src=/js/goatcounter-top.js></script></main></div><script>window.goatcounter=window.goatcounter||{},document.referrer&&document.referrer.includes("pdub.click")&&(window.goatcounter.no_onload=!1)</script><script data-goatcounter=https://philippdubach.goatcounter.com/count async src=https://gc.zgo.at/count.js></script></body></html>