---
title: "Meta's Edge Play"
date: 2024-09-25
external_url: "https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/"
---

Meta's latest Llama 3.2 release is less about raw capability and more about strategic positioning. While everyone else is racing to build bigger, more expensive cloud models, Meta is zigging: they're shipping models small enough to run on your phone.

The standout here isn't the 90B parameter multimodal model—that's table stakes at this point. It's the 1B and 3B parameter models that Meta claims can deliver meaningful AI capabilities while running locally on mobile devices. As they put it: "We're making it easier to build, experiment, and scale AI experiences everywhere — from the cloud to the edge to your phone."

This is a genuinely different bet than what OpenAI or Anthropic are making. While those companies optimize for maximum capability at maximum cost, Meta is optimizing for ubiquity. They're partnering with Qualcomm and MediaTek to get these models running efficiently on mobile processors, which suggests they're serious about this not being just a research exercise.

The economics make sense if you squint. Every API call to GPT-4 costs money; a model running locally on someone's phone costs Meta nothing after deployment. Scale that across billions of devices and the unit economics start to look very different from the current cloud-inference model.

Of course, there's the usual Meta sleight of hand with "open source"—the models are freely available for commercial use, but under Meta's custom license, not a true open source license. It's open enough to build an ecosystem, closed enough to maintain control.

The real test won't be the benchmarks Meta publishes today. It'll be whether a 1B parameter model running on a phone can actually deliver experiences that feel meaningfully intelligent to users, rather than just technically impressive to engineers.