<!doctype html><html lang=en-us><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Home - philippdubach.com</title><meta name=description content="Blog about Personal Projects, Articles and Papers on Economics, Finance and Technology"><meta name=keywords content="Finance,Economics,Technology,Data,Machine Learning"><meta name=author content="Philipp Dubach"><meta name=generator content="Hugo 0.149.0"><link rel=canonical href=http://localhost:1313/><link href=/index.xml rel=alternate type=application/rss+xml title=philippdubach><link href=/index.xml rel=feed type=application/rss+xml title=philippdubach><style>*{margin:0;padding:0;box-sizing:border-box}body{font-family:helvetica neue,Helvetica,Arial,segoe ui,sans-serif;padding-top:90px;line-height:1.6;color:#333;background-color:#f8f9fa}blockquote{margin:1em 0;padding-left:1em;border-left:2px solid #ccc;color:#666}blockquote p{margin:.5em 0}.container{display:flex;max-width:1200px;margin:0 auto;min-height:100vh}.sidebar{width:200px;background-color:#f8f9fa;padding:1.5rem;border-right:0 solid #e9ecef;position:sticky;top:0;height:100vh;overflow-y:auto}.site-title a{text-decoration:none;color:#333;font-size:1.5rem;font-weight:700}.site-description{color:#666;margin:.5rem 0 2rem;font-size:.9rem}.navigation ul{list-style:none;margin-bottom:2rem}.navigation li{margin-bottom:.5rem}.navigation a{text-decoration:none;color:#007acc;font-weight:500}.navigation a:hover{text-decoration:underline}.social-links a{display:inline-block;margin-right:1rem;color:#666;text-decoration:none;font-size:.9rem}.social-links a:hover{color:#007acc}.content{flex:1;padding:2rem;padding-bottom:3rem;max-width:800px}.posts{max-width:90%}.post{margin-bottom:1rem;padding-bottom:1rem}.post:last-child{border-bottom:none}.post-title{margin-bottom:.5rem;line-height:1.3}.post-title a{text-decoration:none;color:#333;font-size:1.25rem;font-weight:600;line-height:1.3}.post-title a:hover{color:#007acc}.post-meta{font-size:.85rem;color:#666;margin-bottom:0}.post-meta a{color:#007acc;text-decoration:none}.post-meta a:hover{text-decoration:underline}.post-content{line-height:1.7}.post-content p{margin-bottom:1rem}.post-content p:last-child{margin-bottom:.5rem}.post-content a{color:#007acc;text-decoration:none}.post-content a:hover{text-decoration:underline}.project-tag{background-color:#e9ecef;color:#495057;padding:2px 6px;border-radius:3px;font-size:9px;font-weight:500;text-transform:uppercase;letter-spacing:.3px;margin-left:6px;display:inline-block;vertical-align:middle;border:1px solid #dee2e6}.archive{max-width:90%}.year h2{margin:2rem 0 1rem;color:#333;font-size:1.5rem}.archive-item{display:flex;margin-bottom:.75rem;align-items:baseline}.archive-meta{width:60px;flex-shrink:0;font-size:.85rem;color:#666}.archive-title{flex:1}.archive-title a{text-decoration:none;color:#333}.archive-title a:hover{color:#007acc}@supports(text-wrap:balance){.archive-title{text-wrap:balance}}.single .post-title{font-size:1.5rem;margin-bottom:1rem;line-height:1.3}.pagination{margin-top:2rem;margin-bottom:1rem;text-align:center;padding-top:2rem;padding-bottom:1rem;border-top:1px solid #e9ecef}.pagination a{color:#007acc;text-decoration:none;font-weight:500}.pagination a:hover{text-decoration:underline}.img-lightbox{cursor:pointer;transition:opacity .2s}.img-lightbox:hover{opacity:.9}.lightbox-overlay{display:none;position:fixed;top:0;left:0;width:100%;height:100%;background:#f8f9fa;z-index:9999;cursor:pointer;align-items:center;justify-content:center;padding:2rem;box-sizing:border-box}.lightbox-overlay:target{display:flex}.lightbox-overlay img{max-width:95%;max-height:95%;object-fit:contain;background:#f8f9fa}.feedback-footer{margin-top:.75rem;padding-top:.75rem;border-top:1px solid #e9ecef;text-align:center;color:#666;font-size:.9rem;margin-bottom:2rem}.feedback-footer p{margin:0;line-height:1.6}.feedback-footer a{color:#007acc;text-decoration:none}.feedback-footer a:hover{text-decoration:underline}@media screen and (max-width:768px){html{font-size:8px !important}body{font-size:1rem !important}.sidebar{width:100px !important;padding:.75rem !important}.container{max-width:600px !important;margin:0 20px !important}.content{max-width:400px !important;padding:1rem !important;padding-bottom:.5rem !important}.pagination{margin-bottom:1.5rem !important;padding-bottom:1.5rem !important}.project-tag{padding:1px 3px !important;border-radius:1.5px !important;font-size:4.5px !important;letter-spacing:.15px !important;margin-left:3px !important}.archive-meta{width:30px !important}.post-title,.post-title a,.single .post-title{line-height:1.2 !important}.post-content p:last-child{margin-bottom:.25rem !important}.feedback-footer{margin-top:.5rem !important;padding-top:.5rem !important;margin-bottom:1rem !important}}</style><link rel=icon type=image/png href=/icons/favicon-96x96.png sizes=96x96><link rel=icon type=image/svg+xml href=/icons/favicon.svg><link rel="shortcut icon" href=/icons/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/icons/apple-touch-icon.png><meta name=apple-mobile-web-app-title content="philippdubach.com"><link rel=manifest href=/icons/site.webmanifest><meta name=theme-color content="#2c3e50"><meta name=msapplication-TileColor content="#2c3e50"><meta property="og:title" content="philippdubach"><meta property="og:description" content="Blog about Personal Projects, Articles and Papers on Economics, Finance and Technology"><meta property="og:type" content="website"><meta property="og:url" content="http://localhost:1313/"><meta property="og:site_name" content="philippdubach.com"><meta property="og:locale" content="en_US"><meta property="og:logo" content="http://localhost:1313/icons/favicon.ico"><meta property="og:image" content="https://static.philippdubach.com/ograph/ograph-post.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="philippdubach"><meta name=twitter:description content="Blog about Personal Projects, Articles and Papers on Economics, Finance and Technology"><meta name=twitter:image content="https://static.philippdubach.com/ograph/ograph-post.jpg"><script>(function(){let e=[],n=[],t=null;const s=["offsetHeight","offsetWidth","offsetTop","offsetLeft","scrollHeight","scrollWidth","scrollTop","scrollLeft","clientHeight","clientWidth","clientTop","clientLeft"];s.forEach(t=>{const s=Object.getOwnPropertyDescriptor(HTMLElement.prototype,t);s&&s.get&&Object.defineProperty(HTMLElement.prototype,t,{get:function(){return e.length>0&&(console.warn(`⚠️ FORCED REFLOW: Reading ${t} after DOM write!`),console.log("Recent writes:",e),console.log("Element:",this),console.trace()),n.push({prop:t,element:this.tagName}),s.get.call(this)}})});const o=Object.getOwnPropertyDescriptor(HTMLElement.prototype,"style");Object.defineProperty(HTMLElement.prototype,"style",{get:function(){const s=o.get.call(this);return new Proxy(s,{set:function(s,o,i){return e.push({property:o,value:i,element:s.parentElement?.tagName}),n=[],t||(t=requestAnimationFrame(()=>{e=[],t=null})),s[o]=i,!0}})}});const i=Element.prototype.getBoundingClientRect;Element.prototype.getBoundingClientRect=function(){return e.length>0&&(console.warn("⚠️ FORCED REFLOW: getBoundingClientRect() called after DOM write!"),console.trace()),i.call(this)}})()</script></head><body class=linkblog><div class=container><aside class=sidebar><div class=sidebar-content><div class=site-header><h1 class=site-title><a href=http://localhost:1313/>philippdubach</a></h1><p class=site-description>Blog about Personal Projects, Articles and Papers on Economics, Finance and Technology</p></div><nav class=navigation><ul><li><a href=/>Home</a></li><li><a href=/projects/>Projects</a></li><li><a href=/about/>About</a></li><li><a href=/posts/>Archive</a></li><li><a href=/index.xml>RSS</a></li></ul></nav><div class=social-links><a href=https://github.com/philippdubach target=_blank rel=noopener>GitHub</a>
<a href=mailto:info@philippdubach.com>Email</a></div></div></aside><main class=content><div class=posts><article class=post><header class=post-header><h2 class=post-title><a href=http://localhost:1313/2025/11/30/deploying-to-production-with-ai-agents-testing-cursor-on-azure/>Deploying to Production with AI Agents: Testing Cursor on Azure</a></h2><div class=post-meta><time datetime=2025-11-30T00:00:00Z>November 30, 2025</time></div></header><div class=post-content><p>I&rsquo;ve been curious about <a href=https://cursor.com/features>Cursor&rsquo;s capabilities</a> for a while, but never had a good reason to try it. This weekend I decided to host my own URL shortener and deployed <a href=https://yourls.org>YOURLS</a>, a free and open-source link shortener, on a fresh Azure VM. It seemed like a solid test case since it involves SSH access, server configuration, database setup, and SSL certificates. If an AI assistant could handle that end-to-end, it would be genuinely useful.</p><p>I was honestly surprised. Cursor didn&rsquo;t just write commands it connected via SSH, navigated the server, installed dependencies, configured Apache virtual hosts, set up MySQL, and handled the SSL certificate setup. It made sensible decisions about file permissions, security settings, and configuration details. When I asked for a custom YOURLS plugin to add date prefixes to short URLs, it built it on the first try. The whole build and deployment took about 15 minutes, which previously took me at least an hour of manual work and troubleshooting.</p><p>The URL shortener is now live and working. You can find this article at <a href=https://pdub.click/2511307>pdub.click/2511307</a>. I made the full scrubbed <a href=https://gist.github.com/philippdubach/d913591f906447041e2752729cd406e5>transcript available</a> if you want to see exactly how Cursor handled each step. If you want to do this installation yourself, I wrote a <a href=https://philippdubach.com/standalone/yourls-azure-tutorial/>step-by-step tutorial</a> covering the entire process, or you might as well let Cursor do it.</p><p>Right after finishing, I closed my laptop and went to clean my bathroom. This reminded me of <a href="https://x.com/AuthorJMac/status/1773679197631701238?lang=en">Joanna Maciejewska&rsquo;s quote</a>:</p><blockquote><p>I want AI to do my laundry and dishes so that I can do art and writing, not for AI to do my art and writing so that I can do laundry and dishes.</p></blockquote></div></article><article class=post><header class=post-header><h2 class=post-title><a href=http://localhost:1313/2025/11/28/michael-burrys-379-newsletter/>Michael Burry's $379 Newsletter</a></h2><div class=post-meta><time datetime=2025-11-28T00:00:00Z>November 28, 2025
</time>· <a href=https://michaeljburry.substack.com target=_blank rel=noopener>via</a></div></header><div class=post-content><p><a href=https://en.wikipedia.org/wiki/Michael_Burry>Michael Burry</a> (who in your head probably looks like <a href=https://www.historyvshollywood.com/reelfaces/big-short/>Christian Bale thanks to The Big Short</a>), the investor who famously predicted the 2008 housing crash, has launched a Substack newsletter after <a href=https://www.bloomberg.com/news/articles/2025-11-18/burry-says-he-s-active-in-markets-after-fund-is-deregistered>deregistering his hedge fund</a>. The $379 annual subscription capitalizes on the 1.6 million followers he&rsquo;s built on <a href=https://twitter.com/michaeljburry>X</a>, offering what he describes as his &ldquo;sole focus&rdquo; going forward.</p><p>The newsletter&rsquo;s <a href=https://michaeljburry.substack.com/p/foundations-my-1999-and-part-of-2000>inaugural post takes</a> (which he kindly enough made accessible for free as a Thanksgiving gift today) readers back to 1999, when Burry was a 27-year-old neurology resident at Stanford making $33'000 annually while carrying $150'000 in medical school debt. There he wrote his <a href=https://michaeljburry.substack.com/api/v1/file/a7e6acc6-aeac-460a-a26a-5fbe43e50d19.pdf>Valuestocks.net article &ldquo;Buffett Revisited&rdquo;</a>. A fellow resident casually mentioned making $1.5 million on Polycom stock. Physicians crowded around terminals checking stocks while patients waited. In that environment, Burry was writing investment analysis late at night, getting paid $1 per word by MSN Money under the pen name &ldquo;Value Doc.&rdquo; His VSN Fund returned 68.1% in 1999, and by February 2000, the <a href=https://michaeljburry.substack.com/api/v1/file/7e7cf8c7-2cd5-4bc1-8e36-0f7354ae04d6.pdf>San Francisco Chronicle</a> noted he had shorted Amazon. Fourteen days after that article appeared, the NASDAQ topped. It was a peak it wouldn&rsquo;t revisit for 15 years.</p><p>Burry&rsquo;s approach today is notably personal and reflective. His analysis of Apple in 1999 exemplifies his contrarian thinking. He bought it for the VSN Portfolio despite pushback, writing that great companies like Coca-Cola, American Express, and Disney had all experienced 11-year periods of negative real returns. Unlike the skeptics who simply dismissed the internet as a fad in 1999, Burry recognized the technology was transformational; he just believed the infrastructure was being overbuilt relative to near-term demand. He&rsquo;s making the same argument about AI today. He believes markets are deep in bubble territory, drawing parallels between the late 1990s tech mania and today&rsquo;s AI boom. His X post&rsquo;s often echoed familiar warnings:</p><blockquote><p>Feb 21, 2000: SF Chronicle says I&rsquo;m short Amazon. Greenspan 2005: &lsquo;bubble in home prices … does not appear likely.&rsquo; Powell &lsquo;25: &lsquo;AI companies actually… are profitable… it&rsquo;s a different thing.&rsquo;</p></blockquote><p>The comparison is deliberate. Burry highlighted then-Fed Chair Alan Greenspan&rsquo;s 2005 insistence that U.S. housing prices showed no signs of a bubble. This was just two years before the subprime implosion validated Burry&rsquo;s famous &ldquo;Big Short.&rdquo; Today, he&rsquo;s openly bearish on AI poster children Nvidia and Palantir, suggesting history is rhyming once again. Shortly after the newsletter was out, <a href="https://x.com/firstadopter/status/1993077524813980131?s=20">Nvidia circulated a seven-page memo</a> to Wall Street analysts explicitly naming Burry in its opening, a rare move for a company of Nvidia&rsquo;s stature. The memo sought to refute his claims about stock-based compensation, depreciation schedules, and what he calls &ldquo;circular financing.&rdquo;</p><p>Mainly, Nvidia disputed Burry&rsquo;s depreciation argument. Burry contends that customers overstate GPU useful lives to justify massive capex, claiming the hardware becomes obsolete in two to three years. Nvidia counters that its A100s, released in 2020, continue running at high utilization rates with &ldquo;meaningful economic value&rdquo; well beyond that timeframe, justifying the standard four-to-six-year depreciation schedule. The memo also rejected suggestions of &ldquo;<a href="https://www.youtube.com/watch?v=Q0TpWitfxPk">circular financing</a>,&rdquo; noting that Nvidia&rsquo;s strategic investments represent a small fraction of revenue and that AI startups raise capital predominantly from outside investors. Burry responded on Substack:</p><blockquote><p>I stand by my analysis. I am not claiming Nvidia is Enron. It is clearly Cisco.</p></blockquote><p>He argues Nvidia now occupies the exact position Cisco held in 1999-2000. It&rsquo;s the key hardware supplier powering a massive capital investment cycle built on optimistic demand forecasts. Just as telecom companies spent tens of billions laying fiber optic cable based on projections that &ldquo;internet traffic doubles every 100 days,&rdquo; today&rsquo;s hyperscalers are promising nearly $3 trillion in AI infrastructure spending over the next three years. The problem? In the early 2000s, less than 5% of U.S. fiber capacity was operational. Burry believes today&rsquo;s AI buildout rests on similarly flawed assumptions about data center power and GPU longevity. &ldquo;And once again there is a Cisco at the center of it all, with the picks and shovels for all and the expansive vision to go with it. Its name is Nvidia,&rdquo; Burry wrote. The analogy might resonate with market observers who remember how that story ended. Cisco&rsquo;s stock peaked above $80 in March 2000. It wouldn&rsquo;t return to that level for nearly 24 years. The company survived and remained profitable, but shareholders who bought at the top experienced a generational loss. One key difference is worth noting: Cisco&rsquo;s forward P/E in 2000 was around 200; Nvidia&rsquo;s is under 40.</p><p>In my opinion the technical argument around depreciation matters more than it might appear. If hyperscalers must depreciate GPUs over three years instead of six, companies like Alphabet would see roughly a 10% hit to net profit. More importantly, it would signal that the economic returns on AI infrastructure spending are weaker than advertised. Alphabet, for example, is currently guiding $90 billion-plus of AI spending this year. Using 5-year straight line depreciation, you get $18 billion per year in expenses. Add $9 billion for a conservative 10% WACC. That&rsquo;s $27 billion, and assuming a 70% blended margin, you need about $40 billion per year in incremental revenue directly attributable to AI to make the infrastructure spending justifiable. Therefore, the question remains: how much of Alphabet&rsquo;s $60 billion annualized revenue increase is actually attributable to AI versus normal growth? Burry closes his first newsletter with characteristic understatement:</p><blockquote><p>I doubted if I should ever come back. I&rsquo;m back.</p></blockquote></div></article><article class=post><header class=post-header><h2 class=post-title><a href=http://localhost:1313/2025/11/24/is-ai-really-eating-the-world-agi-networks-value-2/2/>Is AI Really Eating the World? AGI, Networks, Value [2/2]</a></h2><div class=post-meta><time datetime=2025-11-24T00:00:00Z>November 24, 2025
</time>· <a href=https://www.ben-evans.com/presentations/ target=_blank rel=noopener>via</a></div></header><div class=post-content><p><em>Start by reading <a href=/2025/11/23/is-ai-really-eating-the-world-what-weve-learned-1/2/>Is AI Really Eating the World? What we&rsquo;ve Learned [1/2]</a></em></p><p>All current <a href=https://en.wikipedia.org/wiki/Recommender_system>recommendation systems</a> work by capturing and analyzing user behavior at scale. Netflix needs millions of users watching millions of hours to train its recommendation algorithm. Amazon needs billions of purchases. The <a href=https://en.wikipedia.org/wiki/Network_effect>network effect</a> comes from data scale. What if LLMs can bypass this? What if an LLM can provide useful recommendations by reasoning about conceptual relationships rather than requiring massive behavioral datasets? If I ask for &ldquo;books like Pirsig&rsquo;s Zen and the Art of Motorcycle Maintenance but more focused on Eastern philosophy,&rdquo; a sufficiently capable LLM might answer well without needing to observe 100 million readers. It understands (or appears to understand) the conceptual space. I&rsquo;m uncertain whether LLMs can do this reliably by the end of 2025. The fundamental question is whether they reason or pattern-match at a very sophisticated level. <a href=https://arxiv.org/abs/2308.03762>Recent research suggests LLMs may rely more on statistical correlations than true reasoning</a>. If it&rsquo;s mostly pattern-matching, they still need the massive datasets and we&rsquo;re back to conventional network effects. If they can actually reason over conceptual spaces, that&rsquo;s different. That would unbundle data network effects from recommendation quality. Recommendation quality would depend on model capability, not data scale. And if model capability is commoditizing, then the value in recommendations flows to whoever owns customer relationships and distribution, not to whoever has the most data or the best model. I lean toward thinking LLMs are sophisticated pattern-matchers rather than reasoners, which means traditional network effects still apply. But this is one area where I&rsquo;m genuinely waiting to see more evidence.</p><p>Now, on AGI. The Silicon Valley consensus, articulated by <a href=https://sherwood.news/tech/gi-artificial-general-intelligence-when-predictions/>Sutskever, Altman, Musk, and others</a>, is that we&rsquo;re on a clear path to artificial general intelligence in the next few years, possibly by 2027 or 2028. The argument goes: <a href=https://arxiv.org/abs/2001.08361>scaling laws</a> continue to hold, we&rsquo;re seeing emergent capabilities at each scale jump, and there&rsquo;s no obvious wall before we reach human-level performance across all cognitive domains. I remain unconvinced. Not because I think AGI is impossible, but because the path from &ldquo;really good at pattern completion and probabilistic next-token prediction&rdquo; to &ldquo;general reasoning and planning capabilities&rdquo; seems less straightforward than the AI CEOs suggest. <a href=https://arxiv.org/abs/2305.00050>Current LLMs still fail in characteristic ways on tasks requiring actual causal reasoning</a>, spatial reasoning, or planning over extended horizons. They&rsquo;re getting better, but the improvement curve on these specific capabilities looks different from the improvement curve on language modeling perplexity. That suggests to me that we might need architectural innovations beyond just scaling, and those are harder to predict.</p><p>But let&rsquo;s say I&rsquo;m wrong. Let&rsquo;s say AGI arrives by 2028. Even then, I find it hard to model why this would be tremendously economically beneficial specifically to the companies that control the models. Here&rsquo;s why: we already have multiple competing frontier models (ChatGPT, Claude, Gemini, Microsoft&rsquo;s offerings, and now DeepSeek). If AGI arrives, it likely arrives for multiple players at roughly the same time, given how quickly capabilities diffuse in this space. Multiple competing AGIs means price competition. Price competition in a product with near-zero marginal cost means prices collapse toward marginal cost. Where does economic value flow in that scenario? It flows to the users of AI, not the providers. Engineering firms using AGI for materials development capture value through better materials. Pharmaceutical companies using AGI for drug discovery capture value through better drugs. Retailers using AGI for inventory management capture value through better margins. The AGI providers compete with each other to offer the capability at the lowest price. This is basic microeconomics. You capture value when you have market power, either through monopoly, through differentiation, or through control of a scarce input. If models are commodities or near-commodities, model providers have none of these.</p><p>The counterargument is that one provider achieves escape velocity and reaches AGI first with enough of a lead that they establish dominance before others catch up. This is the OpenAI/Microsoft theory of the case. Maybe. But the evidence so far suggests capability leads are measured in months, not years. <a href=https://openai.com/index/gpt-4-research/>GPT-4 launched in March 2023</a> with a substantial lead. Within six months, <a href=https://www.anthropic.com/news/claude-2>Claude 2 was comparable</a>. Within a year, multiple models clustered around similar capability. The diffusion is fast. Another counterargument is vertical integration. Maybe the hyperscalers that control cloud infrastructure plus model development plus customer relationships plus application distribution can capture value even if models themselves commoditize. This is more plausible, essentially the AWS playbook. Amazon didn&rsquo;t make money by having the best database. They made money by owning the infrastructure, the customer relationships, and the entire stack from hardware to application platform. Microsoft is clearly pursuing this strategy with <a href=https://www.microsoft.com/en-us/microsoft-365/blog/2023/03/16/introducing-microsoft-365-copilot-a-whole-new-way-to-work/>Azure plus OpenAI plus Copilot plus Office integration</a>. Google has Search plus Cloud plus Gemini plus Workspace. This could work, but it&rsquo;s a different thesis than &ldquo;we have the best model.&rdquo; It&rsquo;s &ldquo;we control the distribution and can bundle.&rdquo;</p><p>Evans shows a scatter plot (Slide 34) of model benchmark scores from <a href=https://arxiv.org/abs/2009.03300>standard evaluations like MMLU and HumanEval</a>. Leaders change weekly. The gaps are small. Meanwhile, consumer awareness doesn&rsquo;t track model quality. ChatGPT dominates with over <a href=https://openai.com/index/how-people-are-using-chatgpt/>700 million weekly active users</a> not because it has the best model anymore, but because it got there first and built brand. If models are commodities, value moves up the stack to product design, distribution, vertical integration, and customer relationships. This is exactly what happened with databases. Oracle didn&rsquo;t win because they had the best database engine. They won through enterprise sales, support contracts, and ecosystem lock-in. Microsoft didn&rsquo;t beat them with a better database. They won by bundling SQL Server with Windows Server and offering acceptable performance at a lower price. The SaaS pattern suggests something similar happens here. The model becomes an input. The applications built on top, the customer relationships, the distribution, those become the valuable assets. Why do I think this pattern applies rather than, say, the search pattern where Google maintained dominance despite no fundamental technical moat? Two reasons: (1) Search had massive data network effects. Every search improved the algorithm, and Google&rsquo;s scale meant they improved faster. LLMs have weaker data network effects because the pretraining data is largely static and publicly available, and fine-tuning data requirements are smaller. (2) Search had winner-take-all dynamics through defaults and single-answer demand. You pick one search engine and use it for everything. AI applications look more diverse. You might use different models for different tasks, or your applications might switch between models transparently based on price and performance. The switching costs are lower.</p><p>So where does this leave us? The technology exists and the underlying capabilities are real. But I think the current evidence points toward a world where value flows to applications and customer relationships, and where the $400 billion the hyperscalers are spending buys them competitive positioning rather than monopoly. The integrators are making money now by helping enterprises navigate uncertainty. Some of that will produce real productivity gains. Much of it is expensive signaling and competitive positioning. The startups unbundling existing software will see mixed results, the ones that succeed will do so by owning distribution or solving really specific problems where switching costs are high, not by having better access to AI. The biggest uncertainty is whether the hyperscalers can use vertical integration to capture value anyway, or whether the applications layer fragments and value flows to thousands of specialized companies. That depends less on AI capabilities and more on competitive dynamics, regulation, and whether enterprises prefer integrated platforms or best-of-breed solutions. My guess is we end up somewhere in between. The hyperscalers maintain strong positions through bundling and infrastructure control. A long tail of specialized applications captures value in specific verticals. The model providers themselves, unless they&rsquo;re also infrastructure providers, struggle to capture value proportional to the capability they&rsquo;re creating. But I&rsquo;m genuinely uncertain, and that uncertainty is where the interesting bets are.</p><p>What makes Evans&rsquo; presentation valuable is precisely what frustrated me about it initially: his refusal to collapse uncertainty prematurely. I&rsquo;ve spent this entire post arguing for a specific view of how value will flow in AI markets, but Evans is right that we&rsquo;re pattern-matching from incomplete data. Every previous platform shift looked obvious in retrospect and uncertain in real time. The PC revolution, the internet boom, mobile, they all had credible skeptics who turned out wrong and credible bulls who were right for the wrong reasons. Evans&rsquo; discipline in laying out the full range of possibilities, from commodity to monopoly to something entirely new, is the intellectually honest position. I&rsquo;ve made specific bets here because that&rsquo;s useful for readers trying to navigate the space, but I&rsquo;m more confident in my framework than in my conclusions.</p></div></article><article class=post><header class=post-header><h2 class=post-title><a href=http://localhost:1313/2025/11/23/is-ai-really-eating-the-world-what-weve-learned-1/2/>Is AI Really Eating the World? What we've Learned [1/2]</a></h2><div class=post-meta><time datetime=2025-11-23T00:00:00Z>November 23, 2025
</time>· <a href=https://www.ben-evans.com/presentations/ target=_blank rel=noopener>via</a></div></header><div class=post-content><p>In August 2011, Marc Andreessen wrote <a href=https://a16z.com/why-software-is-eating-the-world/>&ldquo;Why Software Is Eating the World&rdquo;</a>, an essay about how software was transforming industries, disrupting traditional businesses, and revolutionizing the global economy. Recently, <a href=https://www.ben-evans.com/benedictevans/2014/1/18/a16z>Benedict Evans</a>, a former a16z partner, gave a presentation on generative AI three years after ChatGPT&rsquo;s launch. His argument in short:</p><blockquote><p>we know this matters, but we don&rsquo;t know how.</p></blockquote><p>In this article I will try to explain why I find his framing fascinating but incomplete. Evans structures technology history in cycles. Every 10-15 years, the industry reorganizes around a new platform: <a href=https://en.wikipedia.org/wiki/Mainframe_computer>mainframes</a> (1960s-70s), PCs (1980s), web (1990s), smartphones (2000s-2010s). Each shift pulls all innovation, investment, and company creation into its orbit. Generative AI appears to be the next platform shift, or it could break the cycle entirely. The range of outcomes spans from &ldquo;just more software&rdquo; to a single unified intelligence that handles everything. The pattern recognition is smart, but I think the current evidence points more clearly toward commoditization than Evans suggests, with value flowing up the stack rather than to model providers.</p><p>The hyperscalers are spending historic amounts. In 2025, <a href=https://techblog.comsoc.org/2025/11/01/ai-spending-boom-accelerates-big-tech-to-invest-invest-an-aggregate-of-400-billion-in-2025-more-in-2026/>Microsoft, Google, Amazon, and Meta will invest roughly $400 billion</a> in AI infrastructure, more than global telecommunications capex. Microsoft now spends over 30% of revenue on capex, double what Verizon spends. What has this produced? Models that are simultaneously more capable and less defensible. When ChatGPT launched in November 2022, OpenAI had a massive quality advantage. Today, dozens of models cluster around similar performance. <a href=https://newsletter.semianalysis.com/p/deepseek-debates>DeepSeek proved that anyone with $500 million can build a frontier model</a>. Costs have collapsed. <a href=https://techcrunch.com/2025/08/08/openai-priced-gpt-5-so-low-it-may-spark-a-price-war/>OpenAI&rsquo;s API pricing has dropped by 97% since GPT-3&rsquo;s launch</a>, and every year brings an order of magnitude decline in the price of a given output.</p><p>Now, $500 million is still an enormous barrier. Only a few dozen entities globally can deploy that capital with acceptable risk. <a href=https://arxiv.org/abs/2303.08774>GPT-4&rsquo;s performance on complex reasoning tasks</a>, <a href=https://www.anthropic.com/news/claude-2-1>Claude&rsquo;s extended context windows of up to 200,000 tokens</a>, <a href=https://blog.google/technology/ai/google-gemini-ai/>Gemini&rsquo;s multimodal capabilities</a>, these represent genuine breakthroughs. But the economic moat isn&rsquo;t obvious to me (yet).</p><p>Evans uses an extended metaphor: automation that works disappears. In the 1950s, automatic elevators were AI. Today they&rsquo;re just elevators. As <a href=https://en.wikipedia.org/wiki/Larry_Tesler>Larry Tesler</a> noted in 1970,</p><blockquote><p>AI is whatever machines can&rsquo;t do yet. Once it works, it&rsquo;s just software.</p></blockquote><p>The question: will LLMs follow this pattern, or is this different?</p><p>Current deployment shows clear winners but also real constraints. Software development has seen massive adoption, with <a href=https://github.blog/news-insights/research/survey-ai-wave-grows/>GitHub reporting that 92% of developers now use AI coding tools</a>. Marketing has found immediate uses generating ad assets at scale. Customer support has attracted investment, though with the caveat that LLMs produce plausible answers, not necessarily correct ones. Beyond these areas, adoption looks scattered. <a href=https://www.deloitte.com/us/en/insights/industry/telecommunications/connectivity-mobile-trends-survey.html>Deloitte surveys from June 2025 show that roughly 20% of U.S. consumers use generative AI chatbots daily</a>, with another 34% using them weekly or monthly. Enterprise deployment is further behind. <a href=https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai>McKinsey data shows most AI &ldquo;agents&rdquo; remain in pilot or experimental stages</a>. A quarter of CIOs have launched something. Forty percent don&rsquo;t expect production deployment until 2026 or later.</p><p>But I think here&rsquo;s where Evans&rsquo; &ldquo;we don&rsquo;t know&rdquo; approach misses something important. Consulting firms are booking billions in AI contracts right now. <a href=https://www.crn.com/news/ai/2025/accenture-s-3b-ai-bet-is-paying-off-inside-a-massive-transformation-fueled-by-advanced-ai>Accenture alone expects $3 billion in GenAI bookings for fiscal 2025</a>. The revenue isn&rsquo;t coming from the models. It&rsquo;s coming from integration projects, change management, and process redesign. The pitch is simple: your competitors are moving on this, you can&rsquo;t afford to wait. If your competitors are investing and you&rsquo;re not, you risk being left behind. If everyone invests and AI delivers modest gains, you&rsquo;ve maintained relative position. If everyone invests and AI delivers nothing, you&rsquo;ve wasted money but haven&rsquo;t lost competitive ground. Evans notes that cloud adoption took 20 years to reach 30% of enterprise workloads and is still growing. New technology always takes longer than advocates expect. His most useful analogy is spreadsheets. <a href=https://en.wikipedia.org/wiki/VisiCalc>VisiCalc</a> in the late 1970s transformed accounting. If you were an accountant, you had to have it. If you were a lawyer, you thought &ldquo;that&rsquo;s nice for my accountant.&rdquo; ChatGPT today has the same dynamic. Certain people with certain jobs find it immediately essential. Everyone else sees a demo and doesn&rsquo;t know what to do with the blank prompt. This is right, and it suggests we&rsquo;re early. But it doesn&rsquo;t tell us where value will accumulate.</p><p>The standard pattern for deploying technology goes in stages: (1) Absorb it (make it a feature, automate obvious tasks). (2) Innovate (create new products, unbundle incumbents). (3) Disrupt (redefine what the market is). We&rsquo;re mostly in stage one. Stage two is happening in pockets. <a href=https://www.ycombinator.com/companies>Y Combinator&rsquo;s recent batches are overwhelmingly AI-focused</a>, betting on thousands of new companies unbundling existing software (startups are attacking specific enterprise problems like converting COBOL to Java or reconfiguring telco billing systems). Stage three remains speculative. From an economic perspective, there&rsquo;s the automation question: do you do the same work with fewer people, or more work with the same people? This echoes debates about <a href=https://en.wikipedia.org/wiki/Technological_change#Labor-augmenting_technological_change>labor-augmenting technical change</a> in economics. Companies whose competitive advantage was &ldquo;we can afford to hire enough people to do this&rdquo; face real pressure. Companies whose advantage was unique data, customer relationships, or distribution may get stronger. This is standard economic analysis of labor-augmenting technical change, and it probably holds here too.</p><p><em>Continue reading <a href=/2025/11/24/is-ai-really-eating-the-world-agi-networks-value-2/2/>Is AI Really Eating the World? AGI, Networks, and Value [2/2]</a></em></p></div></article><article class=post><header class=post-header><h2 class=post-title><a href=http://localhost:1313/2025/11/22/weather-forecasts-have-improved-a-lot/>Weather Forecasts Have Improved a Lot</a></h2><div class=post-meta><time datetime=2025-11-22T00:00:00Z>November 22, 2025
</time>· <a href=https://ourworldindata.org/weather-forecasts target=_blank rel=noopener>via</a></div></header><div class=post-content><p>Reading the press release for Google DeepMind&rsquo;s <a href=https://deepmind.google/discover/blog/weathernext-2-our-most-advanced-weather-forecasting-model/>WeatherNext 2</a>, I wondered: have weather forecasts actually improved over the past years?</p><p>Turns out they have, dramatically. <a href=https://ourworldindata.org/weather-forecasts>A four-day forecast today matches the accuracy of a one-day forecast from 30 years ago</a>. Hurricane track errors that once exceeded 400 nautical miles for 72-hour forecasts now sit below 80 miles. The <a href=https://charts.ecmwf.int>European Centre for Medium-Range Weather Forecasts reports three-day forecasts now reach 97% accuracy</a>, with seven-day forecasts approaching that threshold.</p><p>Google&rsquo;s new model accelerates this trend. <a href=https://arstechnica.com/science/2025/11/googles-new-weather-model-impressed-during-its-first-hurricane-season/>The hurricane model performed remarkably well this season when tested against actual paths</a>. WeatherNext 2 generates forecasts 8 times faster than its predecessor with resolution down to one hour. Each prediction takes under a minute on a single TPU compared to hours on a supercomputer using physics-based models. The speed comes from a smarter training approach. WeatherNext 2 (along with <a href=https://www.nature.com/articles/s41586-024-07744-y>neuralgcm</a>) uses a continuous ranked probability score (CRPS) objective rather than the L2 losses common in earlier neural weather models. The method adds random noise to parameters and trains the model to minimize L1 loss while maximizing differences between ensemble members with different noise initializations.</p><p>This matters because L2 losses blur predictions when models roll out autoregressively over multiple time steps. Spatial features degrade and the model truncates extremes. <a href="https://news.ycombinator.com/item?id=45957193">Models trained with L2 losses struggle to forecast high-impact extreme weather at moderate lead times</a>. The CRPS objective preserves the sharp spatial features and extreme values needed for cyclone tracking and heat wave prediction. These improvements stem from better satellite and ground station data, faster computers running higher-resolution models, and improved communication through apps and online services. AI systems like WeatherNext 2 and Pangu-Weather (which performs forecasts up to 10,000 times faster than traditional methods) are accelerating progress that has been building for decades.</p></div></article><article class=post><header class=post-header><h2 class=post-title><a href=http://localhost:1313/2025/11/21/glp-1-receptor-agonists-in-asud-treatment/>GLP-1 Receptor Agonists in ASUD Treatment</a></h2><div class=post-meta><time datetime=2025-11-21T00:00:00Z>November 21, 2025
</time>· <a href=https://academic.oup.com/jes/article/9/11/bvaf141/8277723 target=_blank rel=noopener>via</a></div></header><div class=post-content><blockquote><p>Alcohol and other substance use disorders (ASUDs) are complex, multifaceted, but treatable medical conditions with widespread medical, psychological, and societal consequences. However, treatment options remain limited, therefore the discovery and development of new treatments for ASUDs is critical. Glucagon-like peptide-1 receptor agonists (GLP-1RAs), currently approved for the treatment of type 2 diabetes mellitus, obesity, and obstructive sleep apnea, have recently emerged as potential new pharmacotherapies for ASUDs.</p></blockquote><p>This development matters most for people struggling with substance use disorders who have few effective treatment options. It also matters for manufacturers like Novo Nordisk facing <a href=https://philippdubach.com/2025/06/29/novo-nordisks-post-patent-strategy/>patent expiration pressures on Ozempic</a>. The research into GLP-1RAs for addiction treatment is early but notable given the limited pharmacotherapy options currently available for ASUDs. In February 2025, researchers at UNC published results from the first randomized controlled trial of semaglutide for ASUD treatment. The phase 2 trial enrolled 48 non-treatment-seeking adults with AUD and administered low-dose semaglutide <a href=https://jamanetwork.com/journals/jamapsychiatry/fullarticle/2829811>(0.25 mg/week for 4 weeks, 0.5 mg/week for 4 weeks - standard dosing for weight loss reaches 2.4 mg per week)</a> over 9 weeks. Participants on semaglutide consumed less alcohol in controlled laboratory settings and reported fewer drinks per drinking day in their normal lives. They also reported less craving for alcohol. Heavy drinking episodes declined more sharply in the semaglutide group compared to placebo over the nine-week trial. Despite the low doses, effect sizes for some drinking outcomes exceeded those typically seen with naltrexone, one of the few FDA-approved medications for alcohol use disorder. While larger trials are needed to confirm these results, the early evidence suggests GLP-1 may offer a meaningful treatment option for a condition where new therapies have been approved at a rate of roughly one every 25 years.</p></div></article><article class=post><header class=post-header><h2 class=post-title><a href=http://localhost:1313/2025/11/16/damodaran-on-golds-2025-surge/>Damodaran on Gold's 2025 Surge</a></h2><div class=post-meta><time datetime=2025-11-16T00:00:00Z>November 16, 2025
</time>· <a href=https://aswathdamodaran.blogspot.com/2025/11/a-golden-year-2025-golds-price-surge.html target=_blank rel=noopener>via</a></div></header><div class=post-content><p><a href=https://pages.stern.nyu.edu/~adamodar/>Aswath Damodaran&rsquo;s</a> latest analysis into gold&rsquo;s 2025 surge walks through gold&rsquo;s contradictory nature as a collectible rather than an asset with cash flows, showing why it&rsquo;s impossible to &ldquo;value&rdquo; gold in the traditional sense, yet entirely possible to understand what drives its pricing.</p><p>Even though gold is outperforming almost all other assets in my portfolio this year I fundamentally don&rsquo;t like holding it. I&rsquo;m a <a href=https://buffett.cnbc.com/2011-berkshire-hathaway-annual-meeting/>Buffett disciple</a>: gold is an unproductive asset that generates no earnings, pays no dividends.</p><p>But Damodaran&rsquo;s framework helps to understand why tolerating it anyway might be worth it. It&rsquo;s less an investment than insurance against the tail risks of hyperinflation and catastrophic market dislocations, scenarios where correlations go to one and traditional diversification fails. The dissonance between what I believe intellectually (productive assets compound wealth) and what I&rsquo;m actually doing (holding some gold anyway) probably says more about 2025&rsquo;s macro uncertainty than any principled investment thesis.</p><p><em>Damodaran&rsquo;s blog linked in this post’s title.</em></p></div></article><article class=post><header class=post-header><h2 class=post-title><a href=http://localhost:1313/2025/11/14/the-bicycle-needs-riding-to-be-understood/>The Bicycle Needs Riding to be Understood</a></h2><div class=post-meta><time datetime=2025-11-14T00:00:00Z>November 14, 2025
</time>· <a href=https://fly.io/blog/everyone-write-an-agent/ target=_blank rel=noopener>via</a></div></header><div class=post-content><blockquote><p>Some concepts are easy to grasp in the abstract. Boiling water: apply heat and wait. Others you really need to try. You only think you understand how a bicycle works, until you learn to ride one.</p></blockquote><p>You should write an LLM agent—not because they&rsquo;re revolutionary, but because the bicycle needs riding to be understood. Having built agents myself, Ptacek&rsquo;s central insight resonates: the behavior surprises in specific ways, particularly around how models scale effort with complexity before inexplicably retreating.</p><p>Ptacek walks through building a functioning agent in roughly 50 lines of Python, demonstrating how an LLM with ping access autonomously chose multiple Google endpoints without explicit instruction, a moment that crystallizes both promise and unpredictability. His broader point matches my experience: context engineering isn&rsquo;t mystical but straightforward programming—managing token budgets, orchestrating sub-agents, balancing explicit loops against emergent behavior. The open problems in agent design—titrating nondeterminism, connecting to ground truth, allocating tokens—remain remarkably accessible to individual experimentation, each iteration taking minutes rather than requiring institutional resources.</p></div></article><article class=post><header class=post-header><h2 class=post-title><a href=http://localhost:1313/2025/11/09/ai-models-as-standalone-pls/>AI Models as Standalone P&amp;Ls</a></h2><div class=post-meta><time datetime=2025-11-09T00:00:00Z>November 9, 2025
</time>· <a href=https://www.theregister.com/2025/10/29/microsoft_earnings_q1_26_openai_loss/ target=_blank rel=noopener>via</a></div></header><div class=post-content><blockquote><p>Microsoft reported earnings for the quarter ended Sept. [&mldr;] buried in its financial filings were a couple of passages suggesting that OpenAI suffered a net loss of $11.5 billion or more during the quarter.</p></blockquote><p>For every dollar of revenue, they&rsquo;re allegedly spending roughly $5 to deliver the product. What initially sounds like a joke about &ldquo;making it up on volume&rdquo; points to a more fundamental problem facing OpenAI and its competitors. AI companies are locked into continuously releasing more powerful (and expensive) models. If they stop, <a href=https://arxiv.org/abs/2311.16989>open-source alternatives will catch up</a> and offer equivalent capabilities at substantially lower costs. This creates an uncomfortable dynamic. If your current model requires spending more than you earn just to fund the next generation, the path to profitability becomes unclear—perhaps impossible.</p><p>Anthropic CEO Dario Amodei (everybody&rsquo;s favorite AI CEO) recently offered a different perspective in a <a href="https://youtu.be/GcqQ1ebBqkc?si=sEDGAVBuZsjtLpZS&amp;t=1016">conversation with Stripe co-founder John Collison</a>. He argues that treating each model as an independent business unit reveals a different picture than conventional accounting suggests.</p><blockquote><p>Let&rsquo;s say in 2023, you train a model that costs $100 million, and then you deploy it in 2024 and it makes $200 million of revenue.</p></blockquote><p>So far, this looks profitable, a solid 2x return on the training investment. But here&rsquo;s where it gets complicated.</p><blockquote><p>Meanwhile, because of the scaling laws, in 2024, you also train a model that costs $1 billion. If you look in a conventional way at the profit and loss of the company you&rsquo;ve lost $100 million the first year, you&rsquo;ve lost $800 million the second year, and you&rsquo;ve lost $8 billion in the third year, so it looks like it&rsquo;s getting worse and worse.</p></blockquote><p>The pattern continues:</p><blockquote><p>In 2025, you get $2 billion of revenue from that $1 billion model trained the previous year.</p></blockquote><p>Again, viewed in isolation, this model returned 2x its training cost.</p><blockquote><p>And you spend $10 billion to train the model for the following year.</p></blockquote><p>The losses appear to accelerate dramatically, from $100 million to $800 million to $8 billion.</p><p>This is where Amodei&rsquo;s reframing becomes interesting.</p><blockquote><p>If you consider each model to be a company, the model that was trained in 2023 was profitable. You paid $100 million and then it made $200 million of revenue."</p></blockquote><p>He also acknowledges there are inference costs (the actual computing expenses of running the model for users) but suggests these don&rsquo;t fundamentally change the picture in his simplified example. His core argument:</p><blockquote><p>If every model was a company, the model in this example is actually profitable. What&rsquo;s going on is that at the same time as you&rsquo;re reaping the benefits from one company, you&rsquo;re founding another company that&rsquo;s much more expensive and requires much more upfront R&amp;D investment.</p></blockquote><p>This is essentially an argument that AI companies are building a portfolio of profitable products, but the accounting makes it look terrible because each successive &ldquo;product&rdquo; costs 10x more than the last to develop. The losses stem from overlapping these profitable cycles while exponentially increasing investment scale. But this framework only works if two critical assumptions hold: (1) Each model consistently returns roughly 2x its training cost in revenue, and (2) The improvements from spending 10x more justify that investment—meaning customers will pay enough more for the better model to maintain that 2x return.</p><p>Amodei outlines two ways this resolves:</p><blockquote><p>So the way that it&rsquo;s going to shake out is this will keep going up until the numbers go very large and the models can&rsquo;t get larger, and, you know, then it&rsquo;ll be a large, very profitable business.</p></blockquote><p>In this first scenario, scaling hits physical or practical limits. You&rsquo;ve maxed out available compute, data, or capability improvements. Training costs plateau because you literally can&rsquo;t build a meaningfully larger model. At that point, companies stop needing exponentially larger investments and begin harvesting profits from their final-generation models. The second scenario is less optimistic:</p><blockquote><p>Or at some point the models will stop getting better, right? The march to AGI will be halted for some reason.</p></blockquote><p>If the improvements stop delivering proportional returns before reaching natural limits, companies face what Amodei calls overhang.</p><blockquote><p>And then perhaps there&rsquo;ll be some overhang, so there&rsquo;ll be a one-time, &lsquo;Oh man, we spent a lot of money and we didn&rsquo;t get anything for it,&rsquo; and then the business returns to whatever scale it was at.</p></blockquote><p>What Amodei&rsquo;s framework doesn&rsquo;t directly address is the open-source problem. If training Model C costs $10 billion but open-source alternatives <a href=https://synaptic.com/resources/open-source-ai-2024>reach comparable performance six months later</a>, that 2x return window might not materialize. The entire argument depends on maintaining a significant capability lead that customers will pay premium prices for. There&rsquo;s also the question of whether the 2x return assumption holds as models become more expensive. The jump from $100 million to $1 billion to $10 billion in training costs assumes that customers will consistently value the improvements enough to double revenue.</p></div></article><article class=post><header class=post-header><h2 class=post-title><a href=http://localhost:1313/2025/11/08/working-with-models/>Working with Models</a></h2><div class=post-meta><time datetime=2025-11-08T00:00:00Z>November 8, 2025
</time>· <a href=https://arxiv.org/abs/2510.21890 target=_blank rel=noopener>via</a></div></header><div class=post-content><p>There was this &ldquo;<a href=https://us1.discourse-cdn.com/flex001/uploads/ultralytics1/original/1X/45c604467b6f4212858281cf28f71a77083fb45e.jpeg>I work with Models</a>&rdquo; joke which I first heard years ago from an analyst working on a valuation model (<a href=/2025/10/19/everything-is-a-dcf-model/>see my previous post</a>). I guess it has become more relevant than ever:</p><blockquote><p>This monograph presents the core principles that have guided the development of diffusion models, tracing their origins and showing how diverse formulations arise from shared mathematical ideas. Diffusion modeling starts by defining a forward process that gradually corrupts data into noise, linking the data distribution to a simple prior through a continuum of intermediate distributions.</p></blockquote><p>If you want to get into this topic in the first place, be sure to check out <a href=https://deepgenerativemodels.github.io>Stefano Ermon&rsquo;s CS236 Deep Generative Models Course</a>. Lecture recordings of the full course can also be found on <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8">YouTube</a>.</p></div></article><nav class=pagination><a href=/page/2/ class=next>Older Posts →</a></nav></div></main></div></body></html>