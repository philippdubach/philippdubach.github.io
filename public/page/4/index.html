<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta http-equiv=Content-Security-Policy content="default-src 'self'; script-src 'self' 'unsafe-inline' https://gc.zgo.at https://cdn.jsdelivr.net https://pdub.click; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; font-src 'self' data:; connect-src 'self' https://philippdubach.goatcounter.com https://weekly-top-goatcounter-api.philippd.workers.dev https://gc.zgo.at; frame-ancestors 'self'; base-uri 'self';"><link rel=preconnect href=https://gc.zgo.at crossorigin><link rel=preconnect href=https://static.philippdubach.com crossorigin><link rel=dns-prefetch href=https://gc.zgo.at><link rel=dns-prefetch href=https://static.philippdubach.com><meta name=robots content="index, follow"><title>Home - philippdubach.com</title><meta name=description content="Blog about Personal Projects, Articles and Papers on Economics, Finance and Technology"><meta name=keywords content="Finance,Economics,Technology,Data,Machine Learning"><meta name=author content="Philipp Dubach"><meta name=generator content="Hugo 0.149.0"><link rel=canonical href=https://philippdubach.com/><link href=/index.xml rel=alternate type=application/rss+xml title=philippdubach><link href=/index.xml rel=feed type=application/rss+xml title=philippdubach><style>*{margin:0;padding:0;box-sizing:border-box}body{font-family:helvetica neue,Helvetica,Arial,segoe ui,sans-serif;padding-top:90px;line-height:1.6;color:#333;background-color:#ffff}blockquote{margin:1em 0;padding-left:1em;border-left:2px solid #ccc;color:#666}blockquote p{margin:.5em 0}.container{display:flex;max-width:1200px;margin:0 auto;min-height:100vh}.sidebar{width:200px;background-color:#ffff;padding:1.5rem;border-right:0 solid #e9ecef;position:sticky;top:0;height:100vh;overflow-y:auto}.site-title a{text-decoration:none;color:#333;font-size:1.5rem;font-weight:700}.site-description{color:#666;margin:.5rem 0 2rem;font-size:.9rem}.navigation ul{list-style:none;margin-bottom:2rem}.navigation li{margin-bottom:.5rem}.navigation a{text-decoration:none;color:#007acc;font-weight:500}.navigation a:hover{text-decoration:underline}.social-links a{display:inline-block;margin-right:1rem;color:#666;text-decoration:none;font-size:.9rem}.social-links a:hover{color:#007acc}.content{flex:1;padding:2rem;padding-bottom:3rem;max-width:800px}.posts{max-width:90%}.post{margin-bottom:1rem;padding-bottom:1rem}.post:last-child{border-bottom:none}.post-title{margin-bottom:.5rem;line-height:1.3}.post-title a{text-decoration:none;color:#333;font-size:1.25rem;font-weight:600;line-height:1.3}.post-title a:hover{color:#007acc}.post-meta{font-size:.85rem;color:#666;margin-bottom:0}.post-meta a{color:#007acc;text-decoration:none}.post-meta a:hover{text-decoration:underline}.post-content{line-height:1.7}.post-content p{margin-bottom:1rem}.post-content p:last-child{margin-bottom:.5rem}.post-content a{color:#007acc;text-decoration:none}.post-content a:hover{text-decoration:underline}.project-tag{background-color:#e9ecef;color:#495057;padding:2px 6px;border-radius:3px;font-size:9px;font-weight:500;text-transform:uppercase;letter-spacing:.3px;margin-left:6px;display:inline-block;vertical-align:middle;border:1px solid #dee2e6}.archive{max-width:90%}.year h2{margin:2rem 0 1rem;color:#333;font-size:1.5rem}.archive-item{display:flex;margin-bottom:.75rem;align-items:baseline}.archive-meta{width:60px;flex-shrink:0;font-size:.85rem;color:#666}.archive-title{flex:1}.archive-title a{text-decoration:none;color:#333}.archive-title a:hover{color:#007acc}@supports(text-wrap:balance){.archive-title{text-wrap:balance}}.single .post-title{font-size:1.5rem;margin-bottom:1rem;line-height:1.3}.pagination{margin-top:2rem;margin-bottom:1rem;text-align:center;padding-top:2rem;padding-bottom:1rem;border-top:1px solid #e9ecef}.pagination a{color:#007acc;text-decoration:none;font-weight:500}.pagination a:hover{text-decoration:underline}.img-lightbox{cursor:pointer;transition:opacity .2s}.img-lightbox:hover{opacity:.9}.lightbox-overlay{display:none;position:fixed;top:0;left:0;width:100%;height:100%;background:#f8f9fa;z-index:9999;cursor:pointer;align-items:center;justify-content:center;padding:2rem;box-sizing:border-box}.lightbox-overlay:target{display:flex}.lightbox-overlay img{max-width:95%;max-height:95%;object-fit:contain;background:#f8f9fa}.feedback-footer{margin-top:.75rem;padding-top:.75rem;border-top:1px solid #e9ecef;text-align:center;color:#666;font-size:.9rem;margin-bottom:2rem}.feedback-footer p{margin:0;line-height:1.6}.feedback-footer a{color:#007acc;text-decoration:none}.feedback-footer a:hover{text-decoration:underline}@media screen and (max-width:768px){html{font-size:8px !important}body{font-size:1rem !important}.sidebar{width:100px !important;padding:.75rem !important}.container{max-width:600px !important;margin:0 20px !important}.content{max-width:400px !important;padding:1rem !important;padding-bottom:.5rem !important}.pagination{margin-bottom:1.5rem !important;padding-bottom:1.5rem !important}.project-tag{padding:1px 3px !important;border-radius:1.5px !important;font-size:4.5px !important;letter-spacing:.15px !important;margin-left:3px !important}.archive-meta{width:30px !important}.post-title,.post-title a,.single .post-title{line-height:1.2 !important}.post-content p:last-child{margin-bottom:.25rem !important}.feedback-footer{margin-top:.5rem !important;padding-top:.5rem !important;margin-bottom:1rem !important}}</style><link rel=icon type=image/png href=/icons/favicon-96x96.png sizes=96x96><link rel=icon type=image/svg+xml href=/icons/favicon.svg><link rel="shortcut icon" href=/icons/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/icons/apple-touch-icon.png><meta name=apple-mobile-web-app-title content="philippdubach.com"><link rel=manifest href=/icons/site.webmanifest><meta name=theme-color content="#2c3e50"><meta name=msapplication-TileColor content="#2c3e50"><meta property="og:title" content="philippdubach"><meta property="og:description" content="Blog about Personal Projects, Articles and Papers on Economics, Finance and Technology"><meta property="og:type" content="website"><meta property="og:url" content="https://philippdubach.com/"><meta property="og:site_name" content="philippdubach.com"><meta property="og:locale" content="en_US"><meta property="og:logo" content="https://philippdubach.com/icons/favicon.ico"><meta property="og:image" content="https://static.philippdubach.com/ograph/ograph-post.jpg"><meta property="og:image:secure_url" content="https://static.philippdubach.com/ograph/ograph-post.jpg"><meta property="og:image:url" content="https://static.philippdubach.com/ograph/ograph-post.jpg"><meta property="og:image:type" content="image/jpeg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="philippdubach"><meta name=twitter:description content="Blog about Personal Projects, Articles and Papers on Economics, Finance and Technology"><meta name=twitter:image content="https://static.philippdubach.com/ograph/ograph-post.jpg"><meta name=twitter:image:src content="https://static.philippdubach.com/ograph/ograph-post.jpg"></head><body class=linkblog><div class=container><aside class=sidebar><div class=sidebar-content><div class=site-header><div class=site-title><a href=https://philippdubach.com/>philippdubach</a></div><p class=site-description>Blog about Personal Projects, Articles and Papers on Economics, Finance and Technology</p></div><nav class=navigation><ul><li><a href=/>Home</a></li><li><a href=/projects/>Projects</a></li><li><a href=/posts/>Archive</a></li><li><a href=/about/>About</a></li><li><a href=/index.xml>RSS</a></li></ul></nav><div class=social-links><a href=https://github.com/philippdubach target=_blank rel=noopener>GitHub</a>
<a href=mailto:info@philippdubach.com>Email</a></div></div></aside><main class=content><div class=posts><article class=post><header class=post-header><h2 class=post-title><a href=https://philippdubach.com/2025/07/06/counting-cards-with-computer-vision/>Counting Cards with Computer Vision
</a><span class=project-tag>PROJECT</span></h2><div class=post-meta><time datetime=2025-07-06T00:00:00Z>July 6, 2025</time></div></header><div class=post-content><p>After installing <a href=https://www.anthropic.com/claude-code>Claude Code</a></p><blockquote><p>the agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster through natural language commands</p></blockquote><p>I was looking for a task to test its abilities. Fairly quickly we wrote <a href=https://gist.github.com/philippdubach/741cbd56498e43375892966ca691b9c2>less than 200 lines of python code predicting black jack odds</a> using Monte Carlo Simulation. When I went on to test this little tool on <a href=https://games.washingtonpost.com/games/blackjack>Washington Post&rsquo;s</a> online Black Jack (I also didn&rsquo;t know that existed!) I quickly noticed how impractical it was to manually input all the card values on the table manually. What if the tool would also automatically recognize the cards that are on the table and calculate the odds from it? I have never done anything with computer vision so this seemed like a good challenge.
<a href=#lightbox-classification-gif-0 style="display:block;width:80%;margin:0 auto;padding:1rem 0;text-decoration:none"><picture class=img-lightbox><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/classification.gif 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/classification.gif 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/classification.gif 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/classification.gif 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/classification.gif 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/classification.gif 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/classification.gif 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/classification.gif 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/classification.gif" alt="alt text here" loading=lazy style=width:100%;height:auto;display:block>
</picture></a><a href=#_ id=lightbox-classification-gif-0 class=lightbox-overlay><img src="https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=90,format=webp/classification.gif" alt="alt text here">
</a>To get to any reasonable result we have to start with classification where we &ldquo;teach&rdquo; the model to categorize data by showing them lots of examples with correct labels. But where do the labels come from? I manually annotated <a href=https://universe.roboflow.com/cards-agurd/playing_card_classification>409 playing cards across 117 images</a> using Roboflow Annotate (at first I only did half as much - why this wasn&rsquo;t a good idea we&rsquo;ll see in a minute). Once enough screenshots of cards were annotated we can train the model to recognize the cards and predict card values on tables it has never seen before. I was able to use a <a href=https://www.nvidia.com/en-us/data-center/tesla-t4/>NVIDIA T4 GPU</a> inside Google Colab which offers some GPU time for free when capacity is available.
<a href=#lightbox-gpu_setup_colab-png-1 style="display:block;width:80%;margin:0 auto;padding:1rem 0;text-decoration:none"><picture class=img-lightbox><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/gpu_setup_colab.png 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/gpu_setup_colab.png 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/gpu_setup_colab.png 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/gpu_setup_colab.png 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/gpu_setup_colab.png 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/gpu_setup_colab.png 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/gpu_setup_colab.png 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/gpu_setup_colab.png 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/gpu_setup_colab.png" alt="alt text here" loading=lazy style=width:100%;height:auto;display:block>
</picture></a><a href=#_ id=lightbox-gpu_setup_colab-png-1 class=lightbox-overlay><img src="https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=90,format=webp/gpu_setup_colab.png" alt="alt text here">
</a>During training, the algorithm learns patterns from this example data, adjusting its internal parameters millions of times until it gets really good at recognizing the differences between categories (in this case different cards). Once trained, the model can then make predictions on new, unseen data by applying the patterns it learned. With the annotated dataset ready, it was time to implement the actual computer vision model. I chose to run inference on <a href=https://docs.ultralytics.com/de/models/yolo11/>Ultralytics&rsquo; YOLOv11</a> pre-trained model, a state-of-the-art object detection algorithm. I set up the environment in Google Colab following the <a href=https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolo11-object-detection-on-custom-dataset.ipynb>&ldquo;How to Train YOLO11 Object Detection on a Custom Dataset&rdquo;</a> notebook. After extracting the annotated dataset from Roboflow, I began training the model using the pre-trained YOLOv11s weights as a starting point. This approach, called <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, allows the model to leverage patterns already learned from millions of general images and adapt them to this specific task.
I initially set it up to <a href=https://docs.ultralytics.com/guides/model-training-tips/#other-techniques-to-consider-when-handling-a-large-dataset>run for 350 epochs</a>, though the model&rsquo;s built-in early stopping mechanism kicked in after 242 epochs when no improvement was observed for 100 consecutive epochs. The best results were achieved at epoch 142, taking around 13 minutes to complete on the Tesla T4 GPU.
The initial results were quite promising, with an overall mean Average Precision (mAP) of 80.5% at IoU threshold 0.5. Most individual card classes achieved good precision and recall scores, with only a few cards like the 6 and Queen showing slightly lower precision values.
<a href=#lightbox-run1_results-png-2 style="display:block;width:80%;margin:0 auto;padding:1rem 0;text-decoration:none"><picture class=img-lightbox><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/run1_results.png 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/run1_results.png 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/run1_results.png 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/run1_results.png 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/run1_results.png 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/run1_results.png 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/run1_results.png 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/run1_results.png 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/run1_results.png" alt="Training results showing confusion matrix and loss curves" loading=lazy style=width:100%;height:auto;display:block>
</picture></a><a href=#_ id=lightbox-run1_results-png-2 class=lightbox-overlay><img src="https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=90,format=webp/run1_results.png" alt="Training results showing confusion matrix and loss curves">
</a>However, looking at the confusion matrix and loss curves revealed some interesting patterns. While the model was learning effectively (as shown by the steadily decreasing loss), there were still some misclassifications between similar cards, particularly among the numbered cards. This highlighted exactly why I mentioned earlier that annotating only half the amount of data initially &ldquo;wasn&rsquo;t a good idea&rdquo; - more training examples would likely improve these edge cases and reduce confusion between similar-looking cards. My first attempt at solving the remaining accuracy issues was to add another layer to the workflow by sending the detected cards to Anthropic&rsquo;s Claude API for additional OCR processing.
<a href=#lightbox-claude_vision_workflow_results-png-3 style="display:block;width:80%;margin:0 auto;padding:1rem 0;text-decoration:none"><picture class=img-lightbox><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/claude_vision_workflow_results.png 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/claude_vision_workflow_results.png 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/claude_vision_workflow_results.png 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/claude_vision_workflow_results.png 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/claude_vision_workflow_results.png 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/claude_vision_workflow_results.png 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/claude_vision_workflow_results.png 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/claude_vision_workflow_results.png 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/claude_vision_workflow_results.png" alt="Roboflow workflow with Claude API integration" loading=lazy style=width:100%;height:auto;display:block>
</picture></a><a href=#_ id=lightbox-claude_vision_workflow_results-png-3 class=lightbox-overlay><img src="https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=90,format=webp/claude_vision_workflow_results.png" alt="Roboflow workflow with Claude API integration">
</a>This hybrid approach was very effective - the combination of YOLO&rsquo;s object detection to dynamically crop down the Black Jack table to individual cards with Claude&rsquo;s advanced vision capabilities yielded 99.9% accuracy on the predicted cards. However, this solution came with a significant drawback: the additional API layer consumed valuable time and the large model&rsquo;s processing overhead, making it impractical for real-time gameplay.
Seeking a faster solution, I implemented the same workflow <a href=https://github.com/JaidedAI/EasyOCR>locally using easyOCR</a> instead. EasyOCR seems to be really good at extracting black text on white background but <a href=https://stackoverflow.com/questions/68261703/how-to-improve-accuracy-prediction-for-easyocr>might struggle with everything else</a>. While it was able to correctly identify the card numbers when it detected them, it struggled to recognize around half of the cards in the first place - even when fed pre-cropped card images directly from the YOLO model. This inconsistency made it unreliable for the application.
Rather than continue band-aid solutions, I decided to go back and improve my dataset. I doubled the training data by adding another 60 screenshots with the same train/test split as before. More importantly, I went through all the previous annotations and fixed many of the bounding polygons. I noticed that several misidentifications were caused by the model detecting face-down dealer cards as valid cards, which happened because some annotations for face-up cards inadvertently included parts of the card backs next to them. The improved dataset and cleaned annotations delivered what I was hoping for: The confusion matrix now shows a much cleaner diagonal pattern, indicating that the model now correctly identifies most cards without the cross-contamination issues we saw earlier.
<a href=#lightbox-run_best-png-4 style="display:block;width:80%;margin:0 auto;padding:1rem 0;text-decoration:none"><picture class=img-lightbox><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/run_best.png 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/run_best.png 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/run_best.png 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/run_best.png 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/run_best.png 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/run_best.png 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/run_best.png 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/run_best.png 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/run_best.png" alt="Final training results with improved dataset" loading=lazy style=width:100%;height:auto;display:block>
</picture></a><a href=#_ id=lightbox-run_best-png-4 class=lightbox-overlay><img src="https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=90,format=webp/run_best.png" alt="Final training results with improved dataset">
</a>Both the training and validation losses converge smoothly without signs of overfitting, while the precision and recall metrics climb steadily to plateau near perfect scores. The mAP@50 reaches an impressive 99.5%. Most significantly, the confusion matrix now shows that the model has virtually eliminated false positives with background elements. The &ldquo;background&rdquo; column (rightmost) in the confusion matrix is now much cleaner, with only minimal misclassifications of actual cards as background noise.
<a href=#lightbox-local_run_interference_visual-png-5 style="display:block;width:80%;margin:0 auto;padding:1rem 0;text-decoration:none"><picture class=img-lightbox><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/local_run_interference_visual.png 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/local_run_interference_visual.png 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/local_run_interference_visual.png 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/local_run_interference_visual.png 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/local_run_interference_visual.png 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/local_run_interference_visual.png 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/local_run_interference_visual.png 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/local_run_interference_visual.png 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/local_run_interference_visual.png" alt="Real-time blackjack card detection and odds calculation" loading=lazy style=width:100%;height:auto;display:block>
</picture></a><a href=#_ id=lightbox-local_run_interference_visual-png-5 class=lightbox-overlay><img src="https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=90,format=webp/local_run_interference_visual.png" alt="Real-time blackjack card detection and odds calculation">
</a>With the model trained and performing, it was time to deploy it and play some blackjack. Initially, I tested the system using <a href=https://docs.roboflow.com/deploy/serverless-hosted-api-v2>Roboflow&rsquo;s hosted API</a>, which took around 4 seconds per inference - far too slow for practical gameplay. However, running the model locally on my laptop dramatically improved performance, achieving inference times of less than 0.1 seconds per image (1.3ms preprocess, 45.5ms inference, 0.4ms postprocess per image). I then <a href=https://python-mss.readthedocs.io/>integrated the model with MSS</a> to capture a real-time feed of my browser window. The system automatically overlays the detected cards with their predicted values and confidence scores
<a href=#lightbox-black_jack_odds_demo-gif-6 style="display:block;width:80%;margin:0 auto;padding:1rem 0;text-decoration:none"><picture class=img-lightbox><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/black_jack_odds_demo.gif 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/black_jack_odds_demo.gif 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/black_jack_odds_demo.gif 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/black_jack_odds_demo.gif 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/black_jack_odds_demo.gif 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/black_jack_odds_demo.gif 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/black_jack_odds_demo.gif 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/black_jack_odds_demo.gif 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/black_jack_odds_demo.gif" alt="Overview of selected fitted curves" loading=lazy style=width:100%;height:auto;display:block>
</picture></a><a href=#_ id=lightbox-black_jack_odds_demo-gif-6 class=lightbox-overlay><img src="https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=90,format=webp/black_jack_odds_demo.gif" alt="Overview of selected fitted curves">
</a>The final implementation successfully combines the pieces: the computer vision model detects and identifies cards in real-time, feeds this information to the Monte Carlo simulation, and displays both the card recognition results and the calculated odds directly on screen - do not try this at your local (online) casino!</p></div></article><article class=post><header class=post-header><h2 class=post-title><a href=https://philippdubach.com/2025/07/04/nvidia-likes-small-language-models/>NVIDIA Likes Small Language Models</a></h2><div class=post-meta><time datetime=2025-07-04T00:00:00Z>July 4, 2025
</time>· <a href=https://arxiv.org/abs/2506.02153 target=_blank rel=noopener>via</a></div></header><div class=post-content><blockquote><p>A Small Language Model (SLM) is a LM that can fit onto a common consumer electronic device and perform inference with latency sufficiently low to be practical when serving the agentic requests of one user. [&mldr;] We note that as of 2025, we would be comfortable with considering most models below 10bn parameters in size to be SLMs.</p></blockquote><p>The <a href=https://research.nvidia.com/labs/lpr/slm-agents/>(NVIDIA) researchers</a> argue that most agentic applications perform repetitive, specialized tasks that don&rsquo;t require the full generalist capabilities of LLMs. They propose heterogeneous agentic systems where SLMs handle most tasks while LLMs are used selectively for complex reasoning. They present three main arguments: (1) SLMs are sufficiently powerful for agentic tasks, as demonstrated by recent models like <a href=https://azure.microsoft.com/en-us/products/phi>Microsoft&rsquo;s Phi series</a>, <a href=https://research.nvidia.com/labs/adlr/nemotronh/>NVIDIA&rsquo;s Nemotron-H family</a>, and <a href=https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9>Hugging Face&rsquo;s SmolLM2 series</a>, which achieve comparable performance to much larger models <a href=https://arxiv.org/abs/2501.05465>while being 10-30x more efficient</a>. (2) SLMs are inherently more operationally suitable for agentic systems due to their faster inference, lower latency, and ability to run on edge devices. (3) SLMs are necessarily more economical, offering significant cost savings in inference, fine-tuning, and deployment.</p><p>The paper addresses counterarguments about LLMs&rsquo; superior language understanding and centralization benefits with studies (see Appendix B: <em>LLM-to-SLM Replacement Case Studies</em>) showing that 40-70% of LLM queries in popular open-source agents (MetaGPT, Open Operator, Cradle) could be replaced by specialized SLMs. One comment I read <a href="https://news.ycombinator.com/item?id=44432478">raised important concerns</a> about the paper&rsquo;s analysis, particularly regarding context window which are arguably the highest technical barrier to SLM adoption in agentic systems. Modern agentic applications require substantial context: Claude 4 Sonnet&rsquo;s system prompt alone <a href=https://simonwillison.net/2025/May/25/claude-4-system-prompt/>reportedly uses around 25k tokens</a>, and a typical coding agent needs system instructions, tool definitions, file context, and project documentation, totaling 5-10k tokens before any actual work begins. Most SLMs that can run on consumer hardware are capped at 32k or 128k contexts architecturally, but achieving reasonable inference speeds at these limits requires gaming hardware (8GB VRAM for a 7b model at 128k context).</p><p>The paper concludes that the shift to SLMs is inevitable due to economic and operational advantages, despite current barriers including infrastructure investment in LLM serving, generalist benchmark focus, and limited awareness of SLM capabilities. But the economic efficiency claims also face scrutiny under system-level analysis. In Section 3.2 they present simplistic FLOP comparisons while ignoring critical inefficiencies: the reliance on <a href=https://www.promptingguide.ai/techniques/fewshot>multishot-prompting</a> where SLMs might require 3-4 attempts for tasks that LLMs complete with 90% success rate, task decomposition overhead that multiplies context setup costs and error rates, and infrastructure efficiency differences between optimized datacenters (PUE ratios near 1.1, >90% GPU utilization) and consumer hardware (5-10% GPU utilization, residential HVAC, 80-85% power conversion efficiency). When accounting for failed attempts, orchestration overhead, and infrastructure efficiency, many &ldquo;economical&rdquo; SLM deployments might actually consume more total energy than centralized LLM inference.</p><p><em>(05/07/2025) Update: On the topic of speed I just came across <a href=https://arxiv.org/abs/2506.17298>Ultra-Fast Language Models Based on Diffusion</a>. You can also test it yourself using the <a href=https://chat.inceptionlabs.ai/>free playground link</a>, and it is in fact extremely fast. Try the &ldquo;Diffusion Effect&rdquo; in the top right corner which toggles an interesting visualization. I&rsquo;m not sure how realistic this is, it shows text appearing as random noise before gradually resolving into clear words; though the actual process likely involves tokens evolving from imprecise vectors in a multidimensional space toward more precise representations until they crystallize into specific words.</em></p><p><em>(06/07/2025) Update II: Apparently there is also a <a href=https://deepmind.google/models/gemini-diffusion/>Google DeepMind Gemini Diffusion Model</a>.</em></p></div></article><article class=post><header class=post-header><h2 class=post-title><a href=https://philippdubach.com/2025/06/29/novo-nordisks-post-patent-strategy/>Novo Nordisk's Post-Patent Strategy</a></h2><div class=post-meta><time datetime=2025-06-29T00:00:00Z>June 29, 2025
</time>· <a href=https://www.thelancet.com/journals/lancet/article/PIIS0140-6736%2825%2901185-7/ target=_blank rel=noopener>via</a></div></header><div class=post-content><p>Novo Nordisk, a long time member of my &ldquo;regrets&rdquo; stock list, has become <a href=https://finance.yahoo.com/quote/NVO/chart/>reasonably affordable lately (-48% yoy)</a>. Part of the reason being that they currently sit atop a ~$20 billion Ozempic/Wegovy franchise that faces <a href=https://journals.library.columbia.edu/index.php/stlr/blog/view/653>patent expiration in 2031</a>. That&rsquo;s roughly seven years to replace their blockbuster drug. We revisit them today, since per <a href=https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(25)01185-7/fulltext>newly published Lancet data</a>, Novo&rsquo;s lead replacement candidate—amycretin—just posted some genuinely impressive Phase 1 results. The injectable version delivered <a href=https://www.thelancet.com/cms/10.1016/S0140-6736(25)01185-7/asset/6f4ec048-c12e-4185-a860-a2dc988746c4/main.assets/gr3_lrg.jpg>24.3% average weight loss versus 1.1% for placebo</a>, beating both current market leaders (Wegovy at 15% and Lilly&rsquo;s Zepbound at 22.5%). Even the oral version hit 13.1% weight loss in just 12 weeks, with patients still losing weight when the trial ended.</p><p>Amycretin is very elegantly designed: It combines semaglutide (the active ingredient in Ozempic/Wegovy) with amylin, creating what&rsquo;s essentially a dual-pathway satiety signal. Semaglutide activates GLP-1 receptors to slow gastric emptying and reduce appetite centrally, while amylin works through complementary mechanisms to enhance fullness signals. This way both your stomach and your brain&rsquo;s &ldquo;appetite control center&rdquo; are getting the &ldquo;stop eating&rdquo; message simultaneously. One concern raised by <a href=https://www.statnews.com/staff/elaine-chen/>Elaine Chen at STAT</a> is that the <a href=https://www.statnews.com/2025/06/20/novo-nordisk-weight-loss-drug-amylin-hormone-injection-effective-but-side-effects-an-issue/>results of a Phase 1/2 study include unusual findings around dosage</a>. The full text article is behind a paywall unfortunately, so I did not have access. However, looking at the actual data from the study, I am assuming she is referring to Parts C, D, and E, which tested maintenance doses of 20 mg, 5 mg, and 1.25 mg respectively. The weight loss results were:</p><blockquote><p>Part C (20 mg): -22.0% weight loss at 36 weeks<br>Part D (5 mg): -16.2% weight loss at 28 weeks<br>Part E (1.25 mg): -9.7% weight loss at 20 weeks</p></blockquote><p>While there is a dose-response relationship, what&rsquo;s notable is the curves in <a href=https://www.thelancet.com/cms/10.1016/S0140-6736(25)01185-7/asset/6f4ec048-c12e-4185-a860-a2dc988746c4/main.assets/gr3_lrg.jpg>Figure 3</a> show relatively similar trajectories during the overlapping time periods. Typically in drug development, researchers would expect clear separation between dose groups (with higher doses producing proportionally greater effects). When weight-loss curves overlap significantly (which they do in this case), it suggests the doses may be producing similar effects despite different drug concentrations. If lower doses produce similar weight loss with potentially fewer side effects, this could favor using the lower, better-tolerated dose. Further, it might indicate that amycretin reaches maximum effect at relatively low doses. This should probably influence how future Phase 3 trials are designed, potentially focusing on the optimal dose rather than the maximum tolerated dose. Given that gastrointestinal side effects were dose-dependent but efficacy curves overlapped, this supports using the lowest effective dose. How that might be a bad thing I have yet to find out.</p><p>From a financial perspective, <a href=https://www.novonordisk.com/science-and-technology/r-d-pipeline.html>Novo Nordisk&rsquo;s pipeline</a> is very interesting: Amycretin&rsquo;s injectable version is currently in Phase 2, suggesting Phase 3 trials around 2026-2027, with potential approval by 2031; basically right as the Ozempic patents expire. But Novo isn&rsquo;t betting everything on amycretin. They&rsquo;re running what appears to be a diversified pipeline strategy with multiple shots on goal: <a href=https://www.novonordisk-trials.com/trials-conditions/all-trials-v2/NN9541-4919.html>NNC-0519</a> (another next-gen GLP-1), <a href=https://www.novonordisk-trials.com/trials-conditions/all-trials-v2/NN9662-7694.html>NNC-0662</a> (details kept confidential), and cagrilintide combinations. This makes sense: you want multiple candidates because the <a href=https://pmc.ncbi.nlm.nih.gov/articles/PMC9293739/>failure rate in drug development</a> makes even the most promising compounds statistically likely to fail. Eli Lilly&rsquo;s tirzepatide (Mounjaro/Zepbound) <a href=https://mounjaro.lilly.com/hcp/how-mounjaro-works>works through a different mechanism</a>—GLP-1 plus GIP receptor activation—and appears to be gaining market share. <a href=https://investor.lilly.com/news-releases/news-release-details/lillys-oral-glp-1-orforglipron-demonstrated-statistically>Lilly&rsquo;s orforglipron, an oral GLP-1 that hit 14.7% weight loss in Phase 2</a>, represents another competitive threat. Judging by <a href=https://finance.yahoo.com/quote/LLY/chart/>LLY&rsquo;s price development</a>, investors currently seem to think that Lilly is doing a better job at architecting a portfolio than Novo (or at least providing more disclosure about their pipeline). Yet, the overall competitive landscape might actually benefit both companies. The &ldquo;war&rdquo; between Novo and Lilly is expanding the overall market for obesity treatments, potentially growing the pie faster than either company is losing share. Also, to analyze the financial impact of the expiring Ozempic patents, we have to look further than just Novo&rsquo;s research pipeline. Manufacturing these GLP-1 compounds and their <a href=https://yds.ypsomed.com/files/media/03_Documents/12_Articles/%23171_2025_AprMay_Sustainability_Ypsomed.pdf>delivery devices</a> is &ldquo;pretty tough.&rdquo; Complex peptides requiring <a href=https://www.bachem.com/articles/commercial-apis/glucagon-like-peptide-1-glp-1/>specialized manufacturing capabilities</a>, plus the injection devices themselves are patent-protected. This creates what we would call <a href=https://www.morganstanley.com/im/publication/insights/articles/article_measuringthemoat.pdf>a capacity constraint moat</a> in corporate strategy. Novo&rsquo;s manufacturing capabilities/partnerships and injectable device patents are a key competitive advantage. Even when semaglutide goes generic in 2031, the entire generic pharmaceutical industry would essentially need to coordinate to build sufficient manufacturing capacity to meaningfully dent Novo&rsquo;s market share. Meanwhile, Novo could potentially defend by lowering prices while maintaining manufacturing advantages in a monopoly-to-oligopoly transition.</p><p>The other day I came across <a href=https://github.com/martinshkreli/models/blob/main/NOVOB.xlsx>Martin Shkreli&rsquo;s NOVO model</a>. Conservatively, it puts Novo&rsquo;s fair value around 705 DKK (21% upside from ~585 DKK), while a failure scenario drops valuation to 385 DKK. The range reflects what you&rsquo;d expect for a large-cap pharmaceutical company;the market has already incorporated most knowable information about pipeline risks and patent timelines. This also underscores the point that manufacturing capabilities and continuous innovation pipelines can potentially maintain quasi-monopolistic positions longer than traditional patent protection would suggest. Shkreli&rsquo;s analysis suggests Novo Nordisk is reasonably valued with modest upside potential, contingent on successful pipeline execution. Novo Nordisk is at a critical juncture, with substantial franchise value dependent on successful pipeline execution over the next 7-8 years. While the current valuation appears reasonable, the binary nature of drug development success creates both upside potential and significant downside risk.</p><p><em><h6>This article is for informational purposes only, you should not consider any information or other material on this site as investment, financial, or other advice. There are risks associated with investing.</h6></em></p></div></article><article class=post><header class=post-header><h2 class=post-title><a href=https://philippdubach.com/2025/06/26/ai-agents-build-my-investment-reports/>AI Agents Build my Investment Reports
</a><span class=project-tag>PROJECT</span></h2><div class=post-meta><time datetime=2025-06-26T00:00:00Z>June 26, 2025</time></div></header><div class=post-content><p>How I built a multi-agent system that automatically generates personalized morning market reports. But what even are AI Agents? A term that seems to be <a href="https://trends.google.com/trends/explore?date=today%205-y&amp;q=Agentic%20AI&amp;hl=en">everything everywhere all at once</a>right now. <a href=https://en.wikipedia.org/wiki/Agentic_AI>According to Wikipedia</a> Agentic AI is a class of artificial intelligence</p><blockquote><p>that focuses on autonomous systems that can make decisions and perform tasks without human intervention. The independent systems automatically respond to conditions, to produce process results.</p></blockquote><a href=https://static.philippdubach.com/html/20250625_morning_report.html><a href=#lightbox-investment_report_sample-png-0 style="display:block;width:80%;margin:0 auto;padding:1rem 0;text-decoration:none"><picture class=img-lightbox><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/investment_report_sample.png 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/investment_report_sample.png 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/investment_report_sample.png 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/investment_report_sample.png 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/investment_report_sample.png 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/investment_report_sample.png 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/investment_report_sample.png 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/investment_report_sample.png 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/investment_report_sample.png" alt="Sample Overview of a Morning Report" loading=lazy style=width:100%;height:auto;display:block>
</picture></a><a href=#_ id=lightbox-investment_report_sample-png-0 class=lightbox-overlay><img src="https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=90,format=webp/investment_report_sample.png" alt="Sample Overview of a Morning Report"></a></a><p>This sounds like exactly what I was looking for to automate my morning routine of checking the WSJ, FT, local news, overnight market developments and asses todays economic events&rsquo; impact on my portfolio. Instead of doing all of this I would prefer to get a personalized daily report of what happened while I was asleep and what to watch for during the next trading day. So that&rsquo;s what I attempted to build - with AI Agents of course! Let&rsquo;s start with the final architecture.
<a href=#lightbox-agent_architecture_v3-png-1 style="display:block;width:80%;margin:0 auto;padding:1rem 0;text-decoration:none"><picture class=img-lightbox><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/agent_architecture_v3.png 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/agent_architecture_v3.png 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/agent_architecture_v3.png 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/agent_architecture_v3.png 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/agent_architecture_v3.png 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/agent_architecture_v3.png 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/agent_architecture_v3.png 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/agent_architecture_v3.png 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/agent_architecture_v3.png" alt="High-level Agentic Architecture" loading=lazy style=width:100%;height:auto;display:block>
</picture></a><a href=#_ id=lightbox-agent_architecture_v3-png-1 class=lightbox-overlay><img src="https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=90,format=webp/agent_architecture_v3.png" alt="High-level Agentic Architecture">
</a>The interface is a Streamlit app (which I won&rsquo;t pretend I didn&rsquo;t build with Claude Code) where users can input or upload their current portfolio holdings. This then triggers multiple AI agents, represented by starburst symbols indicating Claude Sonnet 4 agents, that orchestrate various specialized components. The multi-agent approach orchestrates specialized AI agents across the pipeline: a ticker validation agent corrects portfolio inputs, news prioritization agents filter relevant headlines from multiple sources, and economic calendar agents identify market-moving events. Real-time market data flows through APIs into a portfolio performance engine that calculates returns across multiple time horizons. The central report generator synthesizes all analyses using structured templates user localization (geography, base currency etc.) and historical context (to avoid repetitive reports), before an HTML formatter creates the final professional output delivered via Streamlit (or as a daily email). If you want to see what the final report could look like, I have <a href=https://static.philippdubach.com/html/20250625_morning_report.html>uploaded a sample report</a> <em>(as you can see from the footer included in this HTML template I think this could offer very interesting applications for wealth managers to offer this to their clients as they could leverage existing APIs to get data from their portfolio engines as well as integrate in-house research. Further, something similar could be integrated into the Client Advisor&rsquo;s CRM allowing them to understand an individual client&rsquo;s situation at a glance)</em>.</p><p>So why use multiple agents? Rather than using a single monolithic prompt, I decomposed the problem into specialized components. Each agent handles a specific domain - like having dedicated equity analysts, macro economists, and portfolio managers. This enables parallel processing, specialized expertise per task, and easier debugging when something breaks.
<a href=#lightbox-demo_report-gif-2 style="display:block;width:80%;margin:0 auto;padding:1rem 0;text-decoration:none"><picture class=img-lightbox><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/demo_report.gif 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/demo_report.gif 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/demo_report.gif 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/demo_report.gif 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/demo_report.gif 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/demo_report.gif 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/demo_report.gif 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/demo_report.gif 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/demo_report.gif" alt="Streamlit App" loading=lazy style=width:100%;height:auto;display:block>
</picture></a><a href=#_ id=lightbox-demo_report-gif-2 class=lightbox-overlay><img src="https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=90,format=webp/demo_report.gif" alt="Streamlit App">
</a>Above you can see a short demo of the user interface - I built this because a couple of friends asked me if they could access the tool (I&rsquo;m thinking about deploying this publicly once i find a way to make the prompts more efficient and hence curb the model costs which are currently ~$0.15 per report) - In a professional context this would be obsolete as it would be connected directly to the portfolio reporting engine.</p><p>Let&rsquo;s have a look at the individual components: (1) Data Integration Pipeline: The system pulls from multiple sources simultaneously: Market data, RSS feeds from AP / Reuters / WSJ / FT etc. for news, and Trading Economics for economic calendar data. The newswire aggregation handles different feed formats, cleans HTML content, and deduplicates articles. The economic calendar scraping uses Selenium to handle dynamic content. (2) Ticker Validation Agent: Corrects portfolio inputs (stocks listed in Paris for example need a .PA ending after the ticker). (3) News Prioritization Agent: Identifies relevant headlines for specific portfolios, understanding that Fed announcements for example impact tech stocks differently than utilities. (4) Economic Calendar Agent: Prioritizes market-moving events, knowing that non-farm payroll releases matter more than housing starts. (5) Portfolio Performance Engine: Calculates comprehensive metrics across timeframes, compares against benchmarks, and adjusts for user&rsquo;s base currency and location. (6) Report Generator: The central orchestrator that synthesizes all analyses into coherent narratives, maintaining consistency through structured templates and memory of previous reports.</p><p>The system generates comprehensive reports in 2-3 minutes, processing hundreds of news articles and real-time data for entire portfolios. The parallel architecture significantly reduces latency compared to sequential processing. Centralized configuration separates model settings, API parameters, and localization preferences. The modular prompt template system allows independent agent refinement and easy addition of new specialized agents. The architecture scales from individual investors to institutional portfolio managers.
Multi-agent coordination introduces complexity requiring comprehensive error handling and fallback mechanisms. Data quality varies across sources, demanding constant monitoring (potentially by an additional agent). Future enhancements could include technical analysis agents, sentiment analysis from social media, email delivery automation, and mobile interfaces. The HTML formatting already ensures consistent rendering across platforms.</p><p>This multi-agent system demonstrates how AI can transform investment research and sales workflows. By decomposing complex analysis into specialized agents, it provides accurate and timely intelligence. The flexible architecture enables continuous improvement as well as modular adaptation for individual needs.</p></div></article><article class=post><header class=post-header><h2 class=post-title><a href=https://philippdubach.com/2025/06/22/behavioral-economics-transit-policy/>Behavioral Economics & Transit Policy</a></h2><div class=post-meta><time datetime=2025-06-22T00:00:00Z>June 22, 2025
</time>· <a href=https://people.duke.edu/~dandan/webfiles/PapersPI/Zero%20as%20a%20Special%20Price.pdf target=_blank rel=noopener>via</a></div></header><div class=post-content><p>Over the weekend a <a href=https://www.wsj.com/opinion/new-yorks-choice-cuomo-or-socialism-election-mayor-race-vote-mamdani-ede84c75>WSJ editorial on the 2025 New York City mayoral election</a> called one of the potential Democratic candidates Zohran Mamdani &ldquo;a literal socialist&rdquo; for - among other things - running on the promise of <a href=https://www.thenation.com/article/society/new-york-city-bus-free-fare/>free bus rides for all</a>:</p><blockquote><p>Zohran won New York&rsquo;s first fare-free bus pilot on five lines across the city. As Mayor, he&rsquo;ll permanently eliminate the fare on every city bus [&mldr;] Fast and free buses will not only make buses reliable and accessible but will improve safety for riders and operators – creating the world-class service New Yorkers deserve.</p></blockquote><p>Free public transit seems to be a <a href=https://en.wikipedia.org/wiki/Free_public_transport#List_of_towns_and_cities_with_area-wide_zero-fare_transport>recurring idea</a> among politicians: For some reason, making it free feels revolutionary in a way that making it cheaper never could. There&rsquo;s actually some solid behavioral economics behind this intuition: &ldquo;Zero as a Special Price: The True Value of Free Products.&rdquo; (Yes, before the <a href="https://www.youtube.com/watch?v=Q3tSG8h_O3A&amp;pp=ygUPZGFuIEFyaWVseSBmYWtl">fabricated data scandals</a>, Ariely did write research that has replicated consistently.) The basic finding: people don&rsquo;t treat &ldquo;free&rdquo; as just another very low price. When you price something at zero, it gets a special psychological boost that makes people value it way more than they should based on pure cost-benefit analysis: Give people a choice between a Hershey&rsquo;s Kiss for 1¢ and a Lindt truffle for 15¢. Most people choose the obviously superior Lindt. Now make it Hershey&rsquo;s for free versus Lindt for 14¢—keeping the price difference exactly the same—and suddenly everyone wants the Hershey&rsquo;s. Free doesn&rsquo;t just eliminate cost; it creates additional perceived value. The mechanism is pure affect. &ldquo;Free&rdquo; makes people feel good in a way that &ldquo;1¢&rdquo; doesn&rsquo;t, even though the economic difference is trivial. When you force people to think analytically about the trade-offs, the effect disappears. But in normal decision-making, that warm fuzzy feeling of getting something for nothing dominates rational calculation.</p><p>The difference between a $2.75 bus fare and $0 isn&rsquo;t meaningfully different from the difference between $2.75 and $0.75 for most riders&rsquo; budgets. But psychologically? Free transit feels like a gift from the city. Cheap transit feels like commerce. The first activates social norms (gratitude, civic participation, shared ownership). The second activates market norms (cost-benefit analysis, value-for-money calculations, consumer complaints when service is bad). On the other side, any positive price, no matter how small, forces people into analytical mode. People start thinking about trade-offs, evaluating whether the service is worth it, considering alternatives.
This is why congestion pricing works so well. A $5 charge to drive in Manhattan (<a href=https://en.wikipedia.org/wiki/Congestion_pricing>or Singapore, London, Stockholm, Milan, Gothenburg</a>) isn&rsquo;t going to bankrupt anyone who can afford to drive in Manhattan. But it makes people think about each trip in a way they never did when driving felt &ldquo;free&rdquo; (ignoring gas, parking, insurance, etc.). Once you&rsquo;re thinking analytically rather than just following habit, you&rsquo;re much more likely to take the subway.</p><p>But!! Free transit might actually make it easier to cut transit funding, not harder. Right now, when transit agencies face budget cuts, fare-paying riders get angry. They&rsquo;re customers! They paid for service! They demand value for money! This creates a natural constituency defending transit budgets. Make transit free, and you&rsquo;ve eliminated that market relationship. Riders become passive beneficiaries rather than paying customers. When service gets worse, they can&rsquo;t complain about not getting their money&rsquo;s worth; they&rsquo;re getting exactly what they paid for.
If I were a politician looking to slash subsidies without political blowback, step one would be eliminating fares. Tell everyone it&rsquo;s about equity and access. Then, once people stop thinking of themselves as customers, start the real cuts. No more late-night service; hey, it&rsquo;s free! Longer waits, dirtier stations, broken escalators; what did you expect for nothing? The behavioral economics is clear: when something is free, people have lower expectations and less standing to complain. The zero-price effect works both ways. None of this means transit shouldn&rsquo;t be affordable!</p></div></article><article class=post><header class=post-header><h2 class=post-title><a href=https://philippdubach.com/2025/06/15/it-just-aint-so/>It Just Ain’t So</a></h2><div class=post-meta><time datetime=2025-06-15T00:00:00Z>June 15, 2025
</time>· <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5283255" target=_blank rel=noopener>via</a></div></header><div class=post-content><blockquote><p>It ain&rsquo;t what you don&rsquo;t know that gets you into trouble. It&rsquo;s what you know for sure that just ain&rsquo;t so.</p></blockquote><p>This (not actually) Mark Twain quote from <a href=https://en.wikipedia.org/wiki/The_Big_Short_(film)>The Big Short</a> captures the sentiment of realizing that some foundational assumptions might be empirically wrong.</p><p>A recent article by <a href=https://antonvorobets.substack.com>Anton Vorobets</a> that I came across in <a href=https://www.bloomberg.com/authors/AQ0Te4IePFE/justina-lee>Justina Lee</a>&rsquo;s Quant Newsletter presents compelling evidence that challenges one of the field&rsquo;s fundamental statistical assumptions, that asset returns follow normal distributions. Using 26 years of data from 10 US equity indices, he ran formal normality tests (Shapiro-Wilk, D&rsquo;Agostino&rsquo;s K², Anderson-Darling) and found that the normal distribution hypothesis gets rejected in most cases. The supposed &ldquo;Aggregational Gaussianity&rdquo; that academics invoke through Central Limit Theorem arguments? It&rsquo;s mostly wishful thinking enabled by small sample sizes. As Vorobets observes:</p><blockquote><p>Finance and economics academia is unfortunately driven by several convenient myths, i.e., claims that are taken for granted and spread among university academics despite their poor empirical support.</p></blockquote><p>The article highlights significant practical consequences for portfolio management and risk assessment. Portfolio optimization based on normal distribution assumptions ignores fat left tails—exactly the kind of extreme downside events that can wipe out portfolios. This misspecification can lead to inadequate risk management and suboptimal asset allocation decisions. Vorobets suggests <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4034316">alternative approaches, including Monte Carlo simulations combined with Conditional Value-at-Risk (CVaR) optimization</a>, which better accommodate the complex distributional properties observed in financial data. While computationally more demanding, these methods offer improved alignment with empirical reality.</p><p>Reading this piece gave me a few ideas for extensions I might want to explore in an upcoming personal project:
(1) While Vorobets focuses on US equity indices, similar analysis across fixed income, commodities, currencies, and alternative assets would provide a more comprehensive view of distributional properties across financial markets. Each asset class exhibits distinct market microstructure characteristics that may influence distributional behavior.
(2) Global Market Coverage: Extending the geographic scope to include developed, emerging, and frontier markets would illuminate whether the documented deviations from normality represent universal phenomena or are specific to US market structures. Cross-regional analysis could reveal important insights about market development, regulatory frameworks, and institutional differences.
(3) Building upon Vorobets&rsquo; foundation, there are opportunities to incorporate multivariate normality testing, regime-dependent analysis, and time-varying parameter models. Additionally, investigating the power and robustness of different statistical tests across various market conditions would strengthen the methodological contribution.
(4) Examining different time horizons, market regimes (pre- and post-financial crisis, COVID period), and potentially higher-frequency data could provide deeper insights into when and why distributional assumptions break down.</p></div></article><article class=post><header class=post-header><h2 class=post-title><a href=https://philippdubach.com/2025/06/12/not-all-ai-skeptics-think-alike/>Not All AI Skeptics Think Alike</a></h2><div class=post-meta><time datetime=2025-06-12T00:00:00Z>June 12, 2025
</time>· <a href=https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf target=_blank rel=noopener>via</a></div></header><div class=post-content><p>Apple&rsquo;s recent paper &ldquo;The Illusion of Thinking&rdquo; has been widely understood to demonstrate that reasoning models don&rsquo;t &lsquo;actually&rsquo; reason. Using controllable puzzle environments instead of contaminated math benchmarks, they discovered something fascinating: there are three distinct performance regimes when it comes to AI reasoning complexity. For simple problems, standard models actually outperform reasoning models while being more token-efficient. At medium complexity, reasoning models show their advantage. But at high complexity? Both collapse completely.
Here&rsquo;s the kicker: reasoning models exhibit counterintuitive scaling behavior—their thinking effort increases with problem complexity up to a point, then declines despite having adequate token budget. It&rsquo;s like watching a student give up mid-exam when the questions get too hard, even though they have plenty of time left.</p><blockquote><p>We observe that reasoning models initially increase their thinking tokens proportionally with problem complexity. However, upon approaching a critical threshold—which closely corresponds to their accuracy collapse point—models counterintuitively begin to reduce their reasoning effort despite increasing problem difficulty.</p></blockquote><p>The researchers found something even more surprising: even when they provided explicit algorithms—essentially giving the models the answers—performance didn&rsquo;t improve. The collapse happened at roughly the same complexity threshold. On the other hand, <a href=https://www.seangoedecke.com/illusion-of-thinking/>Sean Goedecke</a> is not buying Apple&rsquo;s methodology: His core objection? Puzzles &ldquo;require computer-like algorithm-following more than they require the kind of reasoning you need to solve math problems.&rdquo;</p><blockquote><p>You can&rsquo;t compare eight-disk to ten-disk Tower of Hanoi, because you&rsquo;re comparing &ldquo;can the model work through the algorithm&rdquo; to &ldquo;can the model invent a solution that avoids having to work through the algorithm&rdquo;.</p></blockquote><p>From his own testing, models &ldquo;decide early on that hundreds of algorithmic steps are too many to even attempt, so they refuse to even start.&rdquo; That&rsquo;s strategic behavior, not reasoning failure. This matters because it shows how evaluation methodology shapes our understanding of AI capabilities. Goedecke argues Tower of Hanoi puzzles aren&rsquo;t useful for determining reasoning ability, and that the complexity threshold of reasoning models may not be fixed.</p></div></article><article class=post><header class=post-header><h2 class=post-title><a href=https://philippdubach.com/2025/06/10/visualizing-options-volatility/>Visualizing Options Volatility
</a><span class=project-tag>PROJECT</span></h2><div class=post-meta><time datetime=2025-06-10T00:00:00Z>June 10, 2025</time></div></header><div class=post-content></div></article><article class=post><header class=post-header><h2 class=post-title><a href=https://philippdubach.com/2025/05/31/your-ai-assistant-might-rat-you-out/>Your AI Assistant Might Rat You Out</a></h2><div class=post-meta><time datetime=2025-05-31T00:00:00Z>May 31, 2025
</time>· <a href=https://simonwillison.net/2025/May/31/snitchbench-with-llm/ target=_blank rel=noopener>via</a></div></header><div class=post-content><p>There was this story going around the past few days</p><blockquote><p>Anthropic researchers found if Claude Opus 4 thinks you&rsquo;re doing something immoral, it might &ldquo;contact the press, contact regulators, try to lock you out of the system&rdquo;</p></blockquote><p>Mostly driven by a <a href=https://x.com/sleepinyourhat/status/1925593359374328272>Sam Bowman tweet</a> referring to the <a href=https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf>Claude 4 System Card</a> section 4.1.9 on high-agency behavior. The outrage was mostly by people misunderstanding the prerequisites necessary for such a scenario. Nevertheless, an interesting question emerged: What happens when you feed an AI model evidence of fraud and give it an email tool? According to Simon Willison&rsquo;s latest experiment, &ldquo;they pretty much all will&rdquo; snitch on you to the authorities.</p><blockquote><p>A fun new benchmark just dropped! It&rsquo;s called SnitchBench and it&rsquo;s a great example of an eval, deeply entertaining and helps show that the &ldquo;Claude 4 snitches on you&rdquo; thing really isn&rsquo;t as unique a problem as people may have assumed. This is a repo I made to test how aggressively different AI models will &ldquo;snitch&rdquo; on you, as in hit up the FBI/FDA/media given bad behaviors and various tools.</p></blockquote><p>The benchmark creates surprisingly realistic scenarios—like detailed pharmaceutical fraud involving concealed adverse events and hidden patient deaths—then provides models with email capabilities to see if they&rsquo;ll take autonomous action. This reveals something fascinating about AI behavior that goes beyond traditional benchmarks. Rather than testing reasoning or knowledge, SnitchBench probes the boundaries between helpful assistance and autonomous moral decision-making. When models encounter what appears to be serious wrongdoing, do they become digital whistleblowers?</p><p>The implications are both reassuring and unsettling. On one hand, you want AI systems that won&rsquo;t assist with genuinely harmful activities. On the other, the idea of AI models making autonomous decisions about what constitutes reportable behavior feels like a significant step toward AI agency that we haven&rsquo;t fully grappled with yet. Therefore, Anthropic&rsquo;s own advice here seems like a good rule to follow:</p><blockquote><p>Whereas this kind of ethical intervention and whistleblowing is perhaps appropriate in principle, it has a risk of misfiring if users give Opus-based agents access to incomplete or misleading information and prompt them in these ways. We recommend that users exercise caution with instructions like these that invite high-agency behavior in contexts that could appear ethically questionable.</p></blockquote></div></article><article class=post><header class=post-header><h2 class=post-title><a href=https://philippdubach.com/2025/05/30/modeling-glycemic-response-with-xgboost/>Modeling Glycemic Response with XGBoost
</a><span class=project-tag>PROJECT</span></h2><div class=post-meta><time datetime=2025-05-30T00:00:00Z>May 30, 2025</time></div></header><div class=post-content><p>Earlier this year I wrote how <a href=/2025/01/02/i-built-a-cgm-data-reader/>I built a CGM data reader</a> after wearing a continuous glucose monitor myself. Since I was already logging my macronutrients and learning more about molecular biology in an <a href=https://ocw.mit.edu/courses/res-7-008-7-28x-molecular-biology/>MIT MOOC</a> I became curious if given a meal&rsquo;s macronutrients (carbs, protein, fat) and some basic individual characteristics (age, BMI), these could serve as features in a regressor machine learning model to predict the curve parameters of the postprandial glucose curve (how my blood sugar levels change after eating). I came across a paper on <a href="https://www.cell.com/cell/fulltext/S0092-8674(15)01481-6?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0092867415014816%3Fshowall%3Dtrue">Personalized Nutrition by Prediction of Glycemic Responses</a> which did exactly that. Unfortunately, neither the data nor the code were publicly available. And - I wanted to predict my <em>own</em> glycemic response curve. So I decided to build my own model. In the process I wrote this <a href=https://static.philippdubach.com/pdf/Modeling_Postprandial_Glycemic_Response_in_Non_Diabetic_Adults_Using_XGBRegressor.pdf>working paper</a>.
<a href=https://static.philippdubach.com/pdf/Modeling_Postprandial_Glycemic_Response_in_Non_Diabetic_Adults_Using_XGBRegressor.pdf><a href=#lightbox-working_paper_overview-jpg-0 style="display:block;width:80%;margin:0 auto;padding:1rem 0;text-decoration:none"><picture class=img-lightbox><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/working_paper_overview.jpg 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/working_paper_overview.jpg 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/working_paper_overview.jpg 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/working_paper_overview.jpg 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/working_paper_overview.jpg 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/working_paper_overview.jpg 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/working_paper_overview.jpg 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/working_paper_overview.jpg 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/working_paper_overview.jpg" alt="Overview of Working Paper Pages" loading=lazy style=width:100%;height:auto;display:block>
</picture></a><a href=#_ id=lightbox-working_paper_overview-jpg-0 class=lightbox-overlay><img src="https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=90,format=webp/working_paper_overview.jpg" alt="Overview of Working Paper Pages">
</a></a>The paper represents an exercise in applying machine learning techniques to medical applications. The methodologies employed were largely inspired by <a href="https://www.cell.com/cell/fulltext/S0092-8674(15)01481-6?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0092867415014816%3Fshowall%3Dtrue">Zeevi et al.</a>&rsquo;s approach. I quickly realized that training a model on my own data <em>only</em> was not very promising if not impossible. To tackle this, I used the publicly available <a href="https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2005143">Hall dataset</a> containing continuous glucose monitoring data from 57 adults, which I narrowed down to 112 standardized meals from 19 non-diabetic subjects with their respective glucose curve after the meal (full methodology in the paper).
<a href=#lightbox-cgm-workflow-graph-jpg-1 style="display:block;width:80%;margin:0 auto;padding:1rem 0;text-decoration:none"><picture class=img-lightbox><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/cgm-workflow-graph.jpg 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/cgm-workflow-graph.jpg 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/cgm-workflow-graph.jpg 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/cgm-workflow-graph.jpg 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/cgm-workflow-graph.jpg 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/cgm-workflow-graph.jpg 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/cgm-workflow-graph.jpg 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/cgm-workflow-graph.jpg 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/cgm-workflow-graph.jpg" alt="Overview of the CGM pipeline workflow" loading=lazy style=width:100%;height:auto;display:block>
</picture></a><a href=#_ id=lightbox-cgm-workflow-graph-jpg-1 class=lightbox-overlay><img src="https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=90,format=webp/cgm-workflow-graph.jpg" alt="Overview of the CGM pipeline workflow">
</a>Rather than trying to predict the entire glucose curve, I simplified the problem by fitting each postprandial response to a normalized Gaussian function. This gave me three key parameters to predict: amplitude (how high glucose rises), time-to-peak (when it peaks), and curve width (how long the response lasts).
<a href=#lightbox-cgm-fitted-curve-large1-jpg-2 style="display:block;width:80%;margin:0 auto;padding:1rem 0;text-decoration:none"><picture class=img-lightbox><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/cgm-fitted-curve-large1.jpg 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/cgm-fitted-curve-large1.jpg 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/cgm-fitted-curve-large1.jpg 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/cgm-fitted-curve-large1.jpg 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/cgm-fitted-curve-large1.jpg 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/cgm-fitted-curve-large1.jpg 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/cgm-fitted-curve-large1.jpg 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/cgm-fitted-curve-large1.jpg 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/cgm-fitted-curve-large1.jpg" alt="Overview of single fitted curve of cgm measurements" loading=lazy style=width:100%;height:auto;display:block>
</picture></a><a href=#_ id=lightbox-cgm-fitted-curve-large1-jpg-2 class=lightbox-overlay><img src="https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=90,format=webp/cgm-fitted-curve-large1.jpg" alt="Overview of single fitted curve of cgm measurements">
</a>The Gaussian approximation worked surprisingly well for characterizing most glucose responses. While some curves fit better than others, the majority of postprandial responses were well-captured, though there&rsquo;s clear variation between individuals and meals. Some responses were high amplitude, narrow width, while others are more gradual and prolonged.
<a href=#lightbox-example-fitted-cgm-measurements-jpg-3 style="display:block;width:80%;margin:0 auto;padding:1rem 0;text-decoration:none"><picture class=img-lightbox><source media="(max-width: 768px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=320,quality=80,format=webp/example-fitted-cgm-measurements.jpg 320w,
https://static.philippdubach.com/cdn-cgi/image/width=480,quality=80,format=webp/example-fitted-cgm-measurements.jpg 480w,
https://static.philippdubach.com/cdn-cgi/image/width=640,quality=80,format=webp/example-fitted-cgm-measurements.jpg 640w" sizes=80vw><source media="(max-width: 1024px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=768,quality=80,format=webp/example-fitted-cgm-measurements.jpg 768w,
https://static.philippdubach.com/cdn-cgi/image/width=1024,quality=80,format=webp/example-fitted-cgm-measurements.jpg 1024w" sizes=80vw><source media="(min-width: 1025px)" srcset="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80,format=webp/example-fitted-cgm-measurements.jpg 1200w,
https://static.philippdubach.com/cdn-cgi/image/width=1600,quality=80,format=webp/example-fitted-cgm-measurements.jpg 1600w,
https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=80,format=webp/example-fitted-cgm-measurements.jpg 2000w" sizes=80vw><img src="https://static.philippdubach.com/cdn-cgi/image/width=1200,quality=80/example-fitted-cgm-measurements.jpg" alt="Overview of selected fitted curves" loading=lazy style=width:100%;height:auto;display:block>
</picture></a><a href=#_ id=lightbox-example-fitted-cgm-measurements-jpg-3 class=lightbox-overlay><img src="https://static.philippdubach.com/cdn-cgi/image/width=2000,quality=90,format=webp/example-fitted-cgm-measurements.jpg" alt="Overview of selected fitted curves">
</a>I then trained an XGBoost regressor with 27 engineered features including meal composition, participant characteristics, and interaction terms. XGBoost was chosen for its ability to handle mixed data types, built-in feature importance, and strong performance on tabular data. The pipeline included hyperparameter tuning with 5-fold cross-validation to optimize learning rate, tree depth, and regularization parameters. Rather than relying solely on basic meal macronutrients, I engineered features across multiple categories and implemented CGM statistical features calculated over different time windows (24-hour and 4-hour periods), including time-in-range and glucose variability metrics. Architecture wise, I trained three separate XGBoost regressors - one for each Gaussian parameter.</p><p>While the model achieved moderate success predicting amplitude (R² = 0.46), it completely failed at predicting timing - time-to-peak prediction was essentially random (R² = -0.76), and curve width prediction was barely better (R² = 0.10). Even the amplitude prediction, while statistically significant, falls well short of an R² > 0.7. Studies that have achieved better predictive performance typically used much larger datasets (>1000 participants). For my original goal of predicting my own glycemic responses, this suggests that either individual-specific models trained on extensive personal data, or much more sophisticated approaches incorporating larger training datasets, would be necessary.</p><p>The complete code, Jupyter notebooks, processed datasets, and supplementary results are available in my <a href=https://github.com/philippdubach/glucose-response-analysis>GitHub repository</a>.<br>_ _</p><p><em>(10/06/2025) Update: Today I came across Marcel Salathé&rsquo;s <a href="https://www.linkedin.com/posts/salathe_myfoodrepo-digitalhealth-precisionnutrition-activity-7337806988082393088-2Lsu?utm_source=share&amp;utm_medium=member_ios&amp;rcm=ACoAADeInT4BJMhtg5DSjxX1jVtIAs5w_KxZm-g">LinkedIn post</a> on a publication out of EPFL: <a href=https://www.frontiersin.org/journals/nutrition/articles/10.3389/fnut.2025.1539118/full>Personalized glucose prediction using in situ data only</a>.</em></p><blockquote><p><em>With data from over 1,000 participants of the Food & You digital cohort, we show that a machine learning model using only food data from myFoodRepo and a glucose monitor can closely track real blood sugar responses to any meal (correlation of 0.71).</em></p></blockquote><p><em>As expected Singh et. al. achieve a substantially better predictive performance (R = 0.71 vs R² = 0.46). Besides probably higher methodological rigor and scientific quality, the most critical difference is sample size - their 1'000+ participants versus my 19 participants (from the <a href="https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2005143">Hall dataset</a>) represents a fundamental difference in statistical power and generalizability. They addressed one of the shortcomings I faced by leveraging a large digital nutritional cohort from the <a href=https://pubmed.ncbi.nlm.nih.gov/38033170/>&ldquo;Food & You&rdquo; study</a> (including high-resolution data of nutritional intake of more than 46 million kcal collected from 315'126 dishes over 23'335 participant days, 1'470'030 blood glucose measurements, 49'110 survey responses, and 1'024 samples for gut microbiota analysis).</em></p><p><em>Apart from that I am excited to - at a first glance - observe the following similarities:
(1) Both aim to predict postprandial glycemic responses using machine learning, with a focus on personalized nutrition applications.
(2) Both employ XGBoost regression as their primary predictive algorithm and use similar performance metrics (R², RMSE, MAE, Pearson correlation).
(3) Both extract comprehensive feature sets including meal composition (macronutrients), temporal features, and individual characteristics.
(4) Both use mathematical approaches to characterize glucose responses - I used Gaussian curve fitting, while Singh et. al. use incremental area under the curve (iAUC).
(5) Both employ cross-validation techniques for model evaluation and hyperparameter tuning.
(6) SHAP Analysis: Both use SHAP for model interpretability and feature importance analysis.</em><a id=update></p></div></article><nav class=pagination><a href=/page/3/ class=prev>← Newer Posts</a>
<a href=/page/5/ class=next>Older Posts →</a></nav></div></main></div><script>window.goatcounter=window.goatcounter||{},document.referrer&&document.referrer.includes("pdub.click")&&(window.goatcounter.no_onload=!1)</script><script data-goatcounter=https://philippdubach.goatcounter.com/count async src=https://gc.zgo.at/count.js></script></body></html>