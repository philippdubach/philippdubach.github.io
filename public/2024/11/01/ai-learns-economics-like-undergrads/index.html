<!doctype html><html lang=en-us><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=.5"><title>AI Learns Economics Like Undergrads - philippdubach</title><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.147.8"><title>AI Learns Economics Like Undergrads - philippdubach</title><meta name=description content="Personal Projects, Curated Articles and Papers on Economics, Finance and Technology"><meta name=keywords content="Finance,Economics,Technology,Data,Machine Learning"><meta property="og:url" content="http://localhost:1313/2024/11/01/ai-learns-economics-like-undergrads/"><meta property="og:site_name" content="philippdubach"><meta property="og:title" content="AI Learns Economics Like Undergrads"><meta property="og:description" content="This cuts to the heart of how LLMs actually work: Testing Large Language Models on economics problems reveals that these supposedly sophisticated systems don’t just learn correct reasoning—they absorb our misconceptions too. The study found LLMs performing reasonably well on undergraduate economics questions (around 65% accuracy) but falling flat on graduate-level problems (35% accuracy). More tellingly, the specific errors weren’t random failures but systematic mistakes that mirror exactly what human students get wrong."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-11-01T00:00:00+00:00"><meta property="article:modified_time" content="2024-11-01T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="AI Learns Economics Like Undergrads"><meta name=twitter:description content="This cuts to the heart of how LLMs actually work: Testing Large Language Models on economics problems reveals that these supposedly sophisticated systems don’t just learn correct reasoning—they absorb our misconceptions too. The study found LLMs performing reasonably well on undergraduate economics questions (around 65% accuracy) but falling flat on graduate-level problems (35% accuracy). More tellingly, the specific errors weren’t random failures but systematic mistakes that mirror exactly what human students get wrong."><link rel=canonical href=http://localhost:1313/2024/11/01/ai-learns-economics-like-undergrads/><link rel=stylesheet href=/css/custom.css><link rel=apple-touch-icon sizes=180x180 href=/icons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/icons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/icons/favicon-16x16.png><link rel=mask-icon href=/icons/safari-pinned-tab.svg><link rel="shortcut icon" href=/icons/favicon.ico><link rel=icon type=image/svg+xml href=/icons/favicon.svg><meta name=theme-color content="#434648"></head></head><body class=linkblog><div class=container><aside class=sidebar><div class=sidebar-content><div class=site-header><h1 class=site-title><a href=http://localhost:1313/>philippdubach</a></h1><p class=site-description>Personal Projects, Curated Articles and Papers on Economics, Finance and Technology</p></div><nav class=navigation><ul><li><a href=/>Home</a></li><li><a href=/projects/>Projects</a></li><li><a href=/about/>About</a></li><li><a href=/posts/>Archive</a></li><li><a href=/index.xml>RSS</a></li></ul></nav><div class=social-links><a href=https://github.com/philippdubach target=_blank rel=noopener>GitHub</a>
<a href=mailto:info@philippdubach.com>Email</a></div></div></aside><main class=content><article class="post single"><header class=post-header><h1 class=post-title><a href=https://arxiv.org/abs/2411.00782 target=_blank rel=noopener>AI Learns Economics Like Undergrads
<span class=external-link>→</span></a></h1><div class=post-meta><time datetime=2024-11-01T00>November 1, 2024
</time>• <a href=https://arxiv.org/abs/2411.00782 target=_blank rel=noopener>Original Link</a></div></header><div class=post-content><p>This cuts to the heart of how LLMs actually work: Testing Large Language Models on economics problems reveals that these supposedly sophisticated systems don&rsquo;t just learn correct reasoning—they absorb our misconceptions too. The study found LLMs performing reasonably well on undergraduate economics questions (around 65% accuracy) but falling flat on graduate-level problems (35% accuracy). More tellingly, the specific errors weren&rsquo;t random failures but systematic mistakes that mirror exactly what human students get wrong.</p><blockquote><p>&ldquo;Interestingly, the errors made by LLMs often mirror those made by human students, suggesting that these models may have learned not just correct economic reasoning but also common misconceptions.&rdquo;</p></blockquote><p>Which kind of makes sense when we understand how language models actually work: They&rsquo;re not reasoning through economic principles—they&rsquo;re pattern-matching against their training data, which includes millions of wrong answers, confused explanations, and half-understood concepts scattered across the internet.</p><p>What are the practical implications? If you&rsquo;re using AI for financial analysis or economic modeling, you&rsquo;re essentially getting a very confident undergraduate who&rsquo;s memorized a lot of material but fundamentally doesn&rsquo;t understand when to apply which concepts. The models particularly struggled with dynamic optimization and game theory—exactly the areas where getting it wrong costs real money. Perhaps most unsettling: chain-of-thought prompting barely helped. Even when asked to show their work, the models maintained their confident confusion, just with more elaborate explanations of why 2+2 equals 5.</p><p><em>Note: From the paper: &ldquo;testing set: January 1, 2023, to December 31, 2023&rdquo;</em></p></div></article></main></div></body></html>